{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projet Knapsack Problem 0-1\n",
    "\n",
    "**Équipe:** Chaabane, Arman, Bartosz, Ahmed\n",
    "\n",
    "## Structure du projet\n",
    "\n",
    "1. Infrastructure commune (Classes et structures de données)\n",
    "2. Algorithmes implémentés\n",
    "3. Système de benchmarking complet\n",
    "4. Analyse comparative approfondie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Configuration et Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from itertools import combinations\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from types import SimpleNamespace\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "\n",
    "ALGO_COLORS = {\n",
    "    'Brute Force': '#e41a1c',\n",
    "    'Dynamic Programming': '#377eb8',\n",
    "    'DP Top-Down': '#4daf4a',\n",
    "    'Branch and Bound': '#984ea3',\n",
    "    'Greedy Ratio': '#ff7f00', \n",
    "    'Greedy Value': '#ffff33', \n",
    "    'Greedy Weight': '#a65628', \n",
    "    'Fractional Knapsack': '#f781bf',\n",
    "    'Randomized': '#999999', \n",
    "    'Genetic Algorithm': '#17becf',  \n",
    "    'Genetic Adaptive': '#1f77b4',      \n",
    "    'Simulated Annealing': '#d62728',   \n",
    "    'SA Adaptive': '#ff9896',          \n",
    "    'FTPAS (ε=0.1)': '#9467bd',         \n",
    "    'FTPAS (ε=0.05)': '#8c564b',        \n",
    "    'FTPAS Adaptive': '#e377c2',        \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Générateur de Benchmarks Knapsack\n",
    "\n",
    "Ce générateur permet de créer des fichiers de benchmark personnalisés avec différents paramètres :\n",
    "- **Type de corrélation** : uncorrelated, weakly_correlated, strongly_correlated, similar_weights\n",
    "- **Nombre d'items (n)** : nombre d'objets dans l'instance\n",
    "- **Plage des poids (R)** : valeur maximale pour les poids générés aléatoirement\n",
    "- **Capacité** : peut être spécifiée ou calculée automatiquement (généralement 50% de la somme des poids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dossier: benchmarks/generated\n",
      "Types: ['uncorrelated', 'weakly_correlated', 'strongly_correlated', 'similar_weights', 'inverse_strongly_correlated']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from typing import Literal, Optional, Tuple, List\n",
    "from dataclasses import dataclass\n",
    "\n",
    "GENERATED_DIR = \"benchmarks/generated\"\n",
    "\n",
    "@dataclass\n",
    "class KnapsackInstance:\n",
    "    n: int\n",
    "    capacity: int\n",
    "    weights: List[int]\n",
    "    values: List[int]\n",
    "    correlation_type: str\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"KnapsackInstance(n={self.n}, capacity={self.capacity}, type={self.correlation_type})\"\n",
    "\n",
    "\n",
    "class KnapsackBenchmarkGenerator:\n",
    "    CORRELATION_TYPES = [\n",
    "        'uncorrelated',\n",
    "        'weakly_correlated', \n",
    "        'strongly_correlated',\n",
    "        'similar_weights',\n",
    "        'inverse_strongly_correlated'\n",
    "    ]\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.rng = np.random.default_rng()\n",
    "    \n",
    "    def generate(self, n: int, R: int = 1000, capacity: Optional[int] = None,\n",
    "                 capacity_ratio: float = 0.5, correlation_type: str = 'uncorrelated',\n",
    "                 correlation_param: float = 100.0) -> KnapsackInstance:\n",
    "        if correlation_type not in self.CORRELATION_TYPES:\n",
    "            raise ValueError(f\"Type inconnu: {correlation_type}. Choix: {self.CORRELATION_TYPES}\")\n",
    "        \n",
    "        weights = self._generate_weights(n, R, correlation_type, correlation_param)\n",
    "        values = self._generate_values(weights, R, correlation_type, correlation_param)\n",
    "        \n",
    "        if capacity is None:\n",
    "            capacity = int(capacity_ratio * sum(weights))\n",
    "        capacity = max(1, capacity)\n",
    "        \n",
    "        return KnapsackInstance(n=n, capacity=capacity, weights=weights.tolist(), \n",
    "                                values=values.tolist(), correlation_type=correlation_type)\n",
    "    \n",
    "    def _generate_weights(self, n: int, R: int, correlation_type: str, correlation_param: float) -> np.ndarray:\n",
    "        if correlation_type == 'similar_weights':\n",
    "            weights = self.rng.normal(R / 2, correlation_param, n)\n",
    "            weights = np.clip(weights, 1, R).astype(int)\n",
    "        else:\n",
    "            weights = self.rng.integers(1, R + 1, n)\n",
    "        return weights.astype(int)\n",
    "    \n",
    "    def _generate_values(self, weights: np.ndarray, R: int, correlation_type: str, correlation_param: float) -> np.ndarray:\n",
    "        n = len(weights)\n",
    "        if correlation_type == 'uncorrelated':\n",
    "            values = self.rng.integers(1, R + 1, n)\n",
    "        elif correlation_type == 'weakly_correlated':\n",
    "            noise = self.rng.integers(-int(correlation_param), int(correlation_param) + 1, n)\n",
    "            values = np.maximum(weights + noise, 1)\n",
    "        elif correlation_type == 'strongly_correlated':\n",
    "            values = weights + int(correlation_param)\n",
    "        elif correlation_type == 'similar_weights':\n",
    "            values = self.rng.integers(1, R + 1, n)\n",
    "        elif correlation_type == 'inverse_strongly_correlated':\n",
    "            values = np.maximum(weights.max() - weights + int(correlation_param), 1)\n",
    "        return values.astype(int)\n",
    "    \n",
    "    def _build_filename(self, instance: KnapsackInstance, index: int = None, format: str = 'standard') -> str:\n",
    "        ext = '.kp' if format == 'kp' else '.txt'\n",
    "        base = f\"{instance.correlation_type}_n{instance.n}_c{instance.capacity}\"\n",
    "        if index is not None:\n",
    "            return f\"{base}_{index:03d}{ext}\"\n",
    "        return f\"{base}{ext}\"\n",
    "    \n",
    "    def save_to_file(self, instance: KnapsackInstance, filepath: str = None,\n",
    "                     index: int = None, format: Literal['standard', 'kp'] = 'standard') -> str:\n",
    "        if filepath is None:\n",
    "            filepath = os.path.join(GENERATED_DIR, self._build_filename(instance, index, format))\n",
    "        \n",
    "        os.makedirs(os.path.dirname(filepath) if os.path.dirname(filepath) else '.', exist_ok=True)\n",
    "        with open(filepath, 'w') as f:\n",
    "            if format == 'standard':\n",
    "                f.write(f\"{instance.n} {instance.capacity}\\n\")\n",
    "                for v, w in zip(instance.values, instance.weights):\n",
    "                    f.write(f\"{v} {w}\\n\")\n",
    "            elif format == 'kp':\n",
    "                f.write(f\"\\n{instance.n}\\n{instance.capacity}\\n\\n\")\n",
    "                for v, w in zip(instance.values, instance.weights):\n",
    "                    f.write(f\"{v} {w}\\n\")\n",
    "        return filepath\n",
    "\n",
    "\n",
    "def generate_benchmarks(n: int, capacity: int = None, correlation = 'uncorrelated',\n",
    "                        R: int = 1000, count: int = 1, format: str = 'standard') -> List[KnapsackInstance]:\n",
    "    \"\"\"\n",
    "    Génère un ou plusieurs fichiers de benchmark.\n",
    "    \n",
    "    Args:\n",
    "        n: Nombre d'items\n",
    "        capacity: Capacité du sac (None = 50% de la somme des poids)\n",
    "        correlation: Type ou liste de types ('uncorrelated', 'weakly_correlated', \n",
    "                     'strongly_correlated', 'similar_weights', 'inverse_strongly_correlated')\n",
    "        R: Poids max [1, R]\n",
    "        count: Nombre de fichiers à générer par type\n",
    "        format: 'standard' (.txt) ou 'kp'\n",
    "    \"\"\"\n",
    "    generator = KnapsackBenchmarkGenerator()\n",
    "    instances = []\n",
    "    \n",
    "    correlations = [correlation] if isinstance(correlation, str) else correlation\n",
    "    \n",
    "    for corr_type in correlations:\n",
    "        for i in range(count):\n",
    "            instance = generator.generate(n=n, R=R, capacity=capacity, correlation_type=corr_type)\n",
    "            index = i + 1 if count > 1 else None\n",
    "            filepath = generator.save_to_file(instance, index=index, format=format)\n",
    "            instances.append(instance)\n",
    "            print(f\"✓ {filepath}\")\n",
    "    \n",
    "    total = len(correlations) * count\n",
    "    print(f\"\\n{total} fichier(s) généré(s) dans {GENERATED_DIR}/\")\n",
    "    return instances\n",
    "\n",
    "\n",
    "print(f\"Dossier: {GENERATED_DIR}\")\n",
    "print(f\"Types: {KnapsackBenchmarkGenerator.CORRELATION_TYPES}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exemples d'utilisation du générateur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ benchmarks/generated\\uncorrelated_n100_c2000_001.txt\n",
      "✓ benchmarks/generated\\uncorrelated_n100_c2000_002.txt\n",
      "✓ benchmarks/generated\\uncorrelated_n100_c2000_003.txt\n",
      "✓ benchmarks/generated\\uncorrelated_n100_c2000_004.txt\n",
      "✓ benchmarks/generated\\uncorrelated_n100_c2000_005.txt\n",
      "✓ benchmarks/generated\\strongly_correlated_n100_c2000_001.txt\n",
      "✓ benchmarks/generated\\strongly_correlated_n100_c2000_002.txt\n",
      "✓ benchmarks/generated\\strongly_correlated_n100_c2000_003.txt\n",
      "✓ benchmarks/generated\\strongly_correlated_n100_c2000_004.txt\n",
      "✓ benchmarks/generated\\strongly_correlated_n100_c2000_005.txt\n",
      "✓ benchmarks/generated\\weakly_correlated_n100_c2000_001.txt\n",
      "✓ benchmarks/generated\\weakly_correlated_n100_c2000_002.txt\n",
      "✓ benchmarks/generated\\weakly_correlated_n100_c2000_003.txt\n",
      "✓ benchmarks/generated\\weakly_correlated_n100_c2000_004.txt\n",
      "✓ benchmarks/generated\\weakly_correlated_n100_c2000_005.txt\n",
      "\n",
      "15 fichier(s) généré(s) dans benchmarks/generated/\n",
      "✓ benchmarks/generated\\uncorrelated_n500_c2000_001.txt\n",
      "✓ benchmarks/generated\\uncorrelated_n500_c2000_002.txt\n",
      "✓ benchmarks/generated\\uncorrelated_n500_c2000_003.txt\n",
      "✓ benchmarks/generated\\uncorrelated_n500_c2000_004.txt\n",
      "✓ benchmarks/generated\\uncorrelated_n500_c2000_005.txt\n",
      "✓ benchmarks/generated\\strongly_correlated_n500_c2000_001.txt\n",
      "✓ benchmarks/generated\\strongly_correlated_n500_c2000_002.txt\n",
      "✓ benchmarks/generated\\strongly_correlated_n500_c2000_003.txt\n",
      "✓ benchmarks/generated\\strongly_correlated_n500_c2000_004.txt\n",
      "✓ benchmarks/generated\\strongly_correlated_n500_c2000_005.txt\n",
      "✓ benchmarks/generated\\weakly_correlated_n500_c2000_001.txt\n",
      "✓ benchmarks/generated\\weakly_correlated_n500_c2000_002.txt\n",
      "✓ benchmarks/generated\\weakly_correlated_n500_c2000_003.txt\n",
      "✓ benchmarks/generated\\weakly_correlated_n500_c2000_004.txt\n",
      "✓ benchmarks/generated\\weakly_correlated_n500_c2000_005.txt\n",
      "\n",
      "15 fichier(s) généré(s) dans benchmarks/generated/\n",
      "✓ benchmarks/generated\\uncorrelated_n1000_c2000_001.txt\n",
      "✓ benchmarks/generated\\uncorrelated_n1000_c2000_002.txt\n",
      "✓ benchmarks/generated\\uncorrelated_n1000_c2000_003.txt\n",
      "✓ benchmarks/generated\\uncorrelated_n1000_c2000_004.txt\n",
      "✓ benchmarks/generated\\uncorrelated_n1000_c2000_005.txt\n",
      "✓ benchmarks/generated\\strongly_correlated_n1000_c2000_001.txt\n",
      "✓ benchmarks/generated\\strongly_correlated_n1000_c2000_002.txt\n",
      "✓ benchmarks/generated\\strongly_correlated_n1000_c2000_003.txt\n",
      "✓ benchmarks/generated\\strongly_correlated_n1000_c2000_004.txt\n",
      "✓ benchmarks/generated\\strongly_correlated_n1000_c2000_005.txt\n",
      "✓ benchmarks/generated\\weakly_correlated_n1000_c2000_001.txt\n",
      "✓ benchmarks/generated\\weakly_correlated_n1000_c2000_002.txt\n",
      "✓ benchmarks/generated\\weakly_correlated_n1000_c2000_003.txt\n",
      "✓ benchmarks/generated\\weakly_correlated_n1000_c2000_004.txt\n",
      "✓ benchmarks/generated\\weakly_correlated_n1000_c2000_005.txt\n",
      "\n",
      "15 fichier(s) généré(s) dans benchmarks/generated/\n",
      "✓ benchmarks/generated\\uncorrelated_n2000_c2000_001.txt\n",
      "✓ benchmarks/generated\\uncorrelated_n2000_c2000_002.txt\n",
      "✓ benchmarks/generated\\uncorrelated_n2000_c2000_003.txt\n",
      "✓ benchmarks/generated\\strongly_correlated_n2000_c2000_001.txt\n",
      "✓ benchmarks/generated\\strongly_correlated_n2000_c2000_002.txt\n",
      "✓ benchmarks/generated\\strongly_correlated_n2000_c2000_003.txt\n",
      "✓ benchmarks/generated\\weakly_correlated_n2000_c2000_001.txt\n",
      "✓ benchmarks/generated\\weakly_correlated_n2000_c2000_002.txt\n",
      "✓ benchmarks/generated\\weakly_correlated_n2000_c2000_003.txt\n",
      "\n",
      "9 fichier(s) généré(s) dans benchmarks/generated/\n",
      "✓ benchmarks/generated\\uncorrelated_n5000_c2000_001.txt\n",
      "✓ benchmarks/generated\\uncorrelated_n5000_c2000_002.txt\n",
      "✓ benchmarks/generated\\strongly_correlated_n5000_c2000_001.txt\n",
      "✓ benchmarks/generated\\strongly_correlated_n5000_c2000_002.txt\n",
      "✓ benchmarks/generated\\weakly_correlated_n5000_c2000_001.txt\n",
      "✓ benchmarks/generated\\weakly_correlated_n5000_c2000_002.txt\n",
      "\n",
      "6 fichier(s) généré(s) dans benchmarks/generated/\n",
      "✓ benchmarks/generated\\uncorrelated_n10000_c2000.txt\n",
      "✓ benchmarks/generated\\strongly_correlated_n10000_c2000.txt\n",
      "✓ benchmarks/generated\\weakly_correlated_n10000_c2000.txt\n",
      "\n",
      "3 fichier(s) généré(s) dans benchmarks/generated/\n",
      "✓ benchmarks/generated\\similar_weights_n100_c2000_001.txt\n",
      "✓ benchmarks/generated\\similar_weights_n100_c2000_002.txt\n",
      "✓ benchmarks/generated\\similar_weights_n100_c2000_003.txt\n",
      "\n",
      "3 fichier(s) généré(s) dans benchmarks/generated/\n",
      "✓ benchmarks/generated\\similar_weights_n200_c2000_001.txt\n",
      "✓ benchmarks/generated\\similar_weights_n200_c2000_002.txt\n",
      "\n",
      "2 fichier(s) généré(s) dans benchmarks/generated/\n",
      "✓ benchmarks/generated\\similar_weights_n1000_c2000_001.txt\n",
      "✓ benchmarks/generated\\similar_weights_n1000_c2000_002.txt\n",
      "\n",
      "2 fichier(s) généré(s) dans benchmarks/generated/\n",
      "✓ benchmarks/generated\\similar_weights_n500_c2000_001.txt\n",
      "✓ benchmarks/generated\\similar_weights_n500_c2000_002.txt\n",
      "\n",
      "2 fichier(s) généré(s) dans benchmarks/generated/\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[KnapsackInstance(n=500, capacity=2000, weights=[548, 393, 384, 357, 514, 616, 592, 478, 363, 361, 595, 542, 502, 349, 668, 606, 647, 371, 428, 436, 488, 450, 391, 593, 362, 518, 428, 675, 456, 648, 330, 445, 516, 671, 472, 324, 656, 527, 425, 428, 453, 564, 477, 493, 425, 604, 391, 479, 456, 628, 598, 474, 586, 499, 445, 430, 561, 316, 555, 544, 633, 473, 538, 461, 444, 362, 479, 581, 705, 629, 541, 517, 305, 652, 568, 395, 470, 564, 707, 531, 596, 772, 528, 404, 372, 628, 558, 472, 329, 486, 353, 383, 675, 559, 476, 406, 541, 666, 480, 484, 436, 645, 362, 481, 586, 612, 588, 378, 625, 314, 557, 514, 406, 387, 475, 569, 553, 480, 470, 587, 608, 598, 386, 499, 566, 628, 520, 506, 688, 407, 380, 374, 429, 538, 523, 607, 412, 643, 342, 452, 364, 436, 575, 485, 582, 429, 590, 271, 473, 496, 346, 360, 269, 554, 354, 362, 457, 400, 532, 609, 477, 533, 558, 574, 429, 476, 361, 381, 386, 486, 487, 543, 532, 539, 524, 393, 573, 377, 640, 610, 512, 391, 414, 427, 684, 645, 423, 459, 529, 532, 691, 617, 546, 799, 366, 533, 516, 633, 617, 478, 528, 453, 508, 482, 402, 747, 431, 649, 681, 438, 602, 520, 450, 394, 345, 459, 540, 614, 422, 375, 480, 628, 505, 457, 684, 582, 414, 524, 539, 545, 532, 477, 468, 376, 556, 576, 747, 474, 540, 558, 379, 530, 435, 705, 351, 450, 516, 411, 507, 490, 488, 559, 415, 447, 381, 344, 377, 406, 677, 224, 421, 500, 269, 712, 567, 590, 593, 639, 568, 495, 333, 480, 704, 510, 477, 427, 620, 697, 443, 535, 483, 361, 527, 417, 511, 607, 534, 673, 491, 519, 472, 499, 624, 386, 437, 589, 394, 613, 519, 640, 413, 387, 538, 746, 490, 348, 469, 678, 481, 373, 533, 421, 736, 679, 625, 474, 471, 486, 298, 481, 506, 543, 478, 353, 432, 494, 325, 529, 441, 552, 494, 491, 561, 638, 536, 429, 462, 581, 577, 599, 564, 520, 576, 549, 533, 492, 588, 459, 517, 515, 583, 577, 668, 448, 430, 608, 664, 328, 549, 561, 363, 619, 551, 549, 372, 516, 501, 248, 597, 381, 502, 386, 401, 600, 641, 484, 610, 437, 382, 656, 544, 425, 358, 451, 609, 517, 410, 535, 764, 316, 434, 600, 366, 502, 583, 527, 440, 590, 616, 481, 608, 616, 447, 661, 421, 631, 405, 491, 318, 549, 636, 579, 567, 550, 521, 362, 487, 505, 496, 452, 558, 568, 478, 687, 389, 457, 571, 336, 633, 691, 473, 573, 426, 636, 586, 465, 448, 607, 395, 455, 583, 192, 550, 581, 465, 440, 430, 383, 367, 497, 386, 440, 457, 469, 598, 586, 506, 491, 573, 543, 420, 351, 532, 532, 523, 220, 285, 702, 410, 577, 499, 573, 337, 569, 495, 630, 444, 492, 443, 376, 522, 646, 288, 597, 508, 571, 481, 682, 662, 590, 568, 603, 692, 527, 763, 520, 376, 479, 553, 418], values=[292, 570, 189, 512, 346, 504, 474, 220, 60, 175, 221, 76, 687, 907, 921, 685, 801, 996, 409, 398, 164, 486, 682, 900, 139, 375, 674, 661, 463, 663, 273, 614, 439, 448, 263, 233, 976, 777, 195, 157, 465, 987, 388, 705, 671, 570, 638, 936, 910, 841, 327, 741, 335, 393, 109, 625, 695, 390, 663, 899, 129, 278, 717, 237, 95, 971, 934, 660, 648, 424, 837, 800, 241, 346, 265, 664, 133, 411, 116, 540, 455, 823, 707, 881, 70, 606, 537, 11, 517, 23, 174, 122, 732, 512, 603, 557, 797, 135, 239, 652, 577, 86, 943, 25, 484, 503, 136, 578, 194, 834, 416, 965, 459, 936, 903, 723, 250, 678, 131, 149, 758, 277, 75, 468, 744, 394, 194, 433, 178, 125, 245, 636, 748, 830, 261, 208, 320, 373, 162, 331, 152, 779, 742, 56, 109, 688, 470, 620, 925, 968, 557, 756, 740, 386, 955, 596, 844, 120, 831, 930, 546, 553, 394, 193, 186, 336, 958, 583, 815, 14, 485, 432, 926, 849, 148, 705, 427, 672, 180, 677, 137, 875, 809, 837, 666, 480, 803, 434, 332, 450, 418, 388, 947, 296, 518, 858, 840, 409, 798, 600, 293, 891, 357, 237, 710, 790, 757, 101, 380, 971, 528, 762, 829, 460, 658, 561, 625, 372, 541, 463, 221, 9, 146, 926, 965, 385, 696, 117, 684, 633, 294, 259, 667, 904, 319, 965, 528, 724, 1000, 76, 288, 563, 500, 725, 466, 285, 336, 273, 897, 729, 683, 336, 601, 558, 432, 439, 970, 799, 776, 942, 191, 661, 696, 264, 484, 527, 749, 395, 481, 33, 993, 721, 680, 467, 817, 318, 505, 564, 284, 803, 276, 556, 833, 907, 618, 579, 941, 180, 631, 569, 15, 615, 389, 4, 849, 751, 668, 796, 547, 916, 19, 519, 73, 551, 692, 202, 922, 838, 756, 445, 793, 153, 193, 43, 336, 845, 155, 519, 991, 491, 241, 365, 274, 243, 309, 422, 888, 612, 232, 399, 418, 661, 767, 192, 932, 659, 437, 293, 957, 883, 583, 726, 215, 25, 758, 900, 235, 968, 996, 619, 523, 411, 175, 905, 295, 698, 548, 279, 206, 15, 386, 925, 179, 968, 461, 409, 472, 691, 418, 71, 98, 903, 98, 207, 809, 375, 968, 101, 198, 858, 474, 782, 45, 246, 822, 585, 196, 134, 392, 253, 529, 958, 465, 131, 619, 590, 687, 587, 15, 787, 378, 412, 592, 25, 444, 587, 103, 899, 204, 25, 59, 524, 974, 935, 865, 496, 970, 143, 207, 921, 39, 280, 319, 765, 505, 264, 322, 53, 558, 208, 346, 47, 387, 276, 599, 621, 760, 613, 161, 89, 868, 456, 572, 807, 717, 966, 474, 580, 577, 813, 29, 823, 438, 501, 637, 749, 693, 751, 157, 53, 748, 429, 523, 314, 552, 399, 254, 314, 871, 721, 72, 864, 867, 578, 694, 579, 239, 601, 191, 364, 814, 54, 526, 535, 710, 956, 761, 93, 405, 91, 76, 266, 939, 76, 407, 676, 21, 777, 9, 415], correlation_type='similar_weights'),\n",
       " KnapsackInstance(n=500, capacity=2000, weights=[601, 681, 455, 569, 533, 477, 391, 278, 344, 360, 542, 500, 329, 650, 325, 441, 533, 387, 408, 596, 437, 564, 392, 492, 494, 397, 542, 405, 491, 466, 369, 389, 650, 446, 591, 354, 504, 415, 365, 407, 385, 506, 524, 581, 489, 372, 492, 185, 613, 594, 441, 460, 586, 427, 719, 478, 583, 425, 517, 647, 472, 791, 316, 517, 457, 554, 446, 631, 530, 484, 553, 571, 426, 544, 552, 407, 606, 309, 530, 403, 515, 550, 356, 555, 424, 527, 493, 427, 400, 415, 541, 520, 644, 565, 390, 245, 640, 601, 352, 552, 345, 505, 485, 452, 369, 471, 519, 599, 561, 586, 631, 618, 512, 548, 385, 463, 409, 499, 561, 677, 573, 330, 653, 429, 481, 392, 497, 593, 590, 581, 552, 523, 541, 426, 566, 533, 527, 584, 648, 398, 433, 586, 491, 570, 735, 392, 475, 613, 473, 406, 442, 527, 379, 434, 562, 589, 525, 509, 509, 409, 323, 406, 399, 531, 441, 475, 657, 497, 616, 586, 553, 457, 622, 647, 593, 376, 581, 451, 493, 655, 485, 582, 485, 337, 427, 496, 554, 515, 510, 390, 316, 588, 496, 463, 491, 354, 518, 329, 513, 504, 581, 727, 624, 533, 471, 554, 584, 589, 344, 368, 585, 488, 602, 630, 533, 452, 276, 448, 351, 422, 463, 356, 450, 358, 612, 486, 445, 429, 496, 576, 382, 415, 318, 515, 620, 595, 377, 369, 675, 432, 497, 693, 309, 446, 434, 316, 429, 445, 622, 522, 407, 406, 440, 553, 577, 640, 730, 619, 530, 461, 431, 390, 571, 428, 443, 362, 458, 261, 537, 336, 512, 452, 525, 480, 309, 318, 543, 552, 460, 586, 528, 517, 487, 511, 435, 318, 379, 563, 418, 389, 544, 392, 594, 379, 592, 370, 461, 687, 501, 489, 526, 532, 659, 566, 552, 466, 460, 626, 620, 556, 464, 480, 467, 546, 550, 341, 591, 409, 544, 441, 709, 436, 528, 295, 564, 643, 489, 387, 689, 429, 395, 478, 647, 666, 520, 492, 490, 618, 313, 530, 447, 463, 502, 659, 479, 349, 618, 428, 492, 614, 620, 429, 519, 421, 432, 621, 495, 535, 449, 509, 554, 469, 547, 470, 399, 518, 416, 471, 524, 544, 322, 528, 425, 498, 671, 653, 450, 528, 642, 586, 620, 586, 345, 506, 513, 551, 543, 473, 477, 494, 495, 651, 486, 514, 654, 490, 447, 619, 380, 452, 492, 613, 367, 509, 435, 387, 496, 675, 539, 396, 588, 496, 444, 605, 629, 438, 536, 575, 582, 501, 553, 468, 666, 591, 483, 450, 394, 407, 634, 447, 621, 610, 359, 457, 602, 494, 519, 568, 373, 407, 508, 379, 413, 422, 550, 467, 490, 583, 583, 498, 553, 604, 566, 447, 389, 636, 350, 599, 617, 479, 663, 358, 441, 613, 525, 477, 597, 537, 602, 588, 376, 566, 378, 534, 417, 589, 587, 572, 475, 420, 338, 525, 574, 501, 549, 633, 478, 659, 664, 536, 599, 298, 524, 435, 487, 696, 380, 709, 337, 356], values=[189, 654, 566, 380, 659, 608, 595, 475, 162, 738, 715, 753, 886, 668, 729, 846, 526, 634, 702, 828, 698, 297, 640, 285, 759, 565, 93, 131, 376, 795, 16, 961, 26, 134, 339, 114, 557, 9, 731, 483, 605, 355, 612, 654, 968, 667, 98, 334, 98, 322, 890, 436, 977, 846, 759, 168, 476, 778, 559, 849, 701, 531, 885, 245, 896, 655, 692, 652, 381, 445, 165, 830, 282, 377, 960, 499, 969, 850, 292, 84, 92, 44, 466, 812, 281, 507, 909, 430, 92, 465, 128, 790, 270, 17, 13, 229, 553, 1, 148, 290, 120, 972, 272, 923, 703, 30, 609, 963, 148, 899, 925, 307, 521, 976, 73, 908, 667, 475, 528, 361, 317, 396, 211, 794, 491, 260, 180, 721, 872, 216, 720, 519, 966, 520, 558, 814, 665, 90, 862, 915, 676, 922, 299, 244, 588, 17, 925, 106, 643, 830, 474, 541, 44, 93, 575, 230, 636, 491, 640, 147, 203, 365, 291, 494, 260, 530, 253, 587, 283, 693, 26, 320, 98, 590, 718, 156, 926, 604, 924, 549, 612, 335, 893, 71, 822, 593, 7, 159, 355, 643, 120, 230, 299, 270, 823, 185, 315, 386, 885, 389, 579, 822, 918, 201, 732, 814, 92, 693, 235, 469, 696, 645, 963, 371, 113, 357, 333, 13, 123, 278, 934, 232, 541, 280, 821, 548, 316, 718, 227, 959, 54, 193, 470, 47, 588, 644, 81, 753, 368, 192, 136, 72, 301, 225, 10, 198, 210, 453, 631, 60, 899, 373, 600, 930, 755, 512, 596, 114, 108, 269, 589, 608, 318, 605, 250, 560, 486, 570, 161, 389, 757, 482, 102, 742, 58, 989, 198, 541, 225, 91, 154, 315, 446, 589, 621, 405, 16, 573, 916, 858, 1000, 207, 394, 383, 339, 421, 514, 402, 903, 556, 748, 789, 193, 752, 870, 704, 85, 131, 325, 621, 413, 465, 172, 34, 433, 183, 485, 811, 927, 825, 980, 623, 521, 747, 627, 870, 581, 452, 223, 938, 835, 578, 295, 422, 593, 46, 856, 300, 563, 188, 920, 9, 349, 677, 451, 457, 174, 598, 891, 512, 190, 189, 839, 933, 466, 612, 432, 55, 44, 142, 319, 774, 780, 856, 295, 944, 273, 766, 354, 739, 907, 408, 518, 992, 161, 402, 771, 476, 861, 289, 943, 877, 47, 111, 413, 702, 540, 619, 845, 851, 561, 977, 247, 648, 624, 377, 829, 309, 588, 485, 195, 153, 833, 792, 369, 515, 778, 880, 272, 873, 544, 288, 851, 743, 884, 660, 5, 931, 543, 376, 934, 362, 633, 163, 648, 540, 226, 753, 81, 954, 997, 407, 294, 408, 877, 320, 606, 687, 113, 752, 915, 200, 21, 931, 701, 546, 257, 966, 129, 799, 508, 801, 32, 620, 955, 600, 709, 618, 877, 203, 933, 822, 874, 894, 382, 167, 26, 312, 208, 216, 111, 411, 26, 183, 212, 917, 927, 275, 414, 823, 657, 828, 709, 568, 281, 9, 816, 235, 344, 761, 873, 63, 752, 271, 193, 922, 296, 878, 15, 578], correlation_type='similar_weights')]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# GÉNÉRATION DE BENCHMARKS\n",
    "# =============================================================================\n",
    "# Types: 'uncorrelated', 'weakly_correlated', 'strongly_correlated', \n",
    "#        'similar_weights', 'inverse_strongly_correlated'\n",
    "# =============================================================================\n",
    "generate_benchmarks(n=100, capacity=2000, correlation=['uncorrelated', 'strongly_correlated', 'weakly_correlated'], count=5)\n",
    "generate_benchmarks(n=500, capacity=2000, correlation=['uncorrelated', 'strongly_correlated', 'weakly_correlated'], count=5)\n",
    "generate_benchmarks(n=1000, capacity=2000, correlation=['uncorrelated', 'strongly_correlated', 'weakly_correlated'], count=5)\n",
    "generate_benchmarks(n=2000, capacity=2000, correlation=['uncorrelated', 'strongly_correlated', 'weakly_correlated'], count=3)\n",
    "generate_benchmarks(n=5000, capacity=2000, correlation=['uncorrelated', 'strongly_correlated', 'weakly_correlated'], count=2)\n",
    "generate_benchmarks(n=10000, capacity=2000, correlation=['uncorrelated', 'strongly_correlated', 'weakly_correlated'])\n",
    "generate_benchmarks(n=100, capacity=2000, correlation='similar_weights', count=3)\n",
    "generate_benchmarks(n=200, capacity=2000, correlation='similar_weights', count=2)\n",
    "generate_benchmarks(n=1000, capacity=2000, correlation='similar_weights', count=2)\n",
    "generate_benchmarks(n=500, capacity=2000, correlation='similar_weights', count=2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# generate_benchmarks(n=100, capacity=5000, correlation='uncorrelated')\n",
    "# generate_benchmarks(n=100, capacity=5000, correlation='strongly_correlated', count=5)\n",
    "# generate_benchmarks(n=100, capacity=5000, correlation=['uncorrelated', 'strongly_correlated', 'weakly_correlated'])\n",
    "# generate_benchmarks(n=100, capacity=5000, correlation=['uncorrelated', 'strongly_correlated', 'weakly_correlated'], count=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Structures de Données Communes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Item:\n",
    "    \"\"\"Représente un item avec son poids et sa valeur\"\"\"\n",
    "    def __init__(self, item_id, weight, value):\n",
    "        self.id = item_id\n",
    "        self.weight = weight\n",
    "        self.value = value\n",
    "    \n",
    "    def ratio(self):\n",
    "        return self.value / self.weight if self.weight > 0 else 0\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"Item({self.id}, w={self.weight}, v={self.value})\"\n",
    "\n",
    "\n",
    "class Problem:\n",
    "    \"\"\"Représente une instance du problème de knapsack\"\"\"\n",
    "    def __init__(self, items, capacity):\n",
    "        self.items = items\n",
    "        self.capacity = capacity\n",
    "        self.n = len(items)\n",
    "\n",
    "\n",
    "class Solution:\n",
    "    \"\"\"Représente une solution au problème\"\"\"\n",
    "    def __init__(self, selected_items, total_value, total_weight, time_taken):\n",
    "        self.selected_items = selected_items\n",
    "        self.total_value = total_value\n",
    "        self.total_weight = total_weight\n",
    "        self.time = time_taken\n",
    "        self.usage_percent = (total_weight / 1.0) * 100  # Sera mis à jour"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Parsing et Gestion des Benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: La fonction parse_benchmark_file() est maintenant définie dans la section 5.1\n",
    "# avec discover_benchmarks() pour supporter les deux formats de fichiers (.txt et .kp)\n",
    "# Voir la cellule \"Découverte automatique des benchmarks\" plus bas\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Algorithmes Implémentés\n",
    "\n",
    "### 4.1 Brute Force"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def brute_force(problem):\n",
    "    \"\"\"Algorithme exhaustif O(2^n)\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    best_value = 0\n",
    "    best_weight = 0\n",
    "    best_items = []\n",
    "    \n",
    "    for size in range(problem.n + 1):\n",
    "        for combo in combinations(range(problem.n), size):\n",
    "            total_weight = sum(problem.items[i].weight for i in combo)\n",
    "            total_value = sum(problem.items[i].value for i in combo)\n",
    "            \n",
    "            if total_weight <= problem.capacity and total_value > best_value:\n",
    "                best_value = total_value\n",
    "                best_weight = total_weight\n",
    "                best_items = list(combo)\n",
    "    \n",
    "    time_taken = time.time() - start_time\n",
    "    sol = Solution(best_items, best_value, best_weight, time_taken)\n",
    "    sol.usage_percent = (best_weight / problem.capacity) * 100 if problem.capacity > 0 else 0\n",
    "    return sol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Programmation Dynamique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dynamic_programming(problem):\n",
    "    \"\"\"Programmation dynamique O(n x C) avec protection\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    n = problem.n\n",
    "    C = problem.capacity\n",
    "    \n",
    "    total_cells = n * C\n",
    "    if total_cells > 10_000_000:\n",
    "        print(f\"DP Skip: matrice trop grande ({n}×{C:,} = {total_cells:,})\")\n",
    "        return None\n",
    "    \n",
    "    estimated_mb = (total_cells * 8) / (1024 * 1024)\n",
    "    if estimated_mb > 500:\n",
    "        print(f\"DP Skip: mémoire > 500 MB ({estimated_mb:.0f} MB)\")\n",
    "        return None\n",
    "    \n",
    "    dp = [[0 for _ in range(C + 1)] for _ in range(n + 1)]\n",
    "    \n",
    "    for i in range(1, n + 1):\n",
    "        item = problem.items[i - 1]\n",
    "        for w in range(C + 1):\n",
    "            dp[i][w] = dp[i - 1][w]\n",
    "            if item.weight <= w:\n",
    "                dp[i][w] = max(dp[i][w], dp[i - 1][w - item.weight] + item.value)\n",
    "    \n",
    "    # Reconstruction\n",
    "    selected = []\n",
    "    w = C\n",
    "    for i in range(n, 0, -1):\n",
    "        if dp[i][w] != dp[i - 1][w]:\n",
    "            selected.append(i - 1)\n",
    "            w -= problem.items[i - 1].weight\n",
    "    \n",
    "    total_value = dp[n][C]\n",
    "    total_weight = sum(problem.items[i].weight for i in selected)\n",
    "    \n",
    "    time_taken = time.time() - start_time\n",
    "    sol = Solution(selected, total_value, total_weight, time_taken)\n",
    "    sol.usage_percent = (total_weight / problem.capacity) * 100 if problem.capacity > 0 else 0\n",
    "    return sol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.1 Programmation Dynamique Top-Down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dynamic_programming_topdown(problem):\n",
    "    \"\"\"\n",
    "    Programmation dynamique Top-Down avec mémoïsation\n",
    "    \n",
    "    Complexité temporelle: O(n × C)\n",
    "    Complexité spatiale: O(n × C) pour le cache + O(n) pour la pile de récursion\n",
    "    \n",
    "    Avantages par rapport à Bottom-Up:\n",
    "    - Ne calcule que les sous-problèmes nécessaires\n",
    "    - Plus intuitif (suit la définition récursive)\n",
    "    - Peut être plus rapide si tous les sous-problèmes ne sont pas nécessaires\n",
    "    \n",
    "    Returns:\n",
    "        Solution object\n",
    "    \"\"\"\n",
    "    import sys\n",
    "    start_time = time.time()\n",
    "    \n",
    "    n = problem.n\n",
    "    C = problem.capacity\n",
    "    items = problem.items\n",
    "    \n",
    "    # Protection contre les grandes instances\n",
    "    total_cells = n * C\n",
    "    if total_cells > 10_000_000:\n",
    "        print(f\"DP Top-Down Skip: cache trop grand ({n}×{C:,} = {total_cells:,})\")\n",
    "        return None\n",
    "    \n",
    "    # Augmenter la limite de récursion si nécessaire\n",
    "    old_limit = sys.getrecursionlimit()\n",
    "    if n + 100 > old_limit:\n",
    "        sys.setrecursionlimit(max(n + 100, old_limit))\n",
    "    \n",
    "    # Cache pour mémoïsation: memo[i][w] = valeur max avec items 0..i-1 et capacité w\n",
    "    memo = {}\n",
    "    \n",
    "    def knapsack(i, w):\n",
    "        \"\"\"\n",
    "        Retourne la valeur maximale possible avec les items 0..i-1 et capacité w\n",
    "        \"\"\"\n",
    "        # Cas de base\n",
    "        if i == 0 or w == 0:\n",
    "            return 0\n",
    "        \n",
    "        # Vérifier le cache\n",
    "        if (i, w) in memo:\n",
    "            return memo[(i, w)]\n",
    "        \n",
    "        item = items[i - 1]\n",
    "        \n",
    "        # Si l'item est trop lourd, on ne peut pas le prendre\n",
    "        if item.weight > w:\n",
    "            result = knapsack(i - 1, w)\n",
    "        else:\n",
    "            # Max entre prendre et ne pas prendre l'item\n",
    "            not_take = knapsack(i - 1, w)\n",
    "            take = knapsack(i - 1, w - item.weight) + item.value\n",
    "            result = max(not_take, take)\n",
    "        \n",
    "        memo[(i, w)] = result\n",
    "        return result\n",
    "    \n",
    "    # Calculer la valeur optimale\n",
    "    try:\n",
    "        best_value = knapsack(n, C)\n",
    "    except RecursionError:\n",
    "        print(f\"DP Top-Down Skip: récursion trop profonde (n={n})\")\n",
    "        sys.setrecursionlimit(old_limit)\n",
    "        return None\n",
    "    \n",
    "    # Reconstruction de la solution\n",
    "    selected = []\n",
    "    w = C\n",
    "    for i in range(n, 0, -1):\n",
    "        if w == 0:\n",
    "            break\n",
    "        item = items[i - 1]\n",
    "        # Si la valeur change quand on exclut cet item, c'est qu'on l'a pris\n",
    "        val_with = memo.get((i, w), 0)\n",
    "        val_without = memo.get((i - 1, w), 0)\n",
    "        if val_with != val_without:\n",
    "            selected.append(i - 1)\n",
    "            w -= item.weight\n",
    "    \n",
    "    total_weight = sum(items[i].weight for i in selected)\n",
    "    \n",
    "    # Restaurer la limite de récursion\n",
    "    sys.setrecursionlimit(old_limit)\n",
    "    \n",
    "    time_taken = time.time() - start_time\n",
    "    sol = Solution(selected, best_value, total_weight, time_taken)\n",
    "    sol.usage_percent = (total_weight / problem.capacity) * 100 if problem.capacity > 0 else 0\n",
    "    return sol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Branch and Bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def branch_and_bound(problem):\n",
    "    \"\"\"Branch and Bound avec élagage\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    sorted_indices = sorted(range(problem.n), \n",
    "                          key=lambda i: problem.items[i].ratio(), \n",
    "                          reverse=True)\n",
    "    \n",
    "    best_value = 0\n",
    "    best_solution = []\n",
    "    \n",
    "    def bound(level, current_weight, current_value):\n",
    "        if current_weight >= problem.capacity:\n",
    "            return 0\n",
    "        \n",
    "        value_bound = current_value\n",
    "        total_weight = current_weight\n",
    "        \n",
    "        for i in range(level, problem.n):\n",
    "            idx = sorted_indices[i]\n",
    "            item = problem.items[idx]\n",
    "            \n",
    "            if total_weight + item.weight <= problem.capacity:\n",
    "                total_weight += item.weight\n",
    "                value_bound += item.value\n",
    "            else:\n",
    "                remaining = problem.capacity - total_weight\n",
    "                value_bound += item.value * (remaining / item.weight)\n",
    "                break\n",
    "        \n",
    "        return value_bound\n",
    "    \n",
    "    def branch(level, current_weight, current_value, current_items):\n",
    "        nonlocal best_value, best_solution\n",
    "        \n",
    "        if level == problem.n:\n",
    "            if current_value > best_value:\n",
    "                best_value = current_value\n",
    "                best_solution = current_items[:]\n",
    "            return\n",
    "        \n",
    "        idx = sorted_indices[level]\n",
    "        item = problem.items[idx]\n",
    "        \n",
    "        if current_weight + item.weight <= problem.capacity:\n",
    "            new_value = current_value + item.value\n",
    "            if bound(level + 1, current_weight + item.weight, new_value) > best_value:\n",
    "                current_items.append(idx)\n",
    "                branch(level + 1, current_weight + item.weight, new_value, current_items)\n",
    "                current_items.pop()\n",
    "        \n",
    "        if bound(level + 1, current_weight, current_value) > best_value:\n",
    "            branch(level + 1, current_weight, current_value, current_items)\n",
    "    \n",
    "    branch(0, 0, 0, [])\n",
    "    \n",
    "    total_weight = sum(problem.items[i].weight for i in best_solution)\n",
    "    time_taken = time.time() - start_time\n",
    "    \n",
    "    sol = Solution(best_solution, best_value, total_weight, time_taken)\n",
    "    sol.usage_percent = (total_weight / problem.capacity) * 100 if problem.capacity > 0 else 0\n",
    "    return sol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Algorithmes Gloutons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_by_value(problem):\n",
    "    \"\"\"Greedy par valeur décroissante\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    sorted_items = sorted(enumerate(problem.items), key=lambda x: x[1].value, reverse=True)\n",
    "    \n",
    "    selected = []\n",
    "    total_weight = 0\n",
    "    total_value = 0\n",
    "    \n",
    "    for idx, item in sorted_items:\n",
    "        if total_weight + item.weight <= problem.capacity:\n",
    "            selected.append(idx)\n",
    "            total_weight += item.weight\n",
    "            total_value += item.value\n",
    "    \n",
    "    time_taken = time.time() - start_time\n",
    "    sol = Solution(selected, total_value, total_weight, time_taken)\n",
    "    sol.usage_percent = (total_weight / problem.capacity) * 100 if problem.capacity > 0 else 0\n",
    "    return sol\n",
    "\n",
    "\n",
    "def greedy_by_weight(problem):\n",
    "    \"\"\"Greedy par poids croissant\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    sorted_items = sorted(enumerate(problem.items), key=lambda x: x[1].weight)\n",
    "    \n",
    "    selected = []\n",
    "    total_weight = 0\n",
    "    total_value = 0\n",
    "    \n",
    "    for idx, item in sorted_items:\n",
    "        if total_weight + item.weight <= problem.capacity:\n",
    "            selected.append(idx)\n",
    "            total_weight += item.weight\n",
    "            total_value += item.value\n",
    "    \n",
    "    time_taken = time.time() - start_time\n",
    "    sol = Solution(selected, total_value, total_weight, time_taken)\n",
    "    sol.usage_percent = (total_weight / problem.capacity) * 100 if problem.capacity > 0 else 0\n",
    "    return sol\n",
    "\n",
    "\n",
    "def greedy_by_ratio(problem):\n",
    "    \"\"\"Greedy par ratio valeur/poids décroissant\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    sorted_items = sorted(enumerate(problem.items), key=lambda x: x[1].ratio(), reverse=True)\n",
    "    \n",
    "    selected = []\n",
    "    total_weight = 0\n",
    "    total_value = 0\n",
    "    \n",
    "    for idx, item in sorted_items:\n",
    "        if total_weight + item.weight <= problem.capacity:\n",
    "            selected.append(idx)\n",
    "            total_weight += item.weight\n",
    "            total_value += item.value\n",
    "    \n",
    "    time_taken = time.time() - start_time\n",
    "    sol = Solution(selected, total_value, total_weight, time_taken)\n",
    "    sol.usage_percent = (total_weight / problem.capacity) * 100 if problem.capacity > 0 else 0\n",
    "    return sol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4.1 Fractional Knapsack (Sac à dos fractionnel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fractional_knapsack(problem):\n",
    "    \"\"\"\n",
    "    Fractional Knapsack - Algorithme glouton optimal pour le sac à dos fractionnel\n",
    "    \n",
    "    Complexité temporelle: O(n log n) pour le tri\n",
    "    Complexité spatiale: O(n)\n",
    "    \n",
    "    Différence avec 0-1 Knapsack:\n",
    "    - Permet de prendre une FRACTION d'un item\n",
    "    - Solution optimale garantie (contrairement au 0-1)\n",
    "    - Sert de borne supérieure pour le 0-1 Knapsack\n",
    "    \n",
    "    Stratégie: Trier par ratio valeur/poids décroissant et prendre \n",
    "    les items dans cet ordre (fractions si nécessaire)\n",
    "    \n",
    "    Returns:\n",
    "        Solution object avec fraction_taken indiquant les fractions prises\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    n = problem.n\n",
    "    capacity = problem.capacity\n",
    "    items = problem.items\n",
    "    \n",
    "    # Trier les items par ratio valeur/poids décroissant\n",
    "    sorted_items = sorted(enumerate(items), key=lambda x: x[1].ratio(), reverse=True)\n",
    "    \n",
    "    total_value = 0.0\n",
    "    total_weight = 0.0\n",
    "    selected = []  # Liste de tuples (index, fraction_prise)\n",
    "    fractions = {}  # Pour stocker les fractions de chaque item\n",
    "    \n",
    "    remaining_capacity = capacity\n",
    "    \n",
    "    for idx, item in sorted_items:\n",
    "        if remaining_capacity <= 0:\n",
    "            break\n",
    "            \n",
    "        if item.weight <= remaining_capacity:\n",
    "            # Prendre l'item entier\n",
    "            selected.append(idx)\n",
    "            fractions[idx] = 1.0\n",
    "            total_value += item.value\n",
    "            total_weight += item.weight\n",
    "            remaining_capacity -= item.weight\n",
    "        else:\n",
    "            # Prendre une fraction de l'item\n",
    "            fraction = remaining_capacity / item.weight\n",
    "            fractions[idx] = fraction\n",
    "            total_value += item.value * fraction\n",
    "            total_weight += item.weight * fraction\n",
    "            selected.append(idx)\n",
    "            remaining_capacity = 0\n",
    "    \n",
    "    time_taken = time.time() - start_time\n",
    "    \n",
    "    # Créer la solution\n",
    "    # Note: total_value peut être un float, on le garde ainsi pour la précision\n",
    "    sol = Solution(selected, total_value, total_weight, time_taken)\n",
    "    sol.usage_percent = (total_weight / capacity) * 100 if capacity > 0 else 0\n",
    "    sol.fractions = fractions  # Stocker les fractions pour référence\n",
    "    sol.is_fractional = True  # Marquer comme solution fractionnelle\n",
    "    \n",
    "    return sol\n",
    "\n",
    "\n",
    "def fractional_knapsack_bound(problem):\n",
    "    \"\"\"\n",
    "    Calcule uniquement la borne supérieure (valeur max du fractional knapsack)\n",
    "    Utile pour Branch and Bound et comparaisons\n",
    "    \n",
    "    Returns:\n",
    "        float: Valeur maximale possible (borne supérieure pour 0-1)\n",
    "    \"\"\"\n",
    "    sorted_items = sorted(problem.items, key=lambda x: x.ratio(), reverse=True)\n",
    "    \n",
    "    total_value = 0.0\n",
    "    remaining_capacity = problem.capacity\n",
    "    \n",
    "    for item in sorted_items:\n",
    "        if remaining_capacity <= 0:\n",
    "            break\n",
    "        if item.weight <= remaining_capacity:\n",
    "            total_value += item.value\n",
    "            remaining_capacity -= item.weight\n",
    "        else:\n",
    "            total_value += item.value * (remaining_capacity / item.weight)\n",
    "            break\n",
    "    \n",
    "    return total_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Approche Randomisée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomized_approach(problem, iterations=1000, seed=None):\n",
    "    \"\"\"Approche randomisée multi-start\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    if seed is not None:\n",
    "        random.seed(seed)\n",
    "    \n",
    "    best_value = 0\n",
    "    best_weight = 0\n",
    "    best_items = []\n",
    "    \n",
    "    for _ in range(iterations):\n",
    "        indices = list(range(problem.n))\n",
    "        random.shuffle(indices)\n",
    "        \n",
    "        selected = []\n",
    "        total_weight = 0\n",
    "        total_value = 0\n",
    "        \n",
    "        for idx in indices:\n",
    "            item = problem.items[idx]\n",
    "            if total_weight + item.weight <= problem.capacity:\n",
    "                selected.append(idx)\n",
    "                total_weight += item.weight\n",
    "                total_value += item.value\n",
    "        \n",
    "        if total_value > best_value:\n",
    "            best_value = total_value\n",
    "            best_weight = total_weight\n",
    "            best_items = selected\n",
    "    \n",
    "    time_taken = time.time() - start_time\n",
    "    sol = Solution(best_items, best_value, best_weight, time_taken)\n",
    "    sol.usage_percent = (best_weight / problem.capacity) * 100 if problem.capacity > 0 else 0\n",
    "    return sol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 Algorithme Génétique (Genetic Algorithm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genetic_algorithm(problem, population_size=100, generations=50, \n",
    "                     crossover_rate=0.8, mutation_rate=0.02, \n",
    "                     elitism_count=5, seed=None):\n",
    "    \"\"\"\n",
    "\n",
    "    Args:\n",
    "        problem: Instance du problème (Problem object)\n",
    "        population_size: Taille de la population (nombre de chromosomes)\n",
    "        generations: Nombre de générations (itérations)\n",
    "        crossover_rate: Probabilité de croisement (0.0 à 1.0)\n",
    "        mutation_rate: Probabilité de mutation par gène (0.0 à 1.0)\n",
    "        elitism_count: Nombre de meilleures solutions à conserver\n",
    "        seed: Graine aléatoire pour reproductibilité\n",
    "    \n",
    "    Returns:\n",
    "        Solution object\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    if seed is not None:\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "    \n",
    "    n = problem.n\n",
    "    capacity = problem.capacity\n",
    "    items = problem.items\n",
    "    \n",
    "    # === 1. FONCTION DE FITNESS ===\n",
    "    def fitness(chromosome):\n",
    "        \"\"\"Qualité d'un chromosome (solution)\"\"\"\n",
    "        total_weight = sum(chromosome[i] * items[i].weight for i in range(n))\n",
    "        total_value = sum(chromosome[i] * items[i].value for i in range(n))\n",
    "        \n",
    "        # Pénalité si capacité dépassée\n",
    "        if total_weight > capacity:\n",
    "            # Pénalité proportionnelle au dépassement\n",
    "            penalty = (total_weight - capacity) * 10\n",
    "            return max(0, total_value - penalty)\n",
    "        return total_value\n",
    "    \n",
    "    # POPULATION INITIALE\n",
    "    def create_initial_population():\n",
    "        \"\"\"Crée la population initiale avec différentes stratégies\"\"\"\n",
    "        population = []\n",
    "        \n",
    "        # 50% solutions aléatoires\n",
    "        for _ in range(population_size // 2):\n",
    "            chromosome = [random.randint(0, 1) for _ in range(n)]\n",
    "            population.append(chromosome)\n",
    "        \n",
    "        # 25% solutions greedy (ratio)\n",
    "        sorted_indices = sorted(range(n), key=lambda i: items[i].ratio(), reverse=True)\n",
    "        for _ in range(population_size // 4):\n",
    "            chromosome = [0] * n\n",
    "            weight = 0\n",
    "            for idx in sorted_indices:\n",
    "                if weight + items[idx].weight <= capacity and random.random() > 0.3:\n",
    "                    chromosome[idx] = 1\n",
    "                    weight += items[idx].weight\n",
    "            population.append(chromosome)\n",
    "        \n",
    "        # 25% solutions avec densité variable\n",
    "        for _ in range(population_size - len(population)):\n",
    "            chromosome = [0] * n\n",
    "            density = random.uniform(0.2, 0.8)\n",
    "            weight = 0\n",
    "            for i in range(n):\n",
    "                if random.random() < density and weight + items[i].weight <= capacity:\n",
    "                    chromosome[i] = 1\n",
    "                    weight += items[i].weight\n",
    "            population.append(chromosome)\n",
    "        \n",
    "        return population\n",
    "    \n",
    "    # SÉLECTION PAR TOURNOI\n",
    "    def tournament_selection(population, fitnesses, tournament_size=3):\n",
    "        \"\"\"Sélectionne un individu par tournoi\"\"\"\n",
    "        tournament_indices = random.sample(range(len(population)), tournament_size)\n",
    "        tournament_fitnesses = [fitnesses[i] for i in tournament_indices]\n",
    "        winner_idx = tournament_indices[tournament_fitnesses.index(max(tournament_fitnesses))]\n",
    "        return population[winner_idx]\n",
    "    \n",
    "    # CROISEMENT (CROSSOVER)\n",
    "    def crossover(parent1, parent2):\n",
    "        \"\"\"Croisement à deux points\"\"\"\n",
    "        if random.random() > crossover_rate:\n",
    "            return parent1[:], parent2[:]\n",
    "        \n",
    "        # Deux points de coupure\n",
    "        point1 = random.randint(1, n - 2)\n",
    "        point2 = random.randint(point1 + 1, n - 1)\n",
    "        \n",
    "        child1 = parent1[:point1] + parent2[point1:point2] + parent1[point2:]\n",
    "        child2 = parent2[:point1] + parent1[point1:point2] + parent2[point2:]\n",
    "        \n",
    "        return child1, child2\n",
    "    \n",
    "    # MUTATION\n",
    "    def mutate(chromosome):\n",
    "        \"\"\"Mutation par flip de bits\"\"\"\n",
    "        mutated = chromosome[:]\n",
    "        for i in range(n):\n",
    "            if random.random() < mutation_rate:\n",
    "                mutated[i] = 1 - mutated[i]  # Flip 0->1 ou 1->0\n",
    "        return mutated\n",
    "    \n",
    "    # ALGORITHME PRINCIPAL\n",
    "    population = create_initial_population()\n",
    "    best_chromosome = None\n",
    "    best_fitness = -1\n",
    "    \n",
    "    for gen in range(generations):\n",
    "        # Évaluation de la population\n",
    "        fitnesses = [fitness(chromo) for chromo in population]\n",
    "        \n",
    "        # Mise à jour de la meilleure solution\n",
    "        gen_best_idx = fitnesses.index(max(fitnesses))\n",
    "        gen_best_fitness = fitnesses[gen_best_idx]\n",
    "        \n",
    "        if gen_best_fitness > best_fitness:\n",
    "            best_fitness = gen_best_fitness\n",
    "            best_chromosome = population[gen_best_idx][:]\n",
    "        \n",
    "        # Tri par fitness (pour l'élitisme)\n",
    "        sorted_indices = sorted(range(len(population)), key=lambda i: fitnesses[i], reverse=True)\n",
    "        \n",
    "        # Nouvelle génération\n",
    "        new_population = []\n",
    "        \n",
    "        # Élitisme : garder les meilleurs\n",
    "        for i in range(elitism_count):\n",
    "            new_population.append(population[sorted_indices[i]][:])\n",
    "        \n",
    "        # Génération du reste de la population\n",
    "        while len(new_population) < population_size:\n",
    "            # Sélection\n",
    "            parent1 = tournament_selection(population, fitnesses)\n",
    "            parent2 = tournament_selection(population, fitnesses)\n",
    "            \n",
    "            # Croisement\n",
    "            child1, child2 = crossover(parent1, parent2)\n",
    "            \n",
    "            # Mutation\n",
    "            child1 = mutate(child1)\n",
    "            child2 = mutate(child2)\n",
    "            \n",
    "            new_population.append(child1)\n",
    "            if len(new_population) < population_size:\n",
    "                new_population.append(child2)\n",
    "        \n",
    "        population = new_population\n",
    "    \n",
    "    # MEILLEURE SOLUTION\n",
    "    selected_items = [i for i in range(n) if best_chromosome[i] == 1]\n",
    "    total_value = sum(items[i].value for i in selected_items)\n",
    "    total_weight = sum(items[i].weight for i in selected_items)\n",
    "    \n",
    "    time_taken = time.time() - start_time\n",
    "    \n",
    "    sol = Solution(selected_items, total_value, total_weight, time_taken)\n",
    "    sol.usage_percent = (total_weight / capacity * 100) if capacity > 0 else 0\n",
    "    \n",
    "    return sol\n",
    "\n",
    "\n",
    "def genetic_algorithm_adaptive(problem):\n",
    "    \"\"\"\n",
    "    Version adaptative de l'algorithme génétique\n",
    "    Ajuste les paramètres selon la taille du problème\n",
    "    \"\"\"\n",
    "    n = problem.n\n",
    "    \n",
    "    if n <= 50:\n",
    "        return genetic_algorithm(problem, population_size=50, generations=30, mutation_rate=0.03)\n",
    "    elif n <= 100:\n",
    "        return genetic_algorithm(problem, population_size=80, generations=40, mutation_rate=0.02)\n",
    "    elif n <= 500:\n",
    "        return genetic_algorithm(problem, population_size=100, generations=50, mutation_rate=0.02)\n",
    "    elif n <= 1000:\n",
    "        return genetic_algorithm(problem, population_size=120, generations=40, mutation_rate=0.01)\n",
    "    else:\n",
    "        return genetic_algorithm(problem, population_size=150, generations=30, mutation_rate=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6.1 Simulated Annealing (Recuit Simulé)\n",
    "\n",
    "**Principe:** Inspiré du recuit métallurgique, l'algorithme explore l'espace des solutions en acceptant parfois des solutions moins bonnes pour échapper aux optima locaux. La probabilité d'accepter une mauvaise solution diminue avec la \"température\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulated_annealing(problem, initial_temp=1000, cooling_rate=0.995, \n",
    "                        min_temp=1, max_iterations=10000, seed=None):\n",
    "    \"\"\"\n",
    "    Simulated Annealing (Recuit Simulé) pour le Knapsack 0-1\n",
    "    \n",
    "    Complexité temporelle: O(max_iterations × n)\n",
    "    Complexité spatiale: O(n)\n",
    "    \n",
    "    Principe:\n",
    "    - Commence avec une solution initiale (greedy)\n",
    "    - À chaque itération, génère un voisin en flippant un bit\n",
    "    - Accepte toujours les améliorations\n",
    "    - Accepte les dégradations avec probabilité exp(-ΔE/T)\n",
    "    - La température T diminue progressivement (refroidissement)\n",
    "    \n",
    "    Args:\n",
    "        problem: Instance du problème\n",
    "        initial_temp: Température initiale (contrôle l'exploration)\n",
    "        cooling_rate: Taux de refroidissement (0.9 à 0.999)\n",
    "        min_temp: Température minimale (critère d'arrêt)\n",
    "        max_iterations: Nombre maximum d'itérations\n",
    "        seed: Graine aléatoire pour reproductibilité\n",
    "    \n",
    "    Returns:\n",
    "        Solution object\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    if seed is not None:\n",
    "        random.seed(seed)\n",
    "    \n",
    "    n = problem.n\n",
    "    capacity = problem.capacity\n",
    "    items = problem.items\n",
    "    \n",
    "    # === FONCTION D'ÉVALUATION ===\n",
    "    def evaluate(solution):\n",
    "        \"\"\"Calcule valeur et poids d'une solution (liste de 0/1)\"\"\"\n",
    "        total_value = sum(solution[i] * items[i].value for i in range(n))\n",
    "        total_weight = sum(solution[i] * items[i].weight for i in range(n))\n",
    "        return total_value, total_weight\n",
    "    \n",
    "    def fitness(solution):\n",
    "        \"\"\"Fitness avec pénalité si capacité dépassée\"\"\"\n",
    "        value, weight = evaluate(solution)\n",
    "        if weight > capacity:\n",
    "            # Pénalité proportionnelle au dépassement\n",
    "            return value - (weight - capacity) * 10\n",
    "        return value\n",
    "    \n",
    "    # === SOLUTION INITIALE (Greedy par ratio) ===\n",
    "    current_solution = [0] * n\n",
    "    sorted_indices = sorted(range(n), key=lambda i: items[i].ratio(), reverse=True)\n",
    "    current_weight = 0\n",
    "    for idx in sorted_indices:\n",
    "        if current_weight + items[idx].weight <= capacity:\n",
    "            current_solution[idx] = 1\n",
    "            current_weight += items[idx].weight\n",
    "    \n",
    "    current_fitness = fitness(current_solution)\n",
    "    best_solution = current_solution[:]\n",
    "    best_fitness = current_fitness\n",
    "    \n",
    "    # === BOUCLE PRINCIPALE ===\n",
    "    temperature = initial_temp\n",
    "    iteration = 0\n",
    "    \n",
    "    while temperature > min_temp and iteration < max_iterations:\n",
    "        # Générer un voisin en flippant un bit aléatoire\n",
    "        neighbor = current_solution[:]\n",
    "        flip_idx = random.randint(0, n - 1)\n",
    "        neighbor[flip_idx] = 1 - neighbor[flip_idx]\n",
    "        \n",
    "        neighbor_fitness = fitness(neighbor)\n",
    "        \n",
    "        # Calculer la différence d'énergie\n",
    "        delta = neighbor_fitness - current_fitness\n",
    "        \n",
    "        # Décision d'acceptation\n",
    "        if delta > 0:\n",
    "            # Amélioration : toujours accepter\n",
    "            current_solution = neighbor\n",
    "            current_fitness = neighbor_fitness\n",
    "        else:\n",
    "            # Dégradation : accepter avec probabilité exp(delta/T)\n",
    "            acceptance_prob = math.exp(delta / temperature)\n",
    "            if random.random() < acceptance_prob:\n",
    "                current_solution = neighbor\n",
    "                current_fitness = neighbor_fitness\n",
    "        \n",
    "        # Mettre à jour la meilleure solution\n",
    "        if current_fitness > best_fitness:\n",
    "            # Vérifier que la solution est valide\n",
    "            _, weight = evaluate(current_solution)\n",
    "            if weight <= capacity:\n",
    "                best_solution = current_solution[:]\n",
    "                best_fitness = current_fitness\n",
    "        \n",
    "        # Refroidissement\n",
    "        temperature *= cooling_rate\n",
    "        iteration += 1\n",
    "    \n",
    "    # === RÉSULTAT FINAL ===\n",
    "    # S'assurer que la meilleure solution est valide\n",
    "    selected_items = [i for i in range(n) if best_solution[i] == 1]\n",
    "    total_value = sum(items[i].value for i in selected_items)\n",
    "    total_weight = sum(items[i].weight for i in selected_items)\n",
    "    \n",
    "    # Réparer si nécessaire (retirer des items si surpoids)\n",
    "    if total_weight > capacity:\n",
    "        # Trier par ratio croissant et retirer\n",
    "        selected_sorted = sorted(selected_items, key=lambda i: items[i].ratio())\n",
    "        while total_weight > capacity and selected_sorted:\n",
    "            remove_idx = selected_sorted.pop(0)\n",
    "            total_weight -= items[remove_idx].weight\n",
    "            total_value -= items[remove_idx].value\n",
    "            selected_items.remove(remove_idx)\n",
    "    \n",
    "    time_taken = time.time() - start_time\n",
    "    \n",
    "    sol = Solution(selected_items, total_value, total_weight, time_taken)\n",
    "    sol.usage_percent = (total_weight / capacity * 100) if capacity > 0 else 0\n",
    "    sol.iterations = iteration\n",
    "    sol.final_temperature = temperature\n",
    "    \n",
    "    return sol\n",
    "\n",
    "\n",
    "def simulated_annealing_adaptive(problem):\n",
    "    \"\"\"\n",
    "    Version adaptative de Simulated Annealing\n",
    "    Ajuste les paramètres selon la taille du problème\n",
    "    \"\"\"\n",
    "    n = problem.n\n",
    "    \n",
    "    if n <= 50:\n",
    "        return simulated_annealing(problem, initial_temp=500, cooling_rate=0.99, max_iterations=5000)\n",
    "    elif n <= 200:\n",
    "        return simulated_annealing(problem, initial_temp=1000, cooling_rate=0.995, max_iterations=10000)\n",
    "    elif n <= 1000:\n",
    "        return simulated_annealing(problem, initial_temp=2000, cooling_rate=0.997, max_iterations=15000)\n",
    "    else:\n",
    "        return simulated_annealing(problem, initial_temp=5000, cooling_rate=0.999, max_iterations=20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7 FTPAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ftpas(problem, epsilon=0.1):\n",
    "    \"\"\"\n",
    "    Complexité: O(n³/ε)\n",
    "    \n",
    "    Args:\n",
    "        problem: Instance du problème (Problem object)\n",
    "        epsilon: Paramètre d'approximation (0 < ε < 1)\n",
    "                Plus ε est petit, meilleure est l'approximation (mais plus lent)\n",
    "    \n",
    "    Returns:\n",
    "        Solution object\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    n = problem.n\n",
    "    items = problem.items\n",
    "    capacity = problem.capacity\n",
    "    \n",
    "    if epsilon <= 0 or epsilon >= 1:\n",
    "        print(f\"FTPAS: epsilon doit être dans ]0,1[, reçu {epsilon}\")\n",
    "        return None\n",
    "    \n",
    "    v_max = max(item.value for item in items)\n",
    "    \n",
    "    # Facteur de scaling\n",
    "    # K = (ε * v_max) / n\n",
    "    K = (epsilon * v_max) / n\n",
    "    \n",
    "    # Si K trop petit, problèmes numériques\n",
    "    if K < 1e-10:\n",
    "        K = 1e-10\n",
    "    \n",
    "    # Créer les valeurs scalées (arrondi inférieur)\n",
    "    scaled_items = []\n",
    "    for item in items:\n",
    "        scaled_value = math.floor(item.value / K)\n",
    "        scaled_items.append({\n",
    "            'original_idx': item.id,\n",
    "            'weight': item.weight,\n",
    "            'value': item.value,\n",
    "            'scaled_value': scaled_value\n",
    "        })\n",
    "    \n",
    "    V_scaled = sum(si['scaled_value'] for si in scaled_items)\n",
    "    \n",
    "    if V_scaled > 1_000_000:\n",
    "        print(f\"FTPAS Skip: V_scaled trop grand ({V_scaled:,})\")\n",
    "        return None\n",
    "    \n",
    "    # Protection supplémentaire\n",
    "    estimated_mb = (n * V_scaled * 8) / (1024 * 1024)\n",
    "    if estimated_mb > 200:  # Max 200 MB\n",
    "        print(f\"FTPAS Skip: mémoire estimée trop grande ({estimated_mb:.0f} MB)\")\n",
    "        return None\n",
    "    \n",
    "    # DP sur les valeurs scalées\n",
    "    # dp[i][v] = poids minimum pour obtenir exactement la valeur scalée v avec les i premiers items\n",
    "    INF = float('inf')\n",
    "    dp = [[INF for _ in range(int(V_scaled) + 1)] for _ in range(n + 1)]\n",
    "    dp[0][0] = 0\n",
    "    \n",
    "    for i in range(1, n + 1):\n",
    "        si = scaled_items[i - 1]\n",
    "        for v in range(int(V_scaled) + 1):\n",
    "            # Ne pas prendre l'item i\n",
    "            dp[i][v] = dp[i-1][v]\n",
    "            \n",
    "            # Prendre l'item i\n",
    "            if v >= si['scaled_value']:\n",
    "                prev_v = v - si['scaled_value']\n",
    "                if dp[i-1][prev_v] != INF:\n",
    "                    new_weight = dp[i-1][prev_v] + si['weight']\n",
    "                    if new_weight <= capacity:\n",
    "                        dp[i][v] = min(dp[i][v], new_weight)\n",
    "    \n",
    "    best_scaled_value = 0\n",
    "    for v in range(int(V_scaled) + 1):\n",
    "        if dp[n][v] <= capacity:\n",
    "            best_scaled_value = v\n",
    "    \n",
    "    # Reconstruction de la solution\n",
    "    selected = []\n",
    "    v = best_scaled_value\n",
    "    for i in range(n, 0, -1):\n",
    "        if v == 0:\n",
    "            break\n",
    "        si = scaled_items[i - 1]\n",
    "        prev_v = v - si['scaled_value']\n",
    "        if prev_v >= 0 and dp[i-1][prev_v] != INF:\n",
    "            if dp[i][v] == dp[i-1][prev_v] + si['weight']:\n",
    "                selected.append(si['original_idx'])\n",
    "                v = prev_v\n",
    "    \n",
    "    # Calculer la valeur non scalée de la solution\n",
    "    total_value = sum(items[idx].value for idx in selected)\n",
    "    total_weight = sum(items[idx].weight for idx in selected)\n",
    "    \n",
    "    time_taken = time.time() - start_time\n",
    "    \n",
    "    # Créer l'objet Solution\n",
    "    sol = SimpleNamespace(\n",
    "        selected_items=selected,\n",
    "        total_value=total_value,\n",
    "        total_weight=total_weight,\n",
    "        time=time_taken,\n",
    "        usage_percent=(total_weight / capacity * 100) if capacity > 0 else 0,\n",
    "        epsilon=epsilon,\n",
    "        scaling_factor=K\n",
    "    )\n",
    "    \n",
    "    return sol\n",
    "\n",
    "\n",
    "def ftpas_adaptive(problem, time_budget=None):\n",
    "    \"\"\"\n",
    "    Ajuste epsilon selon la taille du problème\n",
    "    \n",
    "    Args:\n",
    "        problem: Instance du problème\n",
    "        time_budget: Budget de temps optionnel (non utilisé pour l'instant)\n",
    "    \n",
    "    Returns:\n",
    "        Solution object\n",
    "    \"\"\"\n",
    "    n = problem.n\n",
    "    \n",
    "    if n <= 50:\n",
    "        epsilon = 0.1\n",
    "    elif n <= 100:\n",
    "        epsilon = 0.2\n",
    "    elif n <= 500:\n",
    "        epsilon = 0.3\n",
    "    else:\n",
    "        epsilon = 0.5  # Très rapide pour grandes instances\n",
    "    \n",
    "    return ftpas(problem, epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Système de Benchmarking\n",
    "\n",
    "### 5.1 Configuration des Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Découvert 103 benchmarks\n",
      "\n",
      "Configuration actuelle:\n",
      "  - large_scale & low_dimension: TOUS les fichiers (y compris n=10000)\n",
      "  - Corrélés (uncorrelated, weakly, strongly): max n=1000\n",
      "\n",
      "Catégories disponibles:\n",
      "  - generated_similar: 9 fichiers (n=[100, 200, 500, 1000])\n",
      "  - generated_strongly: 21 fichiers (n=[100, 500, 1000, 2000, 5000, 10000])\n",
      "  - generated_uncorrelated: 21 fichiers (n=[100, 500, 1000, 2000, 5000, 10000])\n",
      "  - generated_weakly: 21 fichiers (n=[100, 500, 1000, 2000, 5000, 10000])\n",
      "  - large_scale: 21 fichiers (n=[100, 200, 500, 1000, 2000, 5000, 10000])\n",
      "  - low_dimension: 10 fichiers (n=[4, 5, 7, 10, 15, 20, 23])\n"
     ]
    }
   ],
   "source": [
    "MAX_N_CORRELATED = 1000 \n",
    "\n",
    "INCLUDE_CORRELATED = True  # Inclure uncorrelated/weakly/strongly_correlated\n",
    "INCLUDE_GENERATED = True   # Inclure les benchmarks générés (benchmarks/generated/)\n",
    "\n",
    "\n",
    "def discover_benchmarks(base_path='benchmarks', max_n_correlated=None):\n",
    "    \"\"\"\n",
    "    Découvre automatiquement tous les fichiers benchmark disponibles.\n",
    "    Inclut: low_dimension, large_scale, uncorrelated, weakly_correlated, strongly_correlated, generated\n",
    "    \n",
    "    Args:\n",
    "        base_path: Chemin vers le dossier benchmarks\n",
    "        max_n_correlated: Taille max pour les benchmarks corrélés (None = utilise MAX_N_CORRELATED)\n",
    "    \n",
    "    Returns:\n",
    "        dict: Structure contenant les benchmarks organisés par catégorie\n",
    "    \"\"\"\n",
    "    if max_n_correlated is None:\n",
    "        max_n_correlated = MAX_N_CORRELATED\n",
    "        \n",
    "    base = Path(base_path)\n",
    "    \n",
    "    if not base.exists():\n",
    "        print(f\"Dossier '{base_path}' non trouvé\")\n",
    "        return None\n",
    "    \n",
    "    structure = {\n",
    "        'base_path': str(base),\n",
    "        'benchmarks': {}\n",
    "    }\n",
    "    \n",
    "    # Scanner low_dimension et large_scale - SANS limite de taille\n",
    "    simple_categories = ['low_dimension', 'large_scale']\n",
    "    \n",
    "    for category in simple_categories:\n",
    "        category_path = base / category\n",
    "        if not category_path.exists():\n",
    "            continue\n",
    "            \n",
    "        for file_path in category_path.glob('*.txt'):\n",
    "            filename = file_path.name\n",
    "            \n",
    "            if category == 'low_dimension':\n",
    "                parts = filename.replace('.txt', '').split('_')\n",
    "                try:\n",
    "                    n = int(parts[3])\n",
    "                    cap = int(parts[4])\n",
    "                except (IndexError, ValueError):\n",
    "                    continue\n",
    "            else:\n",
    "                parts = filename.replace('.txt', '').split('_')\n",
    "                try:\n",
    "                    n = int(parts[2])\n",
    "                    cap = int(parts[3])\n",
    "                except (IndexError, ValueError):\n",
    "                    continue\n",
    "            \n",
    "            # PAS de filtre de taille pour large_scale et low_dimension\n",
    "            key = f\"{category}_{filename}\"\n",
    "            structure['benchmarks'][key] = {\n",
    "                'path': str(file_path),\n",
    "                'correlation': category,\n",
    "                'size': f\"n={n}\",\n",
    "                'capacity': f\"c={cap}\",\n",
    "                'n': n,\n",
    "                'capacity_value': cap,\n",
    "                'format': 'standard'\n",
    "            }\n",
    "    \n",
    "    # Scanner uncorrelated, weakly_correlated, strongly_correlated - AVEC limite de taille\n",
    "    if INCLUDE_CORRELATED:\n",
    "        correlated_categories = ['uncorrelated', 'weakly_correlated', 'strongly_correlated']\n",
    "        \n",
    "        for category in correlated_categories:\n",
    "            category_path = base / category\n",
    "            if not category_path.exists():\n",
    "                continue\n",
    "            \n",
    "            for file_path in category_path.glob('**/*.kp'):\n",
    "                path_parts = file_path.relative_to(category_path).parts\n",
    "                \n",
    "                n = None\n",
    "                r_value = None\n",
    "                \n",
    "                for part in path_parts:\n",
    "                    if part.startswith('n') and part[1:].isdigit():\n",
    "                        n = int(part[1:])\n",
    "                    elif part.startswith('r') and part[1:].isdigit():\n",
    "                        r_value = int(part[1:])\n",
    "                \n",
    "                if n is None:\n",
    "                    continue\n",
    "                \n",
    "                # Filtrer par taille maximale SEULEMENT pour les corrélés\n",
    "                if n > max_n_correlated:\n",
    "                    continue\n",
    "                \n",
    "                try:\n",
    "                    with open(file_path, 'r') as f:\n",
    "                        lines = f.readlines()\n",
    "                        cap = int(lines[2].strip()) if len(lines) > 2 else 0\n",
    "                except:\n",
    "                    cap = r_value if r_value else 0\n",
    "                \n",
    "                filename = file_path.name\n",
    "                key = f\"{category}_{'/'.join(path_parts)}\"\n",
    "                \n",
    "                structure['benchmarks'][key] = {\n",
    "                    'path': str(file_path),\n",
    "                    'correlation': category,\n",
    "                    'size': f\"n={n}\",\n",
    "                    'capacity': f\"c={cap}\",\n",
    "                    'n': n,\n",
    "                    'capacity_value': cap,\n",
    "                    'format': 'kp'\n",
    "                }\n",
    "    \n",
    "    # Scanner generated/ - fichiers .txt générés par generate_benchmarks()\n",
    "    if INCLUDE_GENERATED:\n",
    "        generated_path = base / 'generated'\n",
    "        if generated_path.exists():\n",
    "            for file_path in generated_path.glob('*.txt'):\n",
    "                filename = file_path.name\n",
    "                # Format: {correlation}_n{n}_c{capacity}.txt ou {correlation}_n{n}_c{capacity}_{index}.txt\n",
    "                parts = filename.replace('.txt', '').split('_')\n",
    "                try:\n",
    "                    # Trouver n et c dans le nom\n",
    "                    correlation = parts[0]\n",
    "                    n = None\n",
    "                    cap = None\n",
    "                    for part in parts:\n",
    "                        if part.startswith('n') and part[1:].isdigit():\n",
    "                            n = int(part[1:])\n",
    "                        elif part.startswith('c') and part[1:].isdigit():\n",
    "                            cap = int(part[1:])\n",
    "                    \n",
    "                    if n is None or cap is None:\n",
    "                        continue\n",
    "                    \n",
    "                    key = f\"generated_{filename}\"\n",
    "                    structure['benchmarks'][key] = {\n",
    "                        'path': str(file_path),\n",
    "                        'correlation': f\"generated_{correlation}\",\n",
    "                        'size': f\"n={n}\",\n",
    "                        'capacity': f\"c={cap}\",\n",
    "                        'n': n,\n",
    "                        'capacity_value': cap,\n",
    "                        'format': 'standard'\n",
    "                    }\n",
    "                except (IndexError, ValueError):\n",
    "                    continue\n",
    "    \n",
    "    print(f\"Découvert {len(structure['benchmarks'])} benchmarks\")\n",
    "    return structure\n",
    "\n",
    "\n",
    "def parse_benchmark_file(filepath):\n",
    "    \"\"\"Parse un fichier benchmark .txt ou .kp\n",
    "    \n",
    "    Format .txt (standard): \n",
    "    - Ligne 1: n capacity (séparés par espace)\n",
    "    - Lignes suivantes: value weight (profit puis poids)\n",
    "    \n",
    "    Format .kp:\n",
    "    - Ligne 1: vide ou commentaire\n",
    "    - Ligne 2: n\n",
    "    - Ligne 3: capacity\n",
    "    - Lignes suivantes: value weight\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(filepath, 'r') as f:\n",
    "            lines = [line.strip() for line in f.readlines()]\n",
    "        \n",
    "        if filepath.endswith('.kp'):\n",
    "            start_idx = 0\n",
    "            while start_idx < len(lines) and not lines[start_idx]:\n",
    "                start_idx += 1\n",
    "            \n",
    "            n = int(lines[start_idx])\n",
    "            capacity = int(lines[start_idx + 1])\n",
    "            \n",
    "            items = []\n",
    "            for i in range(n):\n",
    "                line_idx = start_idx + 2 + i\n",
    "                if line_idx >= len(lines):\n",
    "                    break\n",
    "                parts = lines[line_idx].split()\n",
    "                if len(parts) >= 2:\n",
    "                    value = int(parts[0])\n",
    "                    weight = int(parts[1])\n",
    "                    items.append(Item(i, weight, value))\n",
    "        else:\n",
    "            first_line_parts = lines[0].split()\n",
    "            n = int(first_line_parts[0])\n",
    "            capacity = int(first_line_parts[1])\n",
    "            \n",
    "            items = []\n",
    "            for i in range(n):\n",
    "                parts = lines[1 + i].split()\n",
    "                value = int(parts[0])\n",
    "                weight = int(parts[1])\n",
    "                items.append(Item(i, weight, value))\n",
    "        \n",
    "        return Problem(items, capacity)\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur parsing {filepath}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Initialiser la structure des benchmarks\n",
    "BENCHMARK_STRUCTURE = discover_benchmarks()\n",
    "\n",
    "if BENCHMARK_STRUCTURE:\n",
    "    print(f\"\\nConfiguration actuelle:\")\n",
    "    print(f\"  - large_scale & low_dimension: TOUS les fichiers (y compris n=10000)\")\n",
    "    print(f\"  - Corrélés (uncorrelated, weakly, strongly): max n={MAX_N_CORRELATED}\")\n",
    "    print(f\"\\nCatégories disponibles:\")\n",
    "    categories = {}\n",
    "    sizes_by_cat = {}\n",
    "    for key, info in BENCHMARK_STRUCTURE['benchmarks'].items():\n",
    "        cat = info['correlation']\n",
    "        if cat not in categories:\n",
    "            categories[cat] = 0\n",
    "            sizes_by_cat[cat] = set()\n",
    "        categories[cat] += 1\n",
    "        sizes_by_cat[cat].add(info['n'])\n",
    "    for cat, count in sorted(categories.items()):\n",
    "        sizes = sorted(sizes_by_cat[cat])\n",
    "        print(f\"  - {cat}: {count} fichiers (n={sizes})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition des algorithmes à tester\n",
    "ALL_ALGORITHMS = [\n",
    "    ('Brute Force', brute_force, 20),\n",
    "    ('Dynamic Programming', dynamic_programming, 1000),\n",
    "    ('DP Top-Down', dynamic_programming_topdown, 1000),\n",
    "    ('Branch and Bound', branch_and_bound, 200),\n",
    "    ('Greedy Ratio', greedy_by_ratio, float('inf')),\n",
    "    ('Greedy Value', greedy_by_value, float('inf')),\n",
    "    ('Greedy Weight', greedy_by_weight, float('inf')),\n",
    "    ('Fractional Knapsack', fractional_knapsack, float('inf')),\n",
    "    ('Randomized', lambda p: randomized_approach(p, iterations=100), float('inf')),\n",
    "    ('Genetic Algorithm', lambda p: genetic_algorithm(p, population_size=100, generations=50), float('inf')),\n",
    "    ('Genetic Adaptive', genetic_algorithm_adaptive, float('inf')),\n",
    "    ('Simulated Annealing', lambda p: simulated_annealing(p, initial_temp=1000, cooling_rate=0.995), float('inf')),\n",
    "    ('SA Adaptive', simulated_annealing_adaptive, float('inf')),\n",
    "    ('FTPAS (ε=0.1)', lambda p: ftpas(p, epsilon=0.1), float('inf')),\n",
    "    ('FTPAS (ε=0.05)', lambda p: ftpas(p, epsilon=0.05), float('inf')),\n",
    "    ('FTPAS Adaptive', ftpas_adaptive, float('inf')),\n",
    "]\n",
    "\n",
    "\n",
    "def should_run_algorithm(algo_name, n, max_n, correlation=None):\n",
    "    \"\"\"Détermine si un algorithme doit être exécuté selon la taille et la corrélation\"\"\"\n",
    "    # Brute Force uniquement sur low_dimension\n",
    "    if algo_name == 'Brute Force' and correlation != 'low_dimension':\n",
    "        return False\n",
    "    return n <= max_n\n",
    "\n",
    "\n",
    "def run_benchmark(benchmark_info, algorithms=None, timeout=300):\n",
    "    \"\"\"\n",
    "    Exécute un benchmark sur un fichier.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Résultats pour chaque algorithme\n",
    "    \"\"\"\n",
    "    if algorithms is None:\n",
    "        algorithms = ALL_ALGORITHMS\n",
    "    \n",
    "    problem = parse_benchmark_file(benchmark_info['path'])\n",
    "    if problem is None:\n",
    "        return None\n",
    "    \n",
    "    results = {\n",
    "        'info': benchmark_info,\n",
    "        'n': problem.n,\n",
    "        'capacity': problem.capacity,\n",
    "        'algorithms': {}\n",
    "    }\n",
    "    \n",
    "    for algo_name, algo_func, max_n in algorithms:\n",
    "        if not should_run_algorithm(algo_name, problem.n, max_n):\n",
    "            results['algorithms'][algo_name] = {'skipped': True, 'reason': f'n={problem.n} > max_n={max_n}'}\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            #GESTION DES ERREURS\n",
    "            start = time.time()\n",
    "            sol = algo_func(problem)\n",
    "            elapsed = time.time() - start\n",
    "            \n",
    "            # Vérifier timeout\n",
    "            if elapsed > timeout:\n",
    "                results['algorithms'][algo_name] = {\n",
    "                    'skipped': True,\n",
    "                    'reason': f'timeout (>{timeout}s)'\n",
    "                }\n",
    "                print(f\"{algo_name}: timeout ({elapsed:.1f}s)\")\n",
    "                continue\n",
    "            \n",
    "            # Vérifier si algo a retourné None (protection interne)\n",
    "            if sol is None:\n",
    "                results['algorithms'][algo_name] = {\n",
    "                    'skipped': True,\n",
    "                    'reason': 'protection_triggered'\n",
    "                }\n",
    "                continue\n",
    "            # FIN GESTION ERREURS\n",
    "            \n",
    "            results['algorithms'][algo_name] = {\n",
    "                'value': sol.total_value,\n",
    "                'weight': sol.total_weight,\n",
    "                'time': sol.time,\n",
    "                'usage': sol.usage_percent,\n",
    "                'items_count': len(sol.selected_items),\n",
    "                'skipped': False\n",
    "            }\n",
    "            \n",
    "        except KeyboardInterrupt:\n",
    "            print(f\"\\nInterruption manuelle sur {algo_name}\")\n",
    "            results['algorithms'][algo_name] = {\n",
    "                'skipped': True,\n",
    "                'reason': 'interrupted'\n",
    "            }\n",
    "            continue  # Continuer avec les autres algorithmes\n",
    "            \n",
    "        except MemoryError:\n",
    "            results['algorithms'][algo_name] = {\n",
    "                'skipped': True,\n",
    "                'reason': 'memory_error'\n",
    "            }\n",
    "            print(f\"{algo_name}: Mémoire insuffisante\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            results['algorithms'][algo_name] = {\n",
    "                'skipped': True,\n",
    "                'reason': str(e)\n",
    "            }\n",
    "            print(f\"{algo_name}: {str(e)}\")\n",
    "\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Exécution Complète des Benchmarks\n",
    "\n",
    "**Attention:** Cette cellule peut prendre plusieurs minutes selon le nombre de benchmarks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_all_benchmarks():\n",
    "    \"\"\"\n",
    "    Exécute tous les benchmarks disponibles.\n",
    "    Sauvegarde UNIQUEMENT à la fin, pas de sauvegarde partielle.\n",
    "    Continue même en cas d'erreur sur un algorithme ou un benchmark.\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame avec tous les résultats\n",
    "    \"\"\"\n",
    "    if BENCHMARK_STRUCTURE is None:\n",
    "        print(\"Aucun benchmark disponible\")\n",
    "        return None\n",
    "    \n",
    "    all_results = []\n",
    "    total = len(BENCHMARK_STRUCTURE['benchmarks'])\n",
    "    \n",
    "    print(f\"Exécution de {total} benchmarks...\")    \n",
    "    for i, (key, bench_info) in enumerate(BENCHMARK_STRUCTURE['benchmarks'].items(), 1):\n",
    "        print(f\"\\n[{i}/{total}] {bench_info['correlation']} | {bench_info['size']} | {bench_info['capacity']}\")\n",
    "        \n",
    "        try:\n",
    "            # Parser le problème\n",
    "            problem = parse_benchmark_file(bench_info['path'])\n",
    "            if problem is None:\n",
    "                print(f\"  ERREUR: Impossible de parser ce benchmark, skip\")\n",
    "                continue\n",
    "            \n",
    "            # Informations sur le problème\n",
    "            print(f\"  n={problem.n}, capacity={problem.capacity}\")\n",
    "            \n",
    "            # Tester chaque algorithme\n",
    "            for algo_name, algo_func, max_n in ALL_ALGORITHMS:\n",
    "                # Vérifier si on doit exécuter cet algo\n",
    "                if not should_run_algorithm(algo_name, problem.n, max_n, bench_info['correlation']):\n",
    "                    if algo_name == 'Brute Force' and bench_info['correlation'] != 'low_dimension':\n",
    "                        print(f\"  SKIP {algo_name}: uniquement sur low_dimension\")\n",
    "                    else:\n",
    "                        print(f\"  SKIP {algo_name}: n={problem.n} > max_n={max_n}\")\n",
    "                    continue\n",
    "                \n",
    "                try:\n",
    "                    # Exécuter l'algorithme\n",
    "                    start_algo = time.time()\n",
    "                    sol = algo_func(problem)\n",
    "                    elapsed = time.time() - start_algo\n",
    "                    \n",
    "                    # Si l'algo prend plus de 5 minutes, on le note mais on garde le résultat\n",
    "                    if elapsed > 300:\n",
    "                        print(f\"  WARNING {algo_name}: temps très long ({elapsed:.1f}s)\")\n",
    "                    \n",
    "                    # Vérifier si l'algo a retourné None (protection interne)\n",
    "                    if sol is None:\n",
    "                        print(f\"  SKIP {algo_name}: protection déclenchée\")\n",
    "                        continue\n",
    "                    \n",
    "                    # Enregistrer le résultat\n",
    "                    row = {\n",
    "                        'correlation': bench_info['correlation'],\n",
    "                        'n': problem.n,\n",
    "                        'capacity_type': bench_info['capacity'],\n",
    "                        'capacity_value': problem.capacity,\n",
    "                        'algorithm': algo_name,\n",
    "                        'value': sol.total_value,\n",
    "                        'time_ms': sol.time * 1000,\n",
    "                        'usage_percent': sol.usage_percent,\n",
    "                        'items_selected': len(sol.selected_items)\n",
    "                    }\n",
    "                    all_results.append(row)\n",
    "                                    \n",
    "                except MemoryError:\n",
    "                    print(f\"  ERROR {algo_name}: Erreur mémoire\")\n",
    "                    continue\n",
    "                \n",
    "                except KeyboardInterrupt:\n",
    "                    print(f\"\\n\\nINTERRUPTION MANUELLE DÉTECTÉE\")\n",
    "                    print(f\"Résultats collectés jusqu'ici: {len(all_results)} lignes\")\n",
    "                    \n",
    "                    if len(all_results) > 0:\n",
    "                        df = pd.DataFrame(all_results)\n",
    "                        df.to_csv('benchmark_results_interrupted.csv', index=False)\n",
    "                        print(\"Sauvegarde d'urgence: 'benchmark_results_interrupted.csv'\")\n",
    "                        return df\n",
    "                    else:\n",
    "                        print(\"Aucun résultat à sauvegarder\")\n",
    "                        return None\n",
    "                \n",
    "                except Exception as e:\n",
    "                    print(f\"  ERROR {algo_name}: {str(e)}\")\n",
    "                    continue\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"  ERROR sur ce benchmark: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    # FIN DE TOUS LES BENCHMARKS\n",
    "    print(f\"\\n\\n{'='*60}\")\n",
    "    print(f\"TERMINÉ ! {len(all_results)} résultats collectés\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    if len(all_results) == 0:\n",
    "        print(\"ATTENTION: Aucun résultat collecté\")\n",
    "        return None\n",
    "    \n",
    "    # Créer le DataFrame final\n",
    "    df = pd.DataFrame(all_results)\n",
    "    \n",
    "    # Sauvegarder\n",
    "    try:\n",
    "        df.to_csv('benchmark_results.csv', index=False)\n",
    "        print(\"Résultats sauvegardés: 'benchmark_results.csv'\")\n",
    "        \n",
    "        # Afficher un résumé\n",
    "        print(f\"\\nRésumé:\")\n",
    "        print(f\"  - {df['algorithm'].nunique()} algorithmes testés\")\n",
    "        print(f\"  - {df['n'].nunique()} tailles différentes\")\n",
    "        print(f\"  - {df['correlation'].nunique()} types de corrélation\")\n",
    "        print(f\"\\nAperçu des résultats:\")\n",
    "        print(df.groupby('algorithm')['value'].agg(['count', 'mean', 'std']))\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"ERROR lors de la sauvegarde: {e}\")\n",
    "        print(\"Le DataFrame est quand même retourné\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Chargement des Résultats\n",
    "\n",
    "Si les benchmarks ont déjà été exécutés, charger les résultats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exécution de 103 benchmarks...\n",
      "\n",
      "[1/103] low_dimension | n=20 | c=879\n",
      "  n=20, capacity=879\n",
      "\n",
      "[2/103] low_dimension | n=10 | c=269\n",
      "  n=10, capacity=269\n",
      "\n",
      "[3/103] low_dimension | n=20 | c=878\n",
      "  n=20, capacity=878\n",
      "\n",
      "[4/103] low_dimension | n=4 | c=20\n",
      "  n=4, capacity=20\n",
      "\n",
      "[5/103] low_dimension | n=4 | c=11\n",
      "  n=4, capacity=11\n",
      "\n",
      "[6/103] low_dimension | n=15 | c=375\n",
      "Erreur parsing benchmarks\\low_dimension\\f5_l-d_kp_15_375.txt: invalid literal for int() with base 10: '0.125126'\n",
      "  ERREUR: Impossible de parser ce benchmark, skip\n",
      "\n",
      "[7/103] low_dimension | n=10 | c=60\n",
      "  n=10, capacity=60\n",
      "\n",
      "[8/103] low_dimension | n=7 | c=50\n",
      "  n=7, capacity=50\n",
      "\n",
      "[9/103] low_dimension | n=23 | c=10000\n",
      "  n=23, capacity=10000\n",
      "  SKIP Brute Force: n=23 > max_n=20\n",
      "\n",
      "[10/103] low_dimension | n=5 | c=80\n",
      "  n=5, capacity=80\n",
      "\n",
      "[11/103] large_scale | n=10000 | c=1000\n",
      "  n=10000, capacity=49877\n",
      "  SKIP Brute Force: uniquement sur low_dimension\n",
      "  SKIP Dynamic Programming: n=10000 > max_n=1000\n",
      "  SKIP DP Top-Down: n=10000 > max_n=1000\n",
      "  SKIP Branch and Bound: n=10000 > max_n=200\n",
      "FTPAS Skip: V_scaled trop grand (497,906,700)\n",
      "  SKIP FTPAS (ε=0.1): protection déclenchée\n",
      "FTPAS Skip: V_scaled trop grand (995,813,400)\n",
      "  SKIP FTPAS (ε=0.05): protection déclenchée\n",
      "FTPAS Skip: V_scaled trop grand (99,581,340)\n",
      "  SKIP FTPAS Adaptive: protection déclenchée\n",
      "\n",
      "[12/103] large_scale | n=1000 | c=1000\n",
      "  n=1000, capacity=5002\n",
      "  SKIP Brute Force: uniquement sur low_dimension\n",
      "  SKIP Branch and Bound: n=1000 > max_n=200\n",
      "FTPAS Skip: V_scaled trop grand (4,874,289)\n",
      "  SKIP FTPAS (ε=0.1): protection déclenchée\n",
      "FTPAS Skip: V_scaled trop grand (9,749,086)\n",
      "  SKIP FTPAS (ε=0.05): protection déclenchée\n",
      "FTPAS Skip: mémoire estimée trop grande (7434 MB)\n",
      "  SKIP FTPAS Adaptive: protection déclenchée\n",
      "\n",
      "[13/103] large_scale | n=100 | c=1000\n",
      "  n=100, capacity=995\n",
      "  SKIP Brute Force: uniquement sur low_dimension\n",
      "\n",
      "[14/103] large_scale | n=2000 | c=1000\n",
      "  n=2000, capacity=10011\n",
      "  SKIP Brute Force: uniquement sur low_dimension\n",
      "  SKIP Dynamic Programming: n=2000 > max_n=1000\n",
      "  SKIP DP Top-Down: n=2000 > max_n=1000\n",
      "  SKIP Branch and Bound: n=2000 > max_n=200\n",
      "FTPAS Skip: V_scaled trop grand (19,554,740)\n",
      "  SKIP FTPAS (ε=0.1): protection déclenchée\n",
      "FTPAS Skip: V_scaled trop grand (39,109,480)\n",
      "  SKIP FTPAS (ε=0.05): protection déclenchée\n",
      "FTPAS Skip: V_scaled trop grand (3,910,948)\n",
      "  SKIP FTPAS Adaptive: protection déclenchée\n",
      "\n",
      "[15/103] large_scale | n=200 | c=1000\n",
      "  n=200, capacity=1008\n",
      "  SKIP Brute Force: uniquement sur low_dimension\n",
      "FTPAS Skip: mémoire estimée trop grande (305 MB)\n",
      "  SKIP FTPAS (ε=0.1): protection déclenchée\n",
      "FTPAS Skip: mémoire estimée trop grande (610 MB)\n",
      "  SKIP FTPAS (ε=0.05): protection déclenchée\n",
      "\n",
      "[16/103] large_scale | n=5000 | c=1000\n",
      "  n=5000, capacity=25016\n",
      "  SKIP Brute Force: uniquement sur low_dimension\n",
      "  SKIP Dynamic Programming: n=5000 > max_n=1000\n",
      "  SKIP DP Top-Down: n=5000 > max_n=1000\n",
      "  SKIP Branch and Bound: n=5000 > max_n=200\n",
      "FTPAS Skip: V_scaled trop grand (123,738,600)\n",
      "  SKIP FTPAS (ε=0.1): protection déclenchée\n",
      "FTPAS Skip: V_scaled trop grand (247,477,200)\n",
      "  SKIP FTPAS (ε=0.05): protection déclenchée\n",
      "FTPAS Skip: V_scaled trop grand (24,747,720)\n",
      "  SKIP FTPAS Adaptive: protection déclenchée\n",
      "\n",
      "[17/103] large_scale | n=500 | c=1000\n",
      "  n=500, capacity=2543\n",
      "  SKIP Brute Force: uniquement sur low_dimension\n",
      "  SKIP Branch and Bound: n=500 > max_n=200\n",
      "FTPAS Skip: V_scaled trop grand (1,237,583)\n",
      "  SKIP FTPAS (ε=0.1): protection déclenchée\n",
      "FTPAS Skip: V_scaled trop grand (2,475,405)\n",
      "  SKIP FTPAS (ε=0.05): protection déclenchée\n",
      "FTPAS Skip: mémoire estimée trop grande (1573 MB)\n",
      "  SKIP FTPAS Adaptive: protection déclenchée\n",
      "\n",
      "[18/103] large_scale | n=10000 | c=1000\n",
      "  n=10000, capacity=49877\n",
      "  SKIP Brute Force: uniquement sur low_dimension\n",
      "  SKIP Dynamic Programming: n=10000 > max_n=1000\n",
      "  SKIP DP Top-Down: n=10000 > max_n=1000\n",
      "  SKIP Branch and Bound: n=10000 > max_n=200\n",
      "FTPAS Skip: V_scaled trop grand (459,707,978)\n",
      "  SKIP FTPAS (ε=0.1): protection déclenchée\n",
      "FTPAS Skip: V_scaled trop grand (919,420,627)\n",
      "  SKIP FTPAS (ε=0.05): protection déclenchée\n",
      "FTPAS Skip: V_scaled trop grand (91,938,045)\n",
      "  SKIP FTPAS Adaptive: protection déclenchée\n",
      "\n",
      "[19/103] large_scale | n=1000 | c=1000\n",
      "  n=1000, capacity=5002\n",
      "  SKIP Brute Force: uniquement sur low_dimension\n",
      "  SKIP Branch and Bound: n=1000 > max_n=200\n",
      "FTPAS Skip: V_scaled trop grand (4,655,315)\n",
      "  SKIP FTPAS (ε=0.1): protection déclenchée\n",
      "FTPAS Skip: V_scaled trop grand (9,311,137)\n",
      "  SKIP FTPAS (ε=0.05): protection déclenchée\n",
      "FTPAS Skip: mémoire estimée trop grande (7100 MB)\n",
      "  SKIP FTPAS Adaptive: protection déclenchée\n",
      "\n",
      "[20/103] large_scale | n=100 | c=1000\n",
      "  n=100, capacity=995\n",
      "  SKIP Brute Force: uniquement sur low_dimension\n",
      "\n",
      "[21/103] large_scale | n=2000 | c=1000\n",
      "  n=2000, capacity=10011\n",
      "  SKIP Brute Force: uniquement sur low_dimension\n",
      "  SKIP Dynamic Programming: n=2000 > max_n=1000\n",
      "  SKIP DP Top-Down: n=2000 > max_n=1000\n",
      "  SKIP Branch and Bound: n=2000 > max_n=200\n",
      "FTPAS Skip: V_scaled trop grand (18,608,683)\n",
      "  SKIP FTPAS (ε=0.1): protection déclenchée\n",
      "FTPAS Skip: V_scaled trop grand (37,218,378)\n",
      "  SKIP FTPAS (ε=0.05): protection déclenchée\n",
      "FTPAS Skip: V_scaled trop grand (3,720,914)\n",
      "  SKIP FTPAS Adaptive: protection déclenchée\n",
      "\n",
      "[22/103] large_scale | n=200 | c=1000\n",
      "  n=200, capacity=1008\n",
      "  SKIP Brute Force: uniquement sur low_dimension\n",
      "FTPAS Skip: mémoire estimée trop grande (293 MB)\n",
      "  SKIP FTPAS (ε=0.1): protection déclenchée\n",
      "FTPAS Skip: mémoire estimée trop grande (587 MB)\n",
      "  SKIP FTPAS (ε=0.05): protection déclenchée\n",
      "\n",
      "[23/103] large_scale | n=5000 | c=1000\n",
      "  n=5000, capacity=25016\n",
      "  SKIP Brute Force: uniquement sur low_dimension\n",
      "  SKIP Dynamic Programming: n=5000 > max_n=1000\n",
      "  SKIP DP Top-Down: n=5000 > max_n=1000\n",
      "  SKIP Branch and Bound: n=5000 > max_n=200\n",
      "FTPAS Skip: V_scaled trop grand (116,152,367)\n",
      "  SKIP FTPAS (ε=0.1): protection déclenchée\n",
      "FTPAS Skip: V_scaled trop grand (232,307,365)\n",
      "  SKIP FTPAS (ε=0.05): protection déclenchée\n",
      "FTPAS Skip: V_scaled trop grand (23,228,487)\n",
      "  SKIP FTPAS Adaptive: protection déclenchée\n",
      "\n",
      "[24/103] large_scale | n=500 | c=1000\n",
      "  n=500, capacity=2543\n",
      "  SKIP Brute Force: uniquement sur low_dimension\n",
      "  SKIP Branch and Bound: n=500 > max_n=200\n",
      "FTPAS Skip: V_scaled trop grand (1,202,340)\n",
      "  SKIP FTPAS (ε=0.1): protection déclenchée\n",
      "FTPAS Skip: V_scaled trop grand (2,404,944)\n",
      "  SKIP FTPAS (ε=0.05): protection déclenchée\n",
      "FTPAS Skip: mémoire estimée trop grande (1528 MB)\n",
      "  SKIP FTPAS Adaptive: protection déclenchée\n",
      "\n",
      "[25/103] large_scale | n=10000 | c=1000\n",
      "  n=10000, capacity=49519\n",
      "  SKIP Brute Force: uniquement sur low_dimension\n",
      "  SKIP Dynamic Programming: n=10000 > max_n=1000\n",
      "  SKIP DP Top-Down: n=10000 > max_n=1000\n",
      "  SKIP Branch and Bound: n=10000 > max_n=200\n",
      "FTPAS Skip: V_scaled trop grand (545,578,976)\n",
      "  SKIP FTPAS (ε=0.1): protection déclenchée\n",
      "FTPAS Skip: V_scaled trop grand (1,091,162,497)\n",
      "  SKIP FTPAS (ε=0.05): protection déclenchée\n",
      "FTPAS Skip: V_scaled trop grand (109,112,160)\n",
      "  SKIP FTPAS Adaptive: protection déclenchée\n",
      "\n",
      "[26/103] large_scale | n=1000 | c=1000\n",
      "  n=1000, capacity=4990\n",
      "  SKIP Brute Force: uniquement sur low_dimension\n",
      "  SKIP Branch and Bound: n=1000 > max_n=200\n",
      "FTPAS Skip: V_scaled trop grand (5,500,440)\n",
      "  SKIP FTPAS (ε=0.1): protection déclenchée\n",
      "FTPAS Skip: V_scaled trop grand (11,001,362)\n",
      "  SKIP FTPAS (ε=0.05): protection déclenchée\n",
      "FTPAS Skip: V_scaled trop grand (1,099,678)\n",
      "  SKIP FTPAS Adaptive: protection déclenchée\n",
      "\n",
      "[27/103] large_scale | n=100 | c=1000\n",
      "  n=100, capacity=997\n",
      "  SKIP Brute Force: uniquement sur low_dimension\n",
      "\n",
      "[28/103] large_scale | n=2000 | c=1000\n",
      "  n=2000, capacity=9819\n",
      "  SKIP Brute Force: uniquement sur low_dimension\n",
      "  SKIP Dynamic Programming: n=2000 > max_n=1000\n",
      "  SKIP DP Top-Down: n=2000 > max_n=1000\n",
      "  SKIP Branch and Bound: n=2000 > max_n=200\n",
      "FTPAS Skip: V_scaled trop grand (21,668,078)\n",
      "  SKIP FTPAS (ε=0.1): protection déclenchée\n",
      "FTPAS Skip: V_scaled trop grand (43,337,061)\n",
      "  SKIP FTPAS (ε=0.05): protection déclenchée\n",
      "FTPAS Skip: V_scaled trop grand (4,332,764)\n",
      "  SKIP FTPAS Adaptive: protection déclenchée\n",
      "\n",
      "[29/103] large_scale | n=200 | c=1000\n",
      "  n=200, capacity=997\n",
      "  SKIP Brute Force: uniquement sur low_dimension\n",
      "FTPAS Skip: mémoire estimée trop grande (335 MB)\n",
      "  SKIP FTPAS (ε=0.1): protection déclenchée\n",
      "FTPAS Skip: mémoire estimée trop grande (670 MB)\n",
      "  SKIP FTPAS (ε=0.05): protection déclenchée\n",
      "\n",
      "[30/103] large_scale | n=5000 | c=1000\n",
      "  n=5000, capacity=24805\n",
      "  SKIP Brute Force: uniquement sur low_dimension\n",
      "  SKIP Dynamic Programming: n=5000 > max_n=1000\n",
      "  SKIP DP Top-Down: n=5000 > max_n=1000\n",
      "  SKIP Branch and Bound: n=5000 > max_n=200\n",
      "FTPAS Skip: V_scaled trop grand (136,606,720)\n",
      "  SKIP FTPAS (ε=0.1): protection déclenchée\n",
      "FTPAS Skip: V_scaled trop grand (273,215,722)\n",
      "  SKIP FTPAS (ε=0.05): protection déclenchée\n",
      "FTPAS Skip: V_scaled trop grand (27,319,506)\n",
      "  SKIP FTPAS Adaptive: protection déclenchée\n",
      "\n",
      "[31/103] large_scale | n=500 | c=1000\n",
      "  n=500, capacity=2517\n",
      "  SKIP Brute Force: uniquement sur low_dimension\n",
      "  SKIP Branch and Bound: n=500 > max_n=200\n",
      "FTPAS Skip: V_scaled trop grand (1,385,469)\n",
      "  SKIP FTPAS (ε=0.1): protection déclenchée\n",
      "FTPAS Skip: V_scaled trop grand (2,771,201)\n",
      "  SKIP FTPAS (ε=0.05): protection déclenchée\n",
      "FTPAS Skip: mémoire estimée trop grande (1761 MB)\n",
      "  SKIP FTPAS Adaptive: protection déclenchée\n",
      "\n",
      "[32/103] generated_similar | n=1000 | c=2000\n",
      "  n=1000, capacity=2000\n",
      "  SKIP Brute Force: uniquement sur low_dimension\n",
      "  SKIP Branch and Bound: n=1000 > max_n=200\n",
      "FTPAS Skip: V_scaled trop grand (4,946,590)\n",
      "  SKIP FTPAS (ε=0.1): protection déclenchée\n",
      "FTPAS Skip: V_scaled trop grand (9,893,180)\n",
      "  SKIP FTPAS (ε=0.05): protection déclenchée\n",
      "FTPAS Skip: mémoire estimée trop grande (7548 MB)\n",
      "  SKIP FTPAS Adaptive: protection déclenchée\n",
      "\n",
      "[33/103] generated_similar | n=1000 | c=2000\n",
      "  n=1000, capacity=2000\n",
      "  SKIP Brute Force: uniquement sur low_dimension\n",
      "  SKIP Branch and Bound: n=1000 > max_n=200\n",
      "FTPAS Skip: V_scaled trop grand (4,950,060)\n",
      "  SKIP FTPAS (ε=0.1): protection déclenchée\n",
      "FTPAS Skip: V_scaled trop grand (9,900,120)\n",
      "  SKIP FTPAS (ε=0.05): protection déclenchée\n",
      "FTPAS Skip: mémoire estimée trop grande (7553 MB)\n",
      "  SKIP FTPAS Adaptive: protection déclenchée\n",
      "\n",
      "[34/103] generated_similar | n=100 | c=2000\n",
      "  n=100, capacity=2000\n",
      "  SKIP Brute Force: uniquement sur low_dimension\n",
      "\n",
      "[35/103] generated_similar | n=100 | c=2000\n",
      "  n=100, capacity=2000\n",
      "  SKIP Brute Force: uniquement sur low_dimension\n",
      "\n",
      "[36/103] generated_similar | n=100 | c=2000\n",
      "  n=100, capacity=2000\n",
      "  SKIP Brute Force: uniquement sur low_dimension\n",
      "\n",
      "[37/103] generated_similar | n=200 | c=2000\n",
      "  n=200, capacity=2000\n",
      "  SKIP Brute Force: uniquement sur low_dimension\n",
      "FTPAS Skip: mémoire estimée trop grande (307 MB)\n",
      "  SKIP FTPAS (ε=0.1): protection déclenchée\n",
      "FTPAS Skip: mémoire estimée trop grande (615 MB)\n",
      "  SKIP FTPAS (ε=0.05): protection déclenchée\n",
      "\n",
      "[38/103] generated_similar | n=200 | c=2000\n",
      "  n=200, capacity=2000\n",
      "  SKIP Brute Force: uniquement sur low_dimension\n",
      "FTPAS Skip: mémoire estimée trop grande (330 MB)\n",
      "  SKIP FTPAS (ε=0.1): protection déclenchée\n",
      "FTPAS Skip: mémoire estimée trop grande (659 MB)\n",
      "  SKIP FTPAS (ε=0.05): protection déclenchée\n",
      "\n",
      "[39/103] generated_similar | n=500 | c=2000\n",
      "  n=500, capacity=2000\n",
      "  SKIP Brute Force: uniquement sur low_dimension\n",
      "  SKIP Branch and Bound: n=500 > max_n=200\n",
      "FTPAS Skip: V_scaled trop grand (1,268,120)\n",
      "  SKIP FTPAS (ε=0.1): protection déclenchée\n",
      "FTPAS Skip: V_scaled trop grand (2,536,240)\n",
      "  SKIP FTPAS (ε=0.05): protection déclenchée\n",
      "FTPAS Skip: mémoire estimée trop grande (1612 MB)\n",
      "  SKIP FTPAS Adaptive: protection déclenchée\n",
      "\n",
      "[40/103] generated_similar | n=500 | c=2000\n",
      "  n=500, capacity=2000\n",
      "  SKIP Brute Force: uniquement sur low_dimension\n",
      "  SKIP Branch and Bound: n=500 > max_n=200\n",
      "FTPAS Skip: V_scaled trop grand (1,247,905)\n",
      "  SKIP FTPAS (ε=0.1): protection déclenchée\n",
      "FTPAS Skip: V_scaled trop grand (2,495,810)\n",
      "  SKIP FTPAS (ε=0.05): protection déclenchée\n",
      "FTPAS Skip: mémoire estimée trop grande (1586 MB)\n",
      "  SKIP FTPAS Adaptive: protection déclenchée\n",
      "\n",
      "[41/103] generated_strongly | n=10000 | c=2000\n",
      "  n=10000, capacity=2000\n",
      "  SKIP Brute Force: uniquement sur low_dimension\n",
      "  SKIP Dynamic Programming: n=10000 > max_n=1000\n",
      "  SKIP DP Top-Down: n=10000 > max_n=1000\n",
      "  SKIP Branch and Bound: n=10000 > max_n=200\n",
      "FTPAS Skip: V_scaled trop grand (541,247,836)\n",
      "  SKIP FTPAS (ε=0.1): protection déclenchée\n",
      "FTPAS Skip: V_scaled trop grand (1,082,500,219)\n",
      "  SKIP FTPAS (ε=0.05): protection déclenchée\n",
      "FTPAS Skip: V_scaled trop grand (108,245,908)\n",
      "  SKIP FTPAS Adaptive: protection déclenchée\n",
      "\n",
      "[42/103] generated_strongly | n=1000 | c=2000\n",
      "  n=1000, capacity=2000\n",
      "  SKIP Brute Force: uniquement sur low_dimension\n",
      "  SKIP Branch and Bound: n=1000 > max_n=200\n",
      "FTPAS Skip: V_scaled trop grand (5,466,652)\n",
      "  SKIP FTPAS (ε=0.1): protection déclenchée\n",
      "FTPAS Skip: V_scaled trop grand (10,933,779)\n",
      "  SKIP FTPAS (ε=0.05): protection déclenchée\n",
      "FTPAS Skip: V_scaled trop grand (1,092,912)\n",
      "  SKIP FTPAS Adaptive: protection déclenchée\n",
      "\n",
      "[43/103] generated_strongly | n=1000 | c=2000\n",
      "  n=1000, capacity=2000\n",
      "  SKIP Brute Force: uniquement sur low_dimension\n",
      "  SKIP Branch and Bound: n=1000 > max_n=200\n",
      "FTPAS Skip: V_scaled trop grand (5,534,537)\n",
      "  SKIP FTPAS (ε=0.1): protection déclenchée\n",
      "FTPAS Skip: V_scaled trop grand (11,069,539)\n",
      "  SKIP FTPAS (ε=0.05): protection déclenchée\n",
      "FTPAS Skip: V_scaled trop grand (1,106,493)\n",
      "  SKIP FTPAS Adaptive: protection déclenchée\n",
      "\n",
      "[44/103] generated_strongly | n=1000 | c=2000\n",
      "  n=1000, capacity=2000\n",
      "  SKIP Brute Force: uniquement sur low_dimension\n",
      "  SKIP Branch and Bound: n=1000 > max_n=200\n",
      "FTPAS Skip: V_scaled trop grand (5,538,431)\n",
      "  SKIP FTPAS (ε=0.1): protection déclenchée\n",
      "FTPAS Skip: V_scaled trop grand (11,077,303)\n",
      "  SKIP FTPAS (ε=0.05): protection déclenchée\n",
      "FTPAS Skip: V_scaled trop grand (1,107,265)\n",
      "  SKIP FTPAS Adaptive: protection déclenchée\n",
      "\n",
      "[45/103] generated_strongly | n=1000 | c=2000\n",
      "  n=1000, capacity=2000\n",
      "  SKIP Brute Force: uniquement sur low_dimension\n",
      "  SKIP Branch and Bound: n=1000 > max_n=200\n",
      "FTPAS Skip: V_scaled trop grand (5,396,410)\n",
      "  SKIP FTPAS (ε=0.1): protection déclenchée\n",
      "FTPAS Skip: V_scaled trop grand (10,793,328)\n",
      "  SKIP FTPAS (ε=0.05): protection déclenchée\n",
      "FTPAS Skip: V_scaled trop grand (1,078,883)\n",
      "  SKIP FTPAS Adaptive: protection déclenchée\n",
      "\n",
      "[46/103] generated_strongly | n=1000 | c=2000\n",
      "  n=1000, capacity=2000\n",
      "  SKIP Brute Force: uniquement sur low_dimension\n",
      "  SKIP Branch and Bound: n=1000 > max_n=200\n",
      "FTPAS Skip: V_scaled trop grand (5,476,885)\n",
      "  SKIP FTPAS (ε=0.1): protection déclenchée\n",
      "FTPAS Skip: V_scaled trop grand (10,954,239)\n",
      "  SKIP FTPAS (ε=0.05): protection déclenchée\n",
      "FTPAS Skip: V_scaled trop grand (1,094,947)\n",
      "  SKIP FTPAS Adaptive: protection déclenchée\n",
      "\n",
      "[47/103] generated_strongly | n=100 | c=2000\n",
      "  n=100, capacity=2000\n",
      "  SKIP Brute Force: uniquement sur low_dimension\n",
      "\n",
      "[48/103] generated_strongly | n=100 | c=2000\n",
      "  n=100, capacity=2000\n",
      "  SKIP Brute Force: uniquement sur low_dimension\n",
      "\n",
      "[49/103] generated_strongly | n=100 | c=2000\n",
      "  n=100, capacity=2000\n",
      "  SKIP Brute Force: uniquement sur low_dimension\n",
      "\n",
      "[50/103] generated_strongly | n=100 | c=2000\n",
      "  n=100, capacity=2000\n",
      "  SKIP Brute Force: uniquement sur low_dimension\n",
      "\n",
      "[51/103] generated_strongly | n=100 | c=2000\n",
      "  n=100, capacity=2000\n",
      "  SKIP Brute Force: uniquement sur low_dimension\n",
      "\n",
      "[52/103] generated_strongly | n=2000 | c=2000\n",
      "  n=2000, capacity=2000\n",
      "  SKIP Brute Force: uniquement sur low_dimension\n",
      "  SKIP Dynamic Programming: n=2000 > max_n=1000\n",
      "  SKIP DP Top-Down: n=2000 > max_n=1000\n",
      "  SKIP Branch and Bound: n=2000 > max_n=200\n",
      "FTPAS Skip: V_scaled trop grand (21,670,961)\n",
      "  SKIP FTPAS (ε=0.1): protection déclenchée\n",
      "FTPAS Skip: V_scaled trop grand (43,342,822)\n",
      "  SKIP FTPAS (ε=0.05): protection déclenchée\n",
      "FTPAS Skip: V_scaled trop grand (4,333,366)\n",
      "  SKIP FTPAS Adaptive: protection déclenchée\n",
      "\n",
      "[53/103] generated_strongly | n=2000 | c=2000\n",
      "  n=2000, capacity=2000\n",
      "  SKIP Brute Force: uniquement sur low_dimension\n",
      "  SKIP Dynamic Programming: n=2000 > max_n=1000\n",
      "  SKIP DP Top-Down: n=2000 > max_n=1000\n",
      "  SKIP Branch and Bound: n=2000 > max_n=200\n",
      "FTPAS Skip: V_scaled trop grand (21,855,983)\n",
      "  SKIP FTPAS (ε=0.1): protection déclenchée\n",
      "FTPAS Skip: V_scaled trop grand (43,712,885)\n",
      "  SKIP FTPAS (ε=0.05): protection déclenchée\n",
      "FTPAS Skip: V_scaled trop grand (4,370,341)\n",
      "  SKIP FTPAS Adaptive: protection déclenchée\n",
      "\n",
      "[54/103] generated_strongly | n=2000 | c=2000\n",
      "  n=2000, capacity=2000\n",
      "  SKIP Brute Force: uniquement sur low_dimension\n",
      "  SKIP Dynamic Programming: n=2000 > max_n=1000\n",
      "  SKIP DP Top-Down: n=2000 > max_n=1000\n",
      "  SKIP Branch and Bound: n=2000 > max_n=200\n",
      "FTPAS Skip: V_scaled trop grand (22,148,267)\n",
      "  SKIP FTPAS (ε=0.1): protection déclenchée\n",
      "FTPAS Skip: V_scaled trop grand (44,297,415)\n",
      "  SKIP FTPAS (ε=0.05): protection déclenchée\n",
      "FTPAS Skip: V_scaled trop grand (4,428,807)\n",
      "  SKIP FTPAS Adaptive: protection déclenchée\n",
      "\n",
      "[55/103] generated_strongly | n=5000 | c=2000\n",
      "  n=5000, capacity=2000\n",
      "  SKIP Brute Force: uniquement sur low_dimension\n",
      "  SKIP Dynamic Programming: n=5000 > max_n=1000\n",
      "  SKIP DP Top-Down: n=5000 > max_n=1000\n",
      "  SKIP Branch and Bound: n=5000 > max_n=200\n",
      "FTPAS Skip: V_scaled trop grand (135,174,183)\n",
      "  SKIP FTPAS (ε=0.1): protection déclenchée\n",
      "FTPAS Skip: V_scaled trop grand (270,350,700)\n",
      "  SKIP FTPAS (ε=0.05): protection déclenchée\n",
      "FTPAS Skip: V_scaled trop grand (27,033,038)\n",
      "  SKIP FTPAS Adaptive: protection déclenchée\n",
      "\n",
      "[56/103] generated_strongly | n=5000 | c=2000\n",
      "  n=5000, capacity=2000\n",
      "  SKIP Brute Force: uniquement sur low_dimension\n",
      "  SKIP Dynamic Programming: n=5000 > max_n=1000\n",
      "  SKIP DP Top-Down: n=5000 > max_n=1000\n",
      "  SKIP Branch and Bound: n=5000 > max_n=200\n",
      "FTPAS Skip: V_scaled trop grand (135,322,756)\n",
      "  SKIP FTPAS (ε=0.1): protection déclenchée\n",
      "FTPAS Skip: V_scaled trop grand (270,647,709)\n",
      "  SKIP FTPAS (ε=0.05): protection déclenchée\n",
      "FTPAS Skip: V_scaled trop grand (27,062,752)\n",
      "  SKIP FTPAS Adaptive: protection déclenchée\n",
      "\n",
      "[57/103] generated_strongly | n=500 | c=2000\n",
      "  n=500, capacity=2000\n",
      "  SKIP Brute Force: uniquement sur low_dimension\n",
      "  SKIP Branch and Bound: n=500 > max_n=200\n",
      "FTPAS Skip: V_scaled trop grand (1,399,733)\n",
      "  SKIP FTPAS (ε=0.1): protection déclenchée\n",
      "FTPAS Skip: V_scaled trop grand (2,799,732)\n",
      "  SKIP FTPAS (ε=0.05): protection déclenchée\n",
      "FTPAS Skip: mémoire estimée trop grande (1779 MB)\n",
      "  SKIP FTPAS Adaptive: protection déclenchée\n",
      "\n",
      "[58/103] generated_strongly | n=500 | c=2000\n",
      "  n=500, capacity=2000\n",
      "  SKIP Brute Force: uniquement sur low_dimension\n",
      "  SKIP Branch and Bound: n=500 > max_n=200\n",
      "FTPAS Skip: V_scaled trop grand (1,371,346)\n",
      "  SKIP FTPAS (ε=0.1): protection déclenchée\n",
      "FTPAS Skip: V_scaled trop grand (2,742,940)\n",
      "  SKIP FTPAS (ε=0.05): protection déclenchée\n",
      "FTPAS Skip: mémoire estimée trop grande (1743 MB)\n",
      "  SKIP FTPAS Adaptive: protection déclenchée\n",
      "\n",
      "[59/103] generated_strongly | n=500 | c=2000\n",
      "  n=500, capacity=2000\n",
      "  SKIP Brute Force: uniquement sur low_dimension\n",
      "  SKIP Branch and Bound: n=500 > max_n=200\n",
      "FTPAS Skip: V_scaled trop grand (1,379,114)\n",
      "  SKIP FTPAS (ε=0.1): protection déclenchée\n",
      "FTPAS Skip: V_scaled trop grand (2,758,487)\n",
      "  SKIP FTPAS (ε=0.05): protection déclenchée\n",
      "FTPAS Skip: mémoire estimée trop grande (1753 MB)\n",
      "  SKIP FTPAS Adaptive: protection déclenchée\n",
      "\n",
      "[60/103] generated_strongly | n=500 | c=2000\n",
      "  n=500, capacity=2000\n",
      "  SKIP Brute Force: uniquement sur low_dimension\n",
      "  SKIP Branch and Bound: n=500 > max_n=200\n",
      "FTPAS Skip: V_scaled trop grand (1,358,493)\n",
      "  SKIP FTPAS (ε=0.1): protection déclenchée\n",
      "FTPAS Skip: V_scaled trop grand (2,717,212)\n",
      "  SKIP FTPAS (ε=0.05): protection déclenchée\n",
      "FTPAS Skip: mémoire estimée trop grande (1727 MB)\n",
      "  SKIP FTPAS Adaptive: protection déclenchée\n",
      "\n",
      "[61/103] generated_strongly | n=500 | c=2000\n",
      "  n=500, capacity=2000\n",
      "  SKIP Brute Force: uniquement sur low_dimension\n",
      "  SKIP Branch and Bound: n=500 > max_n=200\n",
      "FTPAS Skip: V_scaled trop grand (1,398,944)\n",
      "  SKIP FTPAS (ε=0.1): protection déclenchée\n",
      "FTPAS Skip: V_scaled trop grand (2,798,140)\n",
      "  SKIP FTPAS (ε=0.05): protection déclenchée\n",
      "FTPAS Skip: mémoire estimée trop grande (1778 MB)\n",
      "  SKIP FTPAS Adaptive: protection déclenchée\n",
      "\n",
      "[62/103] generated_uncorrelated | n=10000 | c=2000\n",
      "  n=10000, capacity=2000\n",
      "  SKIP Brute Force: uniquement sur low_dimension\n",
      "  SKIP Dynamic Programming: n=10000 > max_n=1000\n",
      "  SKIP DP Top-Down: n=10000 > max_n=1000\n",
      "  SKIP Branch and Bound: n=10000 > max_n=200\n",
      "FTPAS Skip: V_scaled trop grand (501,216,300)\n",
      "  SKIP FTPAS (ε=0.1): protection déclenchée\n",
      "FTPAS Skip: V_scaled trop grand (1,002,432,600)\n",
      "  SKIP FTPAS (ε=0.05): protection déclenchée\n",
      "FTPAS Skip: V_scaled trop grand (100,243,260)\n",
      "  SKIP FTPAS Adaptive: protection déclenchée\n",
      "\n",
      "[63/103] generated_uncorrelated | n=1000 | c=2000\n",
      "  n=1000, capacity=2000\n",
      "  SKIP Brute Force: uniquement sur low_dimension\n",
      "  SKIP Branch and Bound: n=1000 > max_n=200\n",
      "FTPAS Skip: V_scaled trop grand (5,081,680)\n",
      "  SKIP FTPAS (ε=0.1): protection déclenchée\n",
      "FTPAS Skip: V_scaled trop grand (10,163,360)\n",
      "  SKIP FTPAS (ε=0.05): protection déclenchée\n",
      "FTPAS Skip: V_scaled trop grand (1,016,336)\n",
      "  SKIP FTPAS Adaptive: protection déclenchée\n",
      "\n",
      "[64/103] generated_uncorrelated | n=1000 | c=2000\n",
      "  n=1000, capacity=2000\n",
      "  SKIP Brute Force: uniquement sur low_dimension\n",
      "  SKIP Branch and Bound: n=1000 > max_n=200\n",
      "FTPAS Skip: V_scaled trop grand (5,008,489)\n",
      "  SKIP FTPAS (ε=0.1): protection déclenchée\n",
      "FTPAS Skip: V_scaled trop grand (10,017,494)\n",
      "  SKIP FTPAS (ε=0.05): protection déclenchée\n",
      "FTPAS Skip: V_scaled trop grand (1,001,298)\n",
      "  SKIP FTPAS Adaptive: protection déclenchée\n",
      "\n",
      "[65/103] generated_uncorrelated | n=1000 | c=2000\n",
      "  n=1000, capacity=2000\n",
      "  SKIP Brute Force: uniquement sur low_dimension\n",
      "  SKIP Branch and Bound: n=1000 > max_n=200\n",
      "FTPAS Skip: V_scaled trop grand (4,982,280)\n",
      "  SKIP FTPAS (ε=0.1): protection déclenchée\n",
      "FTPAS Skip: V_scaled trop grand (9,964,560)\n",
      "  SKIP FTPAS (ε=0.05): protection déclenchée\n",
      "FTPAS Skip: mémoire estimée trop grande (7602 MB)\n",
      "  SKIP FTPAS Adaptive: protection déclenchée\n",
      "\n",
      "[66/103] generated_uncorrelated | n=1000 | c=2000\n",
      "  n=1000, capacity=2000\n",
      "  SKIP Brute Force: uniquement sur low_dimension\n",
      "  SKIP Branch and Bound: n=1000 > max_n=200\n",
      "FTPAS Skip: V_scaled trop grand (4,912,420)\n",
      "  SKIP FTPAS (ε=0.1): protection déclenchée\n",
      "FTPAS Skip: V_scaled trop grand (9,824,840)\n",
      "  SKIP FTPAS (ε=0.05): protection déclenchée\n",
      "FTPAS Skip: mémoire estimée trop grande (7496 MB)\n",
      "  SKIP FTPAS Adaptive: protection déclenchée\n",
      "\n",
      "[67/103] generated_uncorrelated | n=1000 | c=2000\n",
      "  n=1000, capacity=2000\n",
      "  SKIP Brute Force: uniquement sur low_dimension\n",
      "  SKIP Branch and Bound: n=1000 > max_n=200\n",
      "FTPAS Skip: V_scaled trop grand (4,993,240)\n",
      "  SKIP FTPAS (ε=0.1): protection déclenchée\n",
      "FTPAS Skip: V_scaled trop grand (9,986,480)\n",
      "  SKIP FTPAS (ε=0.05): protection déclenchée\n",
      "FTPAS Skip: mémoire estimée trop grande (7619 MB)\n",
      "  SKIP FTPAS Adaptive: protection déclenchée\n",
      "\n",
      "[68/103] generated_uncorrelated | n=100 | c=2000\n",
      "  n=100, capacity=2000\n",
      "  SKIP Brute Force: uniquement sur low_dimension\n",
      "\n",
      "[69/103] generated_uncorrelated | n=100 | c=2000\n",
      "  n=100, capacity=2000\n",
      "  SKIP Brute Force: uniquement sur low_dimension\n",
      "\n",
      "[70/103] generated_uncorrelated | n=100 | c=2000\n",
      "  n=100, capacity=2000\n",
      "  SKIP Brute Force: uniquement sur low_dimension\n",
      "\n",
      "[71/103] generated_uncorrelated | n=100 | c=2000\n",
      "  n=100, capacity=2000\n",
      "  SKIP Brute Force: uniquement sur low_dimension\n",
      "\n",
      "[72/103] generated_uncorrelated | n=100 | c=2000\n",
      "  n=100, capacity=2000\n",
      "  SKIP Brute Force: uniquement sur low_dimension\n",
      "\n",
      "[73/103] generated_uncorrelated | n=2000 | c=2000\n",
      "  n=2000, capacity=2000\n",
      "  SKIP Brute Force: uniquement sur low_dimension\n",
      "  SKIP Dynamic Programming: n=2000 > max_n=1000\n",
      "  SKIP DP Top-Down: n=2000 > max_n=1000\n",
      "  SKIP Branch and Bound: n=2000 > max_n=200\n",
      "FTPAS Skip: V_scaled trop grand (20,267,180)\n",
      "  SKIP FTPAS (ε=0.1): protection déclenchée\n",
      "FTPAS Skip: V_scaled trop grand (40,534,360)\n",
      "  SKIP FTPAS (ε=0.05): protection déclenchée\n",
      "FTPAS Skip: V_scaled trop grand (4,053,436)\n",
      "  SKIP FTPAS Adaptive: protection déclenchée\n",
      "\n",
      "[74/103] generated_uncorrelated | n=2000 | c=2000\n",
      "  n=2000, capacity=2000\n",
      "  SKIP Brute Force: uniquement sur low_dimension\n",
      "  SKIP Dynamic Programming: n=2000 > max_n=1000\n",
      "  SKIP DP Top-Down: n=2000 > max_n=1000\n",
      "  SKIP Branch and Bound: n=2000 > max_n=200\n",
      "FTPAS Skip: V_scaled trop grand (20,042,940)\n",
      "  SKIP FTPAS (ε=0.1): protection déclenchée\n",
      "FTPAS Skip: V_scaled trop grand (40,085,880)\n",
      "  SKIP FTPAS (ε=0.05): protection déclenchée\n",
      "FTPAS Skip: V_scaled trop grand (4,008,588)\n",
      "  SKIP FTPAS Adaptive: protection déclenchée\n",
      "\n",
      "[75/103] generated_uncorrelated | n=2000 | c=2000\n",
      "  n=2000, capacity=2000\n",
      "  SKIP Brute Force: uniquement sur low_dimension\n",
      "  SKIP Dynamic Programming: n=2000 > max_n=1000\n",
      "  SKIP DP Top-Down: n=2000 > max_n=1000\n",
      "  SKIP Branch and Bound: n=2000 > max_n=200\n",
      "FTPAS Skip: V_scaled trop grand (19,511,680)\n",
      "  SKIP FTPAS (ε=0.1): protection déclenchée\n",
      "FTPAS Skip: V_scaled trop grand (39,023,360)\n",
      "  SKIP FTPAS (ε=0.05): protection déclenchée\n",
      "FTPAS Skip: V_scaled trop grand (3,902,336)\n",
      "  SKIP FTPAS Adaptive: protection déclenchée\n",
      "\n",
      "[76/103] generated_uncorrelated | n=5000 | c=2000\n",
      "  n=5000, capacity=2000\n",
      "  SKIP Brute Force: uniquement sur low_dimension\n",
      "  SKIP Dynamic Programming: n=5000 > max_n=1000\n",
      "  SKIP DP Top-Down: n=5000 > max_n=1000\n",
      "  SKIP Branch and Bound: n=5000 > max_n=200\n",
      "FTPAS Skip: V_scaled trop grand (125,107,450)\n",
      "  SKIP FTPAS (ε=0.1): protection déclenchée\n",
      "FTPAS Skip: V_scaled trop grand (250,214,900)\n",
      "  SKIP FTPAS (ε=0.05): protection déclenchée\n",
      "FTPAS Skip: V_scaled trop grand (25,021,490)\n",
      "  SKIP FTPAS Adaptive: protection déclenchée\n",
      "\n",
      "[77/103] generated_uncorrelated | n=5000 | c=2000\n",
      "  n=5000, capacity=2000\n",
      "  SKIP Brute Force: uniquement sur low_dimension\n",
      "  SKIP Dynamic Programming: n=5000 > max_n=1000\n",
      "  SKIP DP Top-Down: n=5000 > max_n=1000\n",
      "  SKIP Branch and Bound: n=5000 > max_n=200\n",
      "FTPAS Skip: V_scaled trop grand (124,777,693)\n",
      "  SKIP FTPAS (ε=0.1): protection déclenchée\n",
      "FTPAS Skip: V_scaled trop grand (249,557,854)\n",
      "  SKIP FTPAS (ε=0.05): protection déclenchée\n",
      "FTPAS Skip: V_scaled trop grand (24,953,560)\n",
      "  SKIP FTPAS Adaptive: protection déclenchée\n",
      "\n",
      "[78/103] generated_uncorrelated | n=500 | c=2000\n",
      "  n=500, capacity=2000\n",
      "  SKIP Brute Force: uniquement sur low_dimension\n",
      "  SKIP Branch and Bound: n=500 > max_n=200\n",
      "FTPAS Skip: V_scaled trop grand (1,200,945)\n",
      "  SKIP FTPAS (ε=0.1): protection déclenchée\n",
      "FTPAS Skip: V_scaled trop grand (2,401,890)\n",
      "  SKIP FTPAS (ε=0.05): protection déclenchée\n",
      "FTPAS Skip: mémoire estimée trop grande (1526 MB)\n",
      "  SKIP FTPAS Adaptive: protection déclenchée\n",
      "\n",
      "[79/103] generated_uncorrelated | n=500 | c=2000\n",
      "  n=500, capacity=2000\n",
      "  SKIP Brute Force: uniquement sur low_dimension\n",
      "  SKIP Branch and Bound: n=500 > max_n=200\n",
      "FTPAS Skip: V_scaled trop grand (1,264,098)\n",
      "  SKIP FTPAS (ε=0.1): protection déclenchée\n",
      "FTPAS Skip: V_scaled trop grand (2,528,454)\n",
      "  SKIP FTPAS (ε=0.05): protection déclenchée\n",
      "FTPAS Skip: mémoire estimée trop grande (1607 MB)\n",
      "  SKIP FTPAS Adaptive: protection déclenchée\n",
      "\n",
      "[80/103] generated_uncorrelated | n=500 | c=2000\n",
      "  n=500, capacity=2000\n",
      "  SKIP Brute Force: uniquement sur low_dimension\n",
      "  SKIP Branch and Bound: n=500 > max_n=200\n",
      "FTPAS Skip: V_scaled trop grand (1,285,875)\n",
      "  SKIP FTPAS (ε=0.1): protection déclenchée\n",
      "FTPAS Skip: V_scaled trop grand (2,571,750)\n",
      "  SKIP FTPAS (ε=0.05): protection déclenchée\n",
      "FTPAS Skip: mémoire estimée trop grande (1634 MB)\n",
      "  SKIP FTPAS Adaptive: protection déclenchée\n",
      "\n",
      "[81/103] generated_uncorrelated | n=500 | c=2000\n",
      "  n=500, capacity=2000\n",
      "  SKIP Brute Force: uniquement sur low_dimension\n",
      "  SKIP Branch and Bound: n=500 > max_n=200\n",
      "FTPAS Skip: V_scaled trop grand (1,299,163)\n",
      "  SKIP FTPAS (ε=0.1): protection déclenchée\n",
      "FTPAS Skip: V_scaled trop grand (2,598,586)\n",
      "  SKIP FTPAS (ε=0.05): protection déclenchée\n",
      "FTPAS Skip: mémoire estimée trop grande (1651 MB)\n",
      "  SKIP FTPAS Adaptive: protection déclenchée\n",
      "\n",
      "[82/103] generated_uncorrelated | n=500 | c=2000\n",
      "  n=500, capacity=2000\n",
      "  SKIP Brute Force: uniquement sur low_dimension\n",
      "  SKIP Branch and Bound: n=500 > max_n=200\n",
      "FTPAS Skip: V_scaled trop grand (1,144,545)\n",
      "  SKIP FTPAS (ε=0.1): protection déclenchée\n",
      "FTPAS Skip: V_scaled trop grand (2,289,090)\n",
      "  SKIP FTPAS (ε=0.05): protection déclenchée\n",
      "FTPAS Skip: mémoire estimée trop grande (1455 MB)\n",
      "  SKIP FTPAS Adaptive: protection déclenchée\n",
      "\n",
      "[83/103] generated_weakly | n=10000 | c=2000\n",
      "  n=10000, capacity=2000\n",
      "  SKIP Brute Force: uniquement sur low_dimension\n",
      "  SKIP Dynamic Programming: n=10000 > max_n=1000\n",
      "  SKIP DP Top-Down: n=10000 > max_n=1000\n",
      "  SKIP Branch and Bound: n=10000 > max_n=200\n",
      "FTPAS Skip: V_scaled trop grand (454,333,348)\n",
      "  SKIP FTPAS (ε=0.1): protection déclenchée\n",
      "FTPAS Skip: V_scaled trop grand (908,671,577)\n",
      "  SKIP FTPAS (ε=0.05): protection déclenchée\n",
      "FTPAS Skip: V_scaled trop grand (90,862,741)\n",
      "  SKIP FTPAS Adaptive: protection déclenchée\n",
      "\n",
      "[84/103] generated_weakly | n=1000 | c=2000\n",
      "  n=1000, capacity=2000\n",
      "  SKIP Brute Force: uniquement sur low_dimension\n",
      "  SKIP Branch and Bound: n=1000 > max_n=200\n",
      "FTPAS Skip: V_scaled trop grand (4,601,970)\n",
      "  SKIP FTPAS (ε=0.1): protection déclenchée\n",
      "FTPAS Skip: V_scaled trop grand (9,204,407)\n",
      "  SKIP FTPAS (ε=0.05): protection déclenchée\n",
      "FTPAS Skip: mémoire estimée trop grande (7019 MB)\n",
      "  SKIP FTPAS Adaptive: protection déclenchée\n",
      "\n",
      "[85/103] generated_weakly | n=1000 | c=2000\n",
      "  n=1000, capacity=2000\n",
      "  SKIP Brute Force: uniquement sur low_dimension\n",
      "  SKIP Branch and Bound: n=1000 > max_n=200\n",
      "FTPAS Skip: V_scaled trop grand (4,721,334)\n",
      "  SKIP FTPAS (ε=0.1): protection déclenchée\n",
      "FTPAS Skip: V_scaled trop grand (9,443,166)\n",
      "  SKIP FTPAS (ε=0.05): protection déclenchée\n",
      "FTPAS Skip: mémoire estimée trop grande (7201 MB)\n",
      "  SKIP FTPAS Adaptive: protection déclenchée\n",
      "\n",
      "[86/103] generated_weakly | n=1000 | c=2000\n",
      "  n=1000, capacity=2000\n",
      "  SKIP Brute Force: uniquement sur low_dimension\n",
      "  SKIP Branch and Bound: n=1000 > max_n=200\n",
      "FTPAS Skip: V_scaled trop grand (4,632,237)\n",
      "  SKIP FTPAS (ε=0.1): protection déclenchée\n",
      "FTPAS Skip: V_scaled trop grand (9,264,992)\n",
      "  SKIP FTPAS (ε=0.05): protection déclenchée\n",
      "FTPAS Skip: mémoire estimée trop grande (7065 MB)\n",
      "  SKIP FTPAS Adaptive: protection déclenchée\n",
      "\n",
      "[87/103] generated_weakly | n=1000 | c=2000\n",
      "  n=1000, capacity=2000\n",
      "  SKIP Brute Force: uniquement sur low_dimension\n",
      "  SKIP Branch and Bound: n=1000 > max_n=200\n",
      "FTPAS Skip: V_scaled trop grand (4,625,630)\n",
      "  SKIP FTPAS (ε=0.1): protection déclenchée\n",
      "FTPAS Skip: V_scaled trop grand (9,251,734)\n",
      "  SKIP FTPAS (ε=0.05): protection déclenchée\n",
      "FTPAS Skip: mémoire estimée trop grande (7055 MB)\n",
      "  SKIP FTPAS Adaptive: protection déclenchée\n",
      "\n",
      "[88/103] generated_weakly | n=1000 | c=2000\n",
      "  n=1000, capacity=2000\n",
      "  SKIP Brute Force: uniquement sur low_dimension\n",
      "  SKIP Branch and Bound: n=1000 > max_n=200\n",
      "FTPAS Skip: V_scaled trop grand (4,595,666)\n",
      "  SKIP FTPAS (ε=0.1): protection déclenchée\n",
      "FTPAS Skip: V_scaled trop grand (9,191,829)\n",
      "  SKIP FTPAS (ε=0.05): protection déclenchée\n",
      "FTPAS Skip: mémoire estimée trop grande (7009 MB)\n",
      "  SKIP FTPAS Adaptive: protection déclenchée\n",
      "\n",
      "[89/103] generated_weakly | n=100 | c=2000\n",
      "  n=100, capacity=2000\n",
      "  SKIP Brute Force: uniquement sur low_dimension\n",
      "\n",
      "[90/103] generated_weakly | n=100 | c=2000\n",
      "  n=100, capacity=2000\n",
      "  SKIP Brute Force: uniquement sur low_dimension\n",
      "\n",
      "[91/103] generated_weakly | n=100 | c=2000\n",
      "  n=100, capacity=2000\n",
      "  SKIP Brute Force: uniquement sur low_dimension\n",
      "\n",
      "[92/103] generated_weakly | n=100 | c=2000\n",
      "  n=100, capacity=2000\n",
      "  SKIP Brute Force: uniquement sur low_dimension\n",
      "\n",
      "[93/103] generated_weakly | n=100 | c=2000\n",
      "  n=100, capacity=2000\n",
      "  SKIP Brute Force: uniquement sur low_dimension\n",
      "\n",
      "[94/103] generated_weakly | n=2000 | c=2000\n",
      "  n=2000, capacity=2000\n",
      "  SKIP Brute Force: uniquement sur low_dimension\n",
      "  SKIP Dynamic Programming: n=2000 > max_n=1000\n",
      "  SKIP DP Top-Down: n=2000 > max_n=1000\n",
      "  SKIP Branch and Bound: n=2000 > max_n=200\n",
      "FTPAS Skip: V_scaled trop grand (18,427,961)\n",
      "  SKIP FTPAS (ε=0.1): protection déclenchée\n",
      "FTPAS Skip: V_scaled trop grand (36,856,912)\n",
      "  SKIP FTPAS (ε=0.05): protection déclenchée\n",
      "FTPAS Skip: V_scaled trop grand (3,684,792)\n",
      "  SKIP FTPAS Adaptive: protection déclenchée\n",
      "\n",
      "[95/103] generated_weakly | n=2000 | c=2000\n",
      "  n=2000, capacity=2000\n",
      "  SKIP Brute Force: uniquement sur low_dimension\n",
      "  SKIP Dynamic Programming: n=2000 > max_n=1000\n",
      "  SKIP DP Top-Down: n=2000 > max_n=1000\n",
      "  SKIP Branch and Bound: n=2000 > max_n=200\n",
      "FTPAS Skip: V_scaled trop grand (18,697,261)\n",
      "  SKIP FTPAS (ε=0.1): protection déclenchée\n",
      "FTPAS Skip: V_scaled trop grand (37,395,482)\n",
      "  SKIP FTPAS (ε=0.05): protection déclenchée\n",
      "FTPAS Skip: V_scaled trop grand (3,738,657)\n",
      "  SKIP FTPAS Adaptive: protection déclenchée\n",
      "\n",
      "[96/103] generated_weakly | n=2000 | c=2000\n",
      "  n=2000, capacity=2000\n",
      "  SKIP Brute Force: uniquement sur low_dimension\n",
      "  SKIP Dynamic Programming: n=2000 > max_n=1000\n",
      "  SKIP DP Top-Down: n=2000 > max_n=1000\n",
      "  SKIP Branch and Bound: n=2000 > max_n=200\n",
      "FTPAS Skip: V_scaled trop grand (18,359,459)\n",
      "  SKIP FTPAS (ε=0.1): protection déclenchée\n",
      "FTPAS Skip: V_scaled trop grand (36,719,947)\n",
      "  SKIP FTPAS (ε=0.05): protection déclenchée\n",
      "FTPAS Skip: V_scaled trop grand (3,671,083)\n",
      "  SKIP FTPAS Adaptive: protection déclenchée\n",
      "\n",
      "[97/103] generated_weakly | n=5000 | c=2000\n",
      "  n=5000, capacity=2000\n",
      "  SKIP Brute Force: uniquement sur low_dimension\n",
      "  SKIP Dynamic Programming: n=5000 > max_n=1000\n",
      "  SKIP DP Top-Down: n=5000 > max_n=1000\n",
      "  SKIP Branch and Bound: n=5000 > max_n=200\n",
      "FTPAS Skip: V_scaled trop grand (117,731,254)\n",
      "  SKIP FTPAS (ε=0.1): protection déclenchée\n",
      "FTPAS Skip: V_scaled trop grand (235,464,904)\n",
      "  SKIP FTPAS (ε=0.05): protection déclenchée\n",
      "FTPAS Skip: V_scaled trop grand (23,544,300)\n",
      "  SKIP FTPAS Adaptive: protection déclenchée\n",
      "\n",
      "[98/103] generated_weakly | n=5000 | c=2000\n",
      "  n=5000, capacity=2000\n",
      "  SKIP Brute Force: uniquement sur low_dimension\n",
      "  SKIP Dynamic Programming: n=5000 > max_n=1000\n",
      "  SKIP DP Top-Down: n=5000 > max_n=1000\n",
      "  SKIP Branch and Bound: n=5000 > max_n=200\n",
      "FTPAS Skip: V_scaled trop grand (115,410,218)\n",
      "  SKIP FTPAS (ε=0.1): protection déclenchée\n",
      "FTPAS Skip: V_scaled trop grand (230,823,143)\n",
      "  SKIP FTPAS (ε=0.05): protection déclenchée\n",
      "FTPAS Skip: V_scaled trop grand (23,080,076)\n",
      "  SKIP FTPAS Adaptive: protection déclenchée\n",
      "\n",
      "[99/103] generated_weakly | n=500 | c=2000\n",
      "  n=500, capacity=2000\n",
      "  SKIP Brute Force: uniquement sur low_dimension\n",
      "  SKIP Branch and Bound: n=500 > max_n=200\n",
      "FTPAS Skip: V_scaled trop grand (1,116,542)\n",
      "  SKIP FTPAS (ε=0.1): protection déclenchée\n",
      "FTPAS Skip: V_scaled trop grand (2,233,331)\n",
      "  SKIP FTPAS (ε=0.05): protection déclenchée\n",
      "FTPAS Skip: mémoire estimée trop grande (1419 MB)\n",
      "  SKIP FTPAS Adaptive: protection déclenchée\n",
      "\n",
      "[100/103] generated_weakly | n=500 | c=2000\n",
      "  n=500, capacity=2000\n",
      "  SKIP Brute Force: uniquement sur low_dimension\n",
      "  SKIP Branch and Bound: n=500 > max_n=200\n",
      "FTPAS Skip: V_scaled trop grand (1,177,680)\n",
      "  SKIP FTPAS (ε=0.1): protection déclenchée\n",
      "FTPAS Skip: V_scaled trop grand (2,355,617)\n",
      "  SKIP FTPAS (ε=0.05): protection déclenchée\n",
      "FTPAS Skip: mémoire estimée trop grande (1497 MB)\n",
      "  SKIP FTPAS Adaptive: protection déclenchée\n",
      "\n",
      "[101/103] generated_weakly | n=500 | c=2000\n",
      "  n=500, capacity=2000\n",
      "  SKIP Brute Force: uniquement sur low_dimension\n",
      "  SKIP Branch and Bound: n=500 > max_n=200\n",
      "FTPAS Skip: V_scaled trop grand (1,202,459)\n",
      "  SKIP FTPAS (ε=0.1): protection déclenchée\n",
      "FTPAS Skip: V_scaled trop grand (2,405,188)\n",
      "  SKIP FTPAS (ε=0.05): protection déclenchée\n",
      "FTPAS Skip: mémoire estimée trop grande (1528 MB)\n",
      "  SKIP FTPAS Adaptive: protection déclenchée\n",
      "\n",
      "[102/103] generated_weakly | n=500 | c=2000\n",
      "  n=500, capacity=2000\n",
      "  SKIP Brute Force: uniquement sur low_dimension\n",
      "  SKIP Branch and Bound: n=500 > max_n=200\n",
      "FTPAS Skip: V_scaled trop grand (1,145,546)\n",
      "  SKIP FTPAS (ε=0.1): protection déclenchée\n",
      "FTPAS Skip: V_scaled trop grand (2,291,360)\n",
      "  SKIP FTPAS (ε=0.05): protection déclenchée\n",
      "FTPAS Skip: mémoire estimée trop grande (1456 MB)\n",
      "  SKIP FTPAS Adaptive: protection déclenchée\n",
      "\n",
      "[103/103] generated_weakly | n=500 | c=2000\n",
      "  n=500, capacity=2000\n",
      "  SKIP Brute Force: uniquement sur low_dimension\n",
      "  SKIP Branch and Bound: n=500 > max_n=200\n",
      "FTPAS Skip: V_scaled trop grand (1,168,000)\n",
      "  SKIP FTPAS (ε=0.1): protection déclenchée\n",
      "FTPAS Skip: V_scaled trop grand (2,336,245)\n",
      "  SKIP FTPAS (ε=0.05): protection déclenchée\n",
      "FTPAS Skip: mémoire estimée trop grande (1485 MB)\n",
      "  SKIP FTPAS Adaptive: protection déclenchée\n",
      "\n",
      "\n",
      "============================================================\n",
      "TERMINÉ ! 1206 résultats collectés\n",
      "============================================================\n",
      "\n",
      "Résultats sauvegardés: 'benchmark_results.csv'\n",
      "\n",
      "Résumé:\n",
      "  - 16 algorithmes testés\n",
      "  - 13 tailles différentes\n",
      "  - 6 types de corrélation\n",
      "\n",
      "Aperçu des résultats:\n",
      "                     count          mean           std\n",
      "algorithm                                             \n",
      "Branch and Bound        35   4375.371429   3758.883385\n",
      "Brute Force              8    336.375000    433.234987\n",
      "DP Top-Down             75   9460.413333  10954.232837\n",
      "Dynamic Programming     75   9460.413333  10954.232837\n",
      "FTPAS (ε=0.05)          30   4278.800000   3819.257162\n",
      "FTPAS (ε=0.1)           30   4278.566667   3819.179806\n",
      "FTPAS Adaptive          35   4378.771429   3760.858961\n",
      "Fractional Knapsack    102  25702.222492  65074.509531\n",
      "Genetic Adaptive       102  23130.186275  56379.841190\n",
      "Genetic Algorithm      102  23013.950980  56165.133910\n",
      "Greedy Ratio           102  25607.431373  65081.800003\n",
      "Greedy Value           102   7129.960784  13828.750577\n",
      "Greedy Weight          102  22320.137255  57613.334575\n",
      "Randomized             102   6898.568627  10608.255724\n",
      "SA Adaptive            102  25608.852941  65081.325705\n",
      "Simulated Annealing    102  25609.901961  65081.196109\n",
      "\n",
      "Aperçu:\n"
     ]
    }
   ],
   "source": [
    "# Charger les résultats\n",
    "try:\n",
    "    results_df = run_all_benchmarks()\n",
    "    print(f\"\\nAperçu:\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Fichier 'benchmark_results.csv' non trouvé\")\n",
    "    print(\"Exécutez d'abord: results_df = run_all_benchmarks()\")\n",
    "    results_df = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Visualisations simples\n",
    "\n",
    "Ci-dessous :\n",
    "- Graphe 1 : Valeur totale (axe Y) par algorithme (axe X), couleur = type de dataset (correlation).\n",
    "- Graphe 2 : Temps d'exécution (ms) par algorithme, couleur = taille `n`.\n",
    "- Graphe 3 : Scatter Temps (X) vs Valeur (Y), 1 point par algorithme, étiquette avec le nom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The palette dictionary is missing keys: {'generated_uncorrelated', 'generated_strongly', 'generated_similar', 'generated_weakly'}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[67]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Fallback pour anciennes catégories si présentes\u001b[39;00m\n\u001b[32m     11\u001b[39m palette_map.update({\u001b[33m'\u001b[39m\u001b[33muncorrelated\u001b[39m\u001b[33m'\u001b[39m:\u001b[33m'\u001b[39m\u001b[33m#1f77b4\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mweakly_correlated\u001b[39m\u001b[33m'\u001b[39m:\u001b[33m'\u001b[39m\u001b[33m#ff7f0e\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mstrongly_correlated\u001b[39m\u001b[33m'\u001b[39m:\u001b[33m'\u001b[39m\u001b[33m#2ca02c\u001b[39m\u001b[33m'\u001b[39m})\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[43msns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbarplot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43magg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43malgorithm\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mvalue\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhue\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcorrelation\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpalette\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpalette_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m plt.xlabel(\u001b[33m'\u001b[39m\u001b[33mAlgorithme\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     14\u001b[39m plt.ylabel(\u001b[33m'\u001b[39m\u001b[33mValeur totale moyenne\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\seaborn\\categorical.py:2370\u001b[39m, in \u001b[36mbarplot\u001b[39m\u001b[34m(data, x, y, hue, order, hue_order, estimator, errorbar, n_boot, seed, units, weights, orient, color, palette, saturation, fill, hue_norm, width, dodge, gap, log_scale, native_scale, formatter, legend, capsize, err_kws, ci, errcolor, errwidth, ax, **kwargs)\u001b[39m\n\u001b[32m   2367\u001b[39m palette, hue_order = p._hue_backcompat(color, palette, hue_order)\n\u001b[32m   2369\u001b[39m saturation = saturation \u001b[38;5;28;01mif\u001b[39;00m fill \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m2370\u001b[39m \u001b[43mp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_hue\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpalette\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpalette\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhue_order\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnorm\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhue_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msaturation\u001b[49m\u001b[43m=\u001b[49m\u001b[43msaturation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2371\u001b[39m color = _default_color(ax.bar, hue, color, kwargs, saturation=saturation)\n\u001b[32m   2373\u001b[39m agg_cls = WeightedAggregator \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mweight\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m p.plot_data \u001b[38;5;28;01melse\u001b[39;00m EstimateAggregator\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\seaborn\\_base.py:838\u001b[39m, in \u001b[36mVectorPlotter.map_hue\u001b[39m\u001b[34m(self, palette, order, norm, saturation)\u001b[39m\n\u001b[32m    837\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmap_hue\u001b[39m(\u001b[38;5;28mself\u001b[39m, palette=\u001b[38;5;28;01mNone\u001b[39;00m, order=\u001b[38;5;28;01mNone\u001b[39;00m, norm=\u001b[38;5;28;01mNone\u001b[39;00m, saturation=\u001b[32m1\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m838\u001b[39m     mapping = \u001b[43mHueMapping\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpalette\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnorm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msaturation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    839\u001b[39m     \u001b[38;5;28mself\u001b[39m._hue_map = mapping\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\seaborn\\_base.py:150\u001b[39m, in \u001b[36mHueMapping.__init__\u001b[39m\u001b[34m(self, plotter, palette, order, norm, saturation)\u001b[39m\n\u001b[32m    147\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m map_type == \u001b[33m\"\u001b[39m\u001b[33mcategorical\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    149\u001b[39m     cmap = norm = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m     levels, lookup_table = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcategorical_mapping\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpalette\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    152\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    154\u001b[39m \u001b[38;5;66;03m# --- Option 3: datetime mapping\u001b[39;00m\n\u001b[32m    155\u001b[39m \n\u001b[32m    156\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    157\u001b[39m     \u001b[38;5;66;03m# TODO this needs actual implementation\u001b[39;00m\n\u001b[32m    158\u001b[39m     cmap = norm = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\seaborn\\_base.py:234\u001b[39m, in \u001b[36mHueMapping.categorical_mapping\u001b[39m\u001b[34m(self, data, palette, order)\u001b[39m\n\u001b[32m    232\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(missing):\n\u001b[32m    233\u001b[39m         err = \u001b[33m\"\u001b[39m\u001b[33mThe palette dictionary is missing keys: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m234\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(err.format(missing))\n\u001b[32m    236\u001b[39m     lookup_table = palette\n\u001b[32m    238\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mValueError\u001b[39m: The palette dictionary is missing keys: {'generated_uncorrelated', 'generated_strongly', 'generated_similar', 'generated_weakly'}"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Graphique 1 : Qualité (valeur totale) par algorithme, couleur = correlation\n",
    "if results_df is not None:\n",
    "    df = results_df.copy()\n",
    "    # moyenne valeur par algorithme x correlation\n",
    "    agg = df.groupby(['algorithm', 'correlation'])['value'].mean().reset_index()\n",
    "    \n",
    "    plt.figure(figsize=(12,6))\n",
    "    # Palette pour toutes les catégories\n",
    "    palette_map = {\n",
    "        'large_scale': '#1f77b4',\n",
    "        'low_dimension': '#ff7f0e',\n",
    "        'uncorrelated': '#2ca02c',\n",
    "        'weakly_correlated': '#d62728',\n",
    "        'strongly_correlated': '#9467bd',\n",
    "        'generated_uncorrelated': '#8c564b',\n",
    "        'generated_weakly_correlated': '#e377c2',\n",
    "        'generated_strongly_correlated': '#7f7f7f',\n",
    "        'generated_similar_weights': '#bcbd22',\n",
    "        'generated_inverse_strongly_correlated': '#17becf',\n",
    "        # Versions courtes au cas où\n",
    "        'generated_weakly': '#e377c2',\n",
    "        'generated_strongly': '#7f7f7f',\n",
    "        'generated_similar': '#bcbd22',\n",
    "    }\n",
    "    # Filtrer la palette pour ne garder que les catégories présentes\n",
    "    categories_present = agg['correlation'].unique()\n",
    "    palette_filtered = {k: v for k, v in palette_map.items() if k in categories_present}\n",
    "    # Ajouter des couleurs auto pour catégories inconnues\n",
    "    for cat in categories_present:\n",
    "        if cat not in palette_filtered:\n",
    "            palette_filtered[cat] = None\n",
    "    \n",
    "    sns.barplot(data=agg, x='algorithm', y='value', hue='correlation', palette=palette_filtered)\n",
    "    plt.xlabel('Algorithme')\n",
    "    plt.ylabel('Valeur totale moyenne')\n",
    "    plt.title('Valeur obtenue par algorithme (couleur = type de dataset)')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"results_df non chargé. Exécutez d'abord la cellule qui charge 'benchmark_results.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graphique 2 : Temps d'exécution par taille n\n",
    "if results_df is not None:\n",
    "    df = results_df.copy()\n",
    "    \n",
    "    # Moyenne du temps par algorithme et taille n\n",
    "    agg_time = df.groupby(['algorithm', 'n'])['time_ms'].mean().reset_index()\n",
    "    \n",
    "    # Trier par n\n",
    "    agg_time = agg_time.sort_values('n')\n",
    "    \n",
    "    plt.figure(figsize=(12,6))\n",
    "    \n",
    "    # Utiliser lineplot avec algorithme comme hue (couleur)\n",
    "    sns.lineplot(data=agg_time, x='n', y='time_ms', hue='algorithm', marker='o', palette='husl', linewidth=2, markersize=8)\n",
    "    \n",
    "    plt.xlabel('Taille du problème (n)')\n",
    "    plt.ylabel('Temps moyen (ms)')\n",
    "    plt.title('Temps d\\'exécution par taille de problème')\n",
    "    plt.yscale('log')\n",
    "    \n",
    "    # Axe X avec ticks tous les 1000\n",
    "    max_n = int(agg_time['n'].max())\n",
    "    plt.xticks(range(0, max_n + 1000, 1000))\n",
    "    plt.xlim(0, max_n + 500)\n",
    "    \n",
    "    plt.legend(title='Algorithme', bbox_to_anchor=(1.05,1), loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"results_df non chargé. Exécutez d'abord la cellule qui charge 'benchmark_results.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graphique 2bis : Temps d'exécution par groupe d'algorithmes\n",
    "if results_df is not None:\n",
    "    df = results_df.copy()\n",
    "    agg_time = df.groupby(['algorithm', 'n'])['time_ms'].mean().reset_index()\n",
    "    \n",
    "    # Groupe 1: Algorithmes limités (n <= 1000)\n",
    "    algos_limited = ['Brute Force', 'Dynamic Programming', 'DP Top-Down', 'Branch and Bound']\n",
    "    \n",
    "    # Groupe 2: Algorithmes scalables (n jusqu'à 10000)\n",
    "    algos_scalable = ['Greedy Ratio', 'Greedy Value', 'Greedy Weight', 'Fractional Knapsack',\n",
    "                      'Randomized', 'Genetic Algorithm', 'Genetic Adaptive', \n",
    "                      'Simulated Annealing', 'SA Adaptive',\n",
    "                      'FTPAS (ε=0.1)', 'FTPAS (ε=0.05)', 'FTPAS Adaptive']\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # --- Graphique 2a: Algorithmes Exacts (n <= 1000) ---\n",
    "    ax1 = axes[0]\n",
    "    data_limited = agg_time[agg_time['algorithm'].isin(algos_limited)]\n",
    "    for algo in algos_limited:\n",
    "        algo_data = data_limited[data_limited['algorithm'] == algo].sort_values('n')\n",
    "        if len(algo_data) > 0:\n",
    "            color = ALGO_COLORS.get(algo, '#333333')\n",
    "            ax1.plot(algo_data['n'], algo_data['time_ms'], 'o-', label=algo, color=color, linewidth=2, markersize=6)\n",
    "    ax1.set_xlabel('Taille du problème (n)', fontsize=11)\n",
    "    ax1.set_ylabel('Temps moyen (ms)', fontsize=11)\n",
    "    ax1.set_title('Algorithmes Exacts (n ≤ 1000)', fontsize=12, fontweight='bold')\n",
    "    ax1.set_yscale('log')\n",
    "    ax1.set_xticks(range(0, 1100, 100))\n",
    "    ax1.set_xlim(0, 1050)\n",
    "    ax1.legend(fontsize=9, loc='upper left')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # --- Graphique 2b: Algorithmes Scalables (n <= 10000) ---\n",
    "    ax2 = axes[1]\n",
    "    data_scalable = agg_time[agg_time['algorithm'].isin(algos_scalable)]\n",
    "    for algo in algos_scalable:\n",
    "        algo_data = data_scalable[data_scalable['algorithm'] == algo].sort_values('n')\n",
    "        if len(algo_data) > 0:\n",
    "            color = ALGO_COLORS.get(algo, '#333333')\n",
    "            ax2.plot(algo_data['n'], algo_data['time_ms'], 'o-', label=algo, color=color, linewidth=2, markersize=6)\n",
    "    ax2.set_xlabel('Taille du problème (n)', fontsize=11)\n",
    "    ax2.set_ylabel('Temps moyen (ms)', fontsize=11)\n",
    "    ax2.set_title('Algorithmes Scalables (n ≤ 10000)', fontsize=12, fontweight='bold')\n",
    "    ax2.set_yscale('log')\n",
    "    ax2.set_xticks(range(0, 11000, 1000))\n",
    "    ax2.set_xlim(0, 10500)\n",
    "    ax2.legend(fontsize=8, loc='upper left')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"results_df non chargé.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graphique 3 : Scatter Temps (X) vs Valeur (Y), 1 point = 1 algorithme (moyennes), annoté\n",
    "if results_df is not None:\n",
    "    df = results_df.copy()\n",
    "    # prendre la moyenne temps et valeur par algorithme\n",
    "    summary = df.groupby('algorithm').agg({'time_ms':'mean','value':'mean'}).reset_index()\n",
    "    \n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.scatter(summary['time_ms'], summary['value'], s=120, alpha=0.8)\n",
    "    for i, row in summary.iterrows():\n",
    "        plt.text(row['time_ms'], row['value'], row['algorithm'], fontsize=9,\n",
    "                 verticalalignment='bottom', horizontalalignment='right')\n",
    "    plt.xscale('log')\n",
    "    plt.xlabel('Temps moyen (ms)')\n",
    "    plt.ylabel('Valeur moyenne')\n",
    "    plt.title('Compromis Temps vs Qualité (un point = un algorithme)')\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"results_df non chargé. Exécutez d'abord la cellule qui charge 'benchmark_results.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if results_df is not None:\n",
    "    df = results_df.copy()\n",
    "    \n",
    "    algorithms = df['algorithm'].unique()\n",
    "    \n",
    "    # couleurs pour les deux catégories principales\n",
    "    colors_cat = {'large_scale':'#1f77b4','low_dimension':'#ff7f0e'}\n",
    "    \n",
    "    for algo in algorithms:\n",
    "        algo_data = df[df['algorithm'] == algo].copy()\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "        fig.suptitle(f'Performance : {algo}', fontsize=14, fontweight='bold')\n",
    "        \n",
    "        # Valeur par catégorie et taille n\n",
    "        if 'category' in algo_data.columns:\n",
    "            agg_value = algo_data.groupby(['category', 'n'])['value'].mean().reset_index()\n",
    "            groups = agg_value['category'].unique()\n",
    "        else:\n",
    "            # fallback vers 'correlation' si présent\n",
    "            agg_value = algo_data.groupby(['correlation', 'n'])['value'].mean().reset_index()\n",
    "            agg_value = agg_value.rename(columns={'correlation':'category'})\n",
    "            groups = agg_value['category'].unique()\n",
    "\n",
    "        for cat in groups:\n",
    "            cat_data = agg_value[agg_value['category'] == cat]\n",
    "            axes[0].plot(cat_data['n'], cat_data['value'], marker='o', label=cat, linewidth=2,\n",
    "                         color=colors_cat.get(cat, None))\n",
    "        \n",
    "        axes[0].set_xlabel('Taille n', fontsize=11)\n",
    "        axes[0].set_ylabel('Valeur moyenne', fontsize=11)\n",
    "        axes[0].set_title('Valeur par taille et catégorie')\n",
    "        axes[0].legend()\n",
    "        axes[0].grid(alpha=0.3)\n",
    "        axes[0].set_xscale('log')\n",
    "        \n",
    "        # Temps d'exécution par taille n\n",
    "        if 'category' in algo_data.columns:\n",
    "            agg_time = algo_data.groupby(['category', 'n'])['time_ms'].mean().reset_index()\n",
    "        else:\n",
    "            agg_time = algo_data.groupby(['correlation', 'n'])['time_ms'].mean().reset_index()\n",
    "            agg_time = agg_time.rename(columns={'correlation':'category'})\n",
    "\n",
    "        for cat in agg_time['category'].unique():\n",
    "            cat_data = agg_time[agg_time['category'] == cat]\n",
    "            axes[1].plot(cat_data['n'], cat_data['time_ms'], marker='s', label=cat, linewidth=2,\n",
    "                         color=colors_cat.get(cat, None))\n",
    "        \n",
    "        axes[1].set_xlabel('Taille n', fontsize=11)\n",
    "        axes[1].set_ylabel('Temps moyen (ms)', fontsize=11)\n",
    "        axes[1].set_title('Temps d\\'exécution par taille')\n",
    "        axes[1].legend()\n",
    "        axes[1].grid(alpha=0.3)\n",
    "        axes[1].set_xscale('log')\n",
    "        axes[1].set_yscale('log')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        print(f\"\\n{'='*60}\\n\")\n",
    "else:\n",
    "    print(\"results_df non chargé. Exécutez d'abord la cellule qui charge 'benchmark_results.csv'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Analyse par Type de Corrélation\n",
    "\n",
    "Ces graphiques comparent les performances des algorithmes selon le type de données :\n",
    "- **uncorrelated**: Poids et valeurs indépendants\n",
    "- **weakly_correlated**: Légère corrélation entre poids et valeurs  \n",
    "- **strongly_correlated**: Forte corrélation (valeur ≈ poids + constante)\n",
    "\n",
    "Les instances fortement corrélées sont généralement plus difficiles pour les heuristiques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graphique 4 : Comparaison par type de corrélation\n",
    "if results_df is not None:\n",
    "    df = results_df.copy()\n",
    "    \n",
    "    # Vérifier quels types de corrélation sont présents\n",
    "    correlation_types = df['correlation'].unique()\n",
    "    print(f\"Types de corrélation présents: {correlation_types}\")\n",
    "    \n",
    "    # Palette de couleurs pour les corrélations\n",
    "    corr_colors = {\n",
    "        'uncorrelated': '#2ecc71',        # Vert\n",
    "        'weakly_correlated': '#f39c12',   # Orange\n",
    "        'strongly_correlated': '#e74c3c', # Rouge\n",
    "        'low_dimension': '#3498db',       # Bleu\n",
    "        'large_scale': '#9b59b6'          # Violet\n",
    "    }\n",
    "    \n",
    "    # Si on a les 3 types de corrélation, créer des graphiques comparatifs\n",
    "    corr_types_present = [c for c in ['uncorrelated', 'weakly_correlated', 'strongly_correlated'] \n",
    "                          if c in correlation_types]\n",
    "    \n",
    "    if len(corr_types_present) > 0:\n",
    "        print(f\"\\nAnalyse des corrélations: {corr_types_present}\")\n",
    "        \n",
    "        # ---- Graphique 4a: Valeur moyenne par algorithme et corrélation ----\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "        \n",
    "        # Filtrer seulement les types de corrélation\n",
    "        df_corr = df[df['correlation'].isin(corr_types_present)]\n",
    "        \n",
    "        if len(df_corr) > 0:\n",
    "            # Valeur moyenne\n",
    "            agg_val = df_corr.groupby(['algorithm', 'correlation'])['value'].mean().reset_index()\n",
    "            pivot_val = agg_val.pivot(index='algorithm', columns='correlation', values='value')\n",
    "            \n",
    "            ax1 = axes[0]\n",
    "            pivot_val.plot(kind='bar', ax=ax1, color=[corr_colors.get(c, '#333') for c in pivot_val.columns])\n",
    "            ax1.set_xlabel('Algorithme', fontsize=11)\n",
    "            ax1.set_ylabel('Valeur moyenne', fontsize=11)\n",
    "            ax1.set_title('Valeur obtenue par type de corrélation', fontsize=12, fontweight='bold')\n",
    "            ax1.tick_params(axis='x', rotation=45)\n",
    "            ax1.legend(title='Corrélation')\n",
    "            ax1.grid(alpha=0.3, axis='y')\n",
    "            \n",
    "            # Temps moyen\n",
    "            agg_time = df_corr.groupby(['algorithm', 'correlation'])['time_ms'].mean().reset_index()\n",
    "            pivot_time = agg_time.pivot(index='algorithm', columns='correlation', values='time_ms')\n",
    "            \n",
    "            ax2 = axes[1]\n",
    "            pivot_time.plot(kind='bar', ax=ax2, color=[corr_colors.get(c, '#333') for c in pivot_time.columns])\n",
    "            ax2.set_xlabel('Algorithme', fontsize=11)\n",
    "            ax2.set_ylabel('Temps moyen (ms)', fontsize=11)\n",
    "            ax2.set_title('Temps d\\'exécution par type de corrélation', fontsize=12, fontweight='bold')\n",
    "            ax2.tick_params(axis='x', rotation=45)\n",
    "            ax2.set_yscale('log')\n",
    "            ax2.legend(title='Corrélation')\n",
    "            ax2.grid(alpha=0.3, axis='y')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        else:\n",
    "            print(\"Pas de données pour les types de corrélation\")\n",
    "    else:\n",
    "        print(\"\\nLes benchmarks de corrélation (uncorrelated, weakly_correlated, strongly_correlated)\")\n",
    "        print(\"n'ont pas encore été exécutés.\")\n",
    "        print(\"Pour les inclure, relancez run_all_benchmarks() après avoir modifié discover_benchmarks()\")\n",
    "else:\n",
    "    print(\"results_df non chargé.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graphique 5 : Évolution des performances selon n pour chaque type de corrélation\n",
    "if results_df is not None:\n",
    "    df = results_df.copy()\n",
    "    \n",
    "    # Palette de couleurs pour les corrélations\n",
    "    corr_colors = {\n",
    "        'uncorrelated': '#2ecc71',\n",
    "        'weakly_correlated': '#f39c12', \n",
    "        'strongly_correlated': '#e74c3c',\n",
    "        'low_dimension': '#3498db',\n",
    "        'large_scale': '#9b59b6'\n",
    "    }\n",
    "    \n",
    "    # Sélectionner quelques algorithmes représentatifs pour la lisibilité\n",
    "    algos_to_show = ['Dynamic Programming', 'Greedy Ratio', 'Genetic Algorithm', 'Simulated Annealing']\n",
    "    algos_present = [a for a in algos_to_show if a in df['algorithm'].unique()]\n",
    "    \n",
    "    if len(algos_present) > 0:\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "        axes = axes.flatten()\n",
    "        \n",
    "        for idx, algo in enumerate(algos_present[:4]):\n",
    "            ax = axes[idx]\n",
    "            algo_data = df[df['algorithm'] == algo]\n",
    "            \n",
    "            for corr in algo_data['correlation'].unique():\n",
    "                corr_data = algo_data[algo_data['correlation'] == corr]\n",
    "                agg = corr_data.groupby('n').agg({'value': 'mean', 'time_ms': 'mean'}).reset_index()\n",
    "                agg = agg.sort_values('n')\n",
    "                \n",
    "                color = corr_colors.get(corr, '#333')\n",
    "                ax.plot(agg['n'], agg['value'], 'o-', label=corr, color=color, linewidth=2, markersize=6)\n",
    "            \n",
    "            ax.set_xlabel('Taille n', fontsize=10)\n",
    "            ax.set_ylabel('Valeur moyenne', fontsize=10)\n",
    "            ax.set_title(f'{algo}', fontsize=11, fontweight='bold')\n",
    "            ax.legend(fontsize=8)\n",
    "            ax.grid(alpha=0.3)\n",
    "            ax.set_xscale('log')\n",
    "        \n",
    "        plt.suptitle('Valeur obtenue par taille et type de corrélation', fontsize=13, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "else:\n",
    "    print(\"results_df non chargé.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graphique 6 : Heatmap de performance relative par corrélation\n",
    "if results_df is not None:\n",
    "    df = results_df.copy()\n",
    "    \n",
    "    # Calculer la valeur moyenne par algorithme et corrélation\n",
    "    pivot = df.groupby(['algorithm', 'correlation'])['value'].mean().unstack()\n",
    "    \n",
    "    if pivot.shape[1] > 1:  # Au moins 2 types de corrélation\n",
    "        # Normaliser par ligne (algorithme) pour voir les différences relatives\n",
    "        pivot_norm = pivot.div(pivot.max(axis=1), axis=0) * 100\n",
    "        \n",
    "        plt.figure(figsize=(12, 8))\n",
    "        sns.heatmap(pivot_norm, annot=True, fmt='.1f', cmap='RdYlGn', \n",
    "                    cbar_kws={'label': '% de la meilleure valeur'},\n",
    "                    linewidths=0.5)\n",
    "        plt.title('Performance relative par type de corrélation\\n', \n",
    "                  fontsize=12, fontweight='bold')\n",
    "        plt.xlabel('Type de corrélation', fontsize=11)\n",
    "        plt.ylabel('Algorithme', fontsize=11)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "else:\n",
    "    print(\"results_df non chargé.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graphique 7 : Tableau récapitulatif détaillé\n",
    "if results_df is not None:\n",
    "    df = results_df.copy()\n",
    "    \n",
    "    # Créer un tableau récapitulatif complet\n",
    "    summary = df.groupby(['correlation', 'algorithm']).agg({\n",
    "        'value': ['mean', 'std', 'count'],\n",
    "        'time_ms': ['mean', 'std'],\n",
    "        'n': ['min', 'max']\n",
    "    }).round(2)\n",
    "    \n",
    "    # Aplatir les colonnes multi-niveaux\n",
    "    summary.columns = ['_'.join(col).strip() for col in summary.columns.values]\n",
    "    summary = summary.reset_index()\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"TABLEAU RÉCAPITULATIF DES RÉSULTATS PAR TYPE DE CORRÉLATION\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for corr in summary['correlation'].unique():\n",
    "        print(f\"\\n>>> {corr.upper()} <<<\")\n",
    "        print(\"-\" * 60)\n",
    "        corr_data = summary[summary['correlation'] == corr]\n",
    "        \n",
    "        # Trier par valeur moyenne décroissante\n",
    "        corr_data = corr_data.sort_values('value_mean', ascending=False)\n",
    "        \n",
    "        for _, row in corr_data.iterrows():\n",
    "            print(f\"  {row['algorithm']:25s} | Valeur: {row['value_mean']:>10.1f} ± {row['value_std']:>8.1f} | \"\n",
    "                  f\"Temps: {row['time_ms_mean']:>10.2f}ms | n: {int(row['n_min'])}-{int(row['n_max'])}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "else:\n",
    "    print(\"results_df non chargé.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Analyse de Complexité\n",
    "\n",
    "### 7.1 Complexité Théorique des Algorithmes\n",
    "\n",
    "| Algorithme | Complexité Temporelle | Complexité Spatiale | Type |\n",
    "|------------|----------------------|---------------------|------|\n",
    "| **Brute Force** | $O(2^n)$ | $O(n)$ | Exact |\n",
    "| **DP Bottom-Up** | $O(n \\cdot C)$ | $O(n \\cdot C)$ | Exact (pseudo-poly) |\n",
    "| **DP Top-Down** | $O(n \\cdot C)$ | $O(n \\cdot C)$ | Exact (pseudo-poly) |\n",
    "| **Branch & Bound** | $O(2^n)$ pire cas | $O(n)$ | Exact |\n",
    "| **Greedy (ratio)** | $O(n \\log n)$ | $O(n)$ | Approximation |\n",
    "| **Fractional Knapsack** | $O(n \\log n)$ | $O(n)$ | Exact (relaxé) |\n",
    "| **Randomized** | $O(k \\cdot n)$ | $O(n)$ | Heuristique |\n",
    "| **Genetic Algorithm** | $O(g \\cdot p \\cdot n)$ | $O(p \\cdot n)$ | Métaheuristique |\n",
    "| **Simulated Annealing** | $O(i \\cdot n)$ | $O(n)$ | Métaheuristique |\n",
    "| **FPTAS** | $O(n^2 / \\varepsilon)$ | $O(n / \\varepsilon)$ | Approximation |\n",
    "\n",
    "$n$ = nombre d'items\n",
    "$C$ = capacité \n",
    "$k$ = itérations\n",
    "$g$ = générations\n",
    "$p$ = population\n",
    "$i$ = itérations SA\n",
    "$\\varepsilon$ = paramètre d'approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Classification des Algorithmes\n",
    "\n",
    "**Algorithmes Exacts:**\n",
    "- Garantissent la solution optimale\n",
    "- Brute Force: $O(2^n)$ - inutilisable pour $n > 25$\n",
    "- DP: $O(n \\cdot C)$ - pseudo-polynomial, dépend de la capacité\n",
    "- Branch & Bound: Élagage efficace, souvent meilleur que $O(2^n)$ en pratique\n",
    "\n",
    "**Algorithmes d'Approximation:**\n",
    "- FPTAS garantit $(1-\\varepsilon) \\times OPT$ en temps polynomial\n",
    "- Greedy ratio: approximation $\\frac{1}{2} \\times OPT$ garantie\n",
    "\n",
    "**Métaheuristiques:**\n",
    "- Pas de garantie théorique sur la qualité\n",
    "- Genetic Algorithm et Simulated Annealing explorent l'espace de recherche\n",
    "- Bons résultats en pratique, surtout pour grandes instances"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
