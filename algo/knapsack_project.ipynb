{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projet Knapsack Problem 0-1\n",
    "\n",
    "**Équipe:** Chaabane, Arman, Bartosz, Ahmed\n",
    "\n",
    "## Structure du projet\n",
    "\n",
    "1. Infrastructure commune (Classes et structures de données)\n",
    "2. Algorithmes implémentés\n",
    "3. Système de benchmarking complet\n",
    "4. Analyse comparative approfondie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Configuration et Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from itertools import combinations\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from types import SimpleNamespace\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "\n",
    "ALGO_COLORS = {\n",
    "    'Brute Force': '#e41a1c',\n",
    "    'Dynamic Programming': '#377eb8',\n",
    "    'DP Top-Down': '#4daf4a',\n",
    "    'Branch and Bound': '#984ea3',\n",
    "    'Greedy Ratio': '#ff7f00', \n",
    "    'Greedy Value': '#ffff33', \n",
    "    'Greedy Weight': '#a65628', \n",
    "    'Fractional Knapsack': '#f781bf',\n",
    "    'Randomized': '#999999', \n",
    "    'Genetic Algorithm': '#17becf',  \n",
    "    'Genetic Adaptive': '#1f77b4',      \n",
    "    'Simulated Annealing': '#d62728',   \n",
    "    'SA Adaptive': '#ff9896',          \n",
    "    'FTPAS (ε=0.1)': '#9467bd',         \n",
    "    'FTPAS (ε=0.05)': '#8c564b',        \n",
    "    'FTPAS Adaptive': '#e377c2',        \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Générateur de Benchmarks Knapsack\n",
    "\n",
    "Ce générateur permet de créer des fichiers de benchmark personnalisés avec différents paramètres :\n",
    "- **Type de corrélation** : uncorrelated, weakly_correlated, strongly_correlated, similar_weights\n",
    "- **Nombre d'items (n)** : nombre d'objets dans l'instance\n",
    "- **Plage des poids (R)** : valeur maximale pour les poids générés aléatoirement\n",
    "- **Capacité** : peut être spécifiée ou calculée automatiquement (généralement 50% de la somme des poids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dossier: benchmarks/generated\n",
      "Types: ['uncorrelated', 'weakly_correlated', 'strongly_correlated', 'similar_weights', 'inverse_strongly_correlated']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from typing import Literal, Optional, Tuple, List\n",
    "from dataclasses import dataclass\n",
    "\n",
    "GENERATED_DIR = \"benchmarks/generated\"\n",
    "\n",
    "@dataclass\n",
    "class KnapsackInstance:\n",
    "    n: int\n",
    "    capacity: int\n",
    "    weights: List[int]\n",
    "    values: List[int]\n",
    "    correlation_type: str\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"KnapsackInstance(n={self.n}, capacity={self.capacity}, type={self.correlation_type})\"\n",
    "\n",
    "\n",
    "class KnapsackBenchmarkGenerator:\n",
    "    CORRELATION_TYPES = [\n",
    "        'uncorrelated',\n",
    "        'weakly_correlated', \n",
    "        'strongly_correlated',\n",
    "        'similar_weights',\n",
    "        'inverse_strongly_correlated'\n",
    "    ]\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.rng = np.random.default_rng()\n",
    "    \n",
    "    def generate(self, n: int, R: int = 1000, capacity: Optional[int] = None,\n",
    "                 capacity_ratio: float = 0.5, correlation_type: str = 'uncorrelated',\n",
    "                 correlation_param: float = 100.0) -> KnapsackInstance:\n",
    "        if correlation_type not in self.CORRELATION_TYPES:\n",
    "            raise ValueError(f\"Type inconnu: {correlation_type}. Choix: {self.CORRELATION_TYPES}\")\n",
    "        \n",
    "        weights = self._generate_weights(n, R, correlation_type, correlation_param)\n",
    "        values = self._generate_values(weights, R, correlation_type, correlation_param)\n",
    "        \n",
    "        if capacity is None:\n",
    "            capacity = int(capacity_ratio * sum(weights))\n",
    "        capacity = max(1, capacity)\n",
    "        \n",
    "        return KnapsackInstance(n=n, capacity=capacity, weights=weights.tolist(), \n",
    "                                values=values.tolist(), correlation_type=correlation_type)\n",
    "    \n",
    "    def _generate_weights(self, n: int, R: int, correlation_type: str, correlation_param: float) -> np.ndarray:\n",
    "        if correlation_type == 'similar_weights':\n",
    "            weights = self.rng.normal(R / 2, correlation_param, n)\n",
    "            weights = np.clip(weights, 1, R).astype(int)\n",
    "        else:\n",
    "            weights = self.rng.integers(1, R + 1, n)\n",
    "        return weights.astype(int)\n",
    "    \n",
    "    def _generate_values(self, weights: np.ndarray, R: int, correlation_type: str, correlation_param: float) -> np.ndarray:\n",
    "        n = len(weights)\n",
    "        if correlation_type == 'uncorrelated':\n",
    "            values = self.rng.integers(1, R + 1, n)\n",
    "        elif correlation_type == 'weakly_correlated':\n",
    "            noise = self.rng.integers(-int(correlation_param), int(correlation_param) + 1, n)\n",
    "            values = np.maximum(weights + noise, 1)\n",
    "        elif correlation_type == 'strongly_correlated':\n",
    "            values = weights + int(correlation_param)\n",
    "        elif correlation_type == 'similar_weights':\n",
    "            values = self.rng.integers(1, R + 1, n)\n",
    "        elif correlation_type == 'inverse_strongly_correlated':\n",
    "            values = np.maximum(weights.max() - weights + int(correlation_param), 1)\n",
    "        return values.astype(int)\n",
    "    \n",
    "    def _build_filename(self, instance: KnapsackInstance, index: int = None, format: str = 'standard') -> str:\n",
    "        ext = '.kp' if format == 'kp' else '.txt'\n",
    "        base = f\"{instance.correlation_type}_n{instance.n}_c{instance.capacity}\"\n",
    "        if index is not None:\n",
    "            return f\"{base}_{index:03d}{ext}\"\n",
    "        return f\"{base}{ext}\"\n",
    "    \n",
    "    def save_to_file(self, instance: KnapsackInstance, filepath: str = None,\n",
    "                     index: int = None, format: Literal['standard', 'kp'] = 'standard') -> str:\n",
    "        if filepath is None:\n",
    "            filepath = os.path.join(GENERATED_DIR, self._build_filename(instance, index, format))\n",
    "        \n",
    "        os.makedirs(os.path.dirname(filepath) if os.path.dirname(filepath) else '.', exist_ok=True)\n",
    "        with open(filepath, 'w') as f:\n",
    "            if format == 'standard':\n",
    "                f.write(f\"{instance.n} {instance.capacity}\\n\")\n",
    "                for v, w in zip(instance.values, instance.weights):\n",
    "                    f.write(f\"{v} {w}\\n\")\n",
    "            elif format == 'kp':\n",
    "                f.write(f\"\\n{instance.n}\\n{instance.capacity}\\n\\n\")\n",
    "                for v, w in zip(instance.values, instance.weights):\n",
    "                    f.write(f\"{v} {w}\\n\")\n",
    "        return filepath\n",
    "\n",
    "\n",
    "def generate_benchmarks(n: int, capacity: int = None, correlation = 'uncorrelated',\n",
    "                        R: int = 1000, count: int = 1, format: str = 'standard') -> List[KnapsackInstance]:\n",
    "    \"\"\"\n",
    "    Génère un ou plusieurs fichiers de benchmark.\n",
    "    \n",
    "    Args:\n",
    "        n: Nombre d'items\n",
    "        capacity: Capacité du sac (None = 50% de la somme des poids)\n",
    "        correlation: Type ou liste de types ('uncorrelated', 'weakly_correlated', \n",
    "                     'strongly_correlated', 'similar_weights', 'inverse_strongly_correlated')\n",
    "        R: Poids max [1, R]\n",
    "        count: Nombre de fichiers à générer par type\n",
    "        format: 'standard' (.txt) ou 'kp'\n",
    "    \"\"\"\n",
    "    generator = KnapsackBenchmarkGenerator()\n",
    "    instances = []\n",
    "    \n",
    "    correlations = [correlation] if isinstance(correlation, str) else correlation\n",
    "    \n",
    "    for corr_type in correlations:\n",
    "        for i in range(count):\n",
    "            instance = generator.generate(n=n, R=R, capacity=capacity, correlation_type=corr_type)\n",
    "            index = i + 1 if count > 1 else None\n",
    "            filepath = generator.save_to_file(instance, index=index, format=format)\n",
    "            instances.append(instance)\n",
    "            print(f\"✓ {filepath}\")\n",
    "    \n",
    "    total = len(correlations) * count\n",
    "    print(f\"\\n{total} fichier(s) généré(s) dans {GENERATED_DIR}/\")\n",
    "    return instances\n",
    "\n",
    "\n",
    "print(f\"Dossier: {GENERATED_DIR}\")\n",
    "print(f\"Types: {KnapsackBenchmarkGenerator.CORRELATION_TYPES}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exemples d'utilisation du générateur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ benchmarks/generated\\uncorrelated_n100_c2000_001.txt\n",
      "✓ benchmarks/generated\\uncorrelated_n100_c2000_002.txt\n",
      "✓ benchmarks/generated\\uncorrelated_n100_c2000_003.txt\n",
      "✓ benchmarks/generated\\strongly_correlated_n100_c2000_001.txt\n",
      "✓ benchmarks/generated\\strongly_correlated_n100_c2000_002.txt\n",
      "✓ benchmarks/generated\\strongly_correlated_n100_c2000_003.txt\n",
      "✓ benchmarks/generated\\weakly_correlated_n100_c2000_001.txt\n",
      "✓ benchmarks/generated\\weakly_correlated_n100_c2000_002.txt\n",
      "✓ benchmarks/generated\\weakly_correlated_n100_c2000_003.txt\n",
      "\n",
      "9 fichier(s) généré(s) dans benchmarks/generated/\n",
      "✓ benchmarks/generated\\uncorrelated_n500_c2000_001.txt\n",
      "✓ benchmarks/generated\\uncorrelated_n500_c2000_002.txt\n",
      "✓ benchmarks/generated\\uncorrelated_n500_c2000_003.txt\n",
      "✓ benchmarks/generated\\strongly_correlated_n500_c2000_001.txt\n",
      "✓ benchmarks/generated\\strongly_correlated_n500_c2000_002.txt\n",
      "✓ benchmarks/generated\\strongly_correlated_n500_c2000_003.txt\n",
      "✓ benchmarks/generated\\weakly_correlated_n500_c2000_001.txt\n",
      "✓ benchmarks/generated\\weakly_correlated_n500_c2000_002.txt\n",
      "✓ benchmarks/generated\\weakly_correlated_n500_c2000_003.txt\n",
      "\n",
      "9 fichier(s) généré(s) dans benchmarks/generated/\n",
      "✓ benchmarks/generated\\uncorrelated_n1000_c2000_001.txt\n",
      "✓ benchmarks/generated\\uncorrelated_n1000_c2000_002.txt\n",
      "✓ benchmarks/generated\\uncorrelated_n1000_c2000_003.txt\n",
      "✓ benchmarks/generated\\strongly_correlated_n1000_c2000_001.txt\n",
      "✓ benchmarks/generated\\strongly_correlated_n1000_c2000_002.txt\n",
      "✓ benchmarks/generated\\strongly_correlated_n1000_c2000_003.txt\n",
      "✓ benchmarks/generated\\weakly_correlated_n1000_c2000_001.txt\n",
      "✓ benchmarks/generated\\weakly_correlated_n1000_c2000_002.txt\n",
      "✓ benchmarks/generated\\weakly_correlated_n1000_c2000_003.txt\n",
      "\n",
      "9 fichier(s) généré(s) dans benchmarks/generated/\n",
      "✓ benchmarks/generated\\uncorrelated_n2000_c2000_001.txt\n",
      "✓ benchmarks/generated\\uncorrelated_n2000_c2000_002.txt\n",
      "✓ benchmarks/generated\\uncorrelated_n2000_c2000_003.txt\n",
      "✓ benchmarks/generated\\strongly_correlated_n2000_c2000_001.txt\n",
      "✓ benchmarks/generated\\strongly_correlated_n2000_c2000_002.txt\n",
      "✓ benchmarks/generated\\strongly_correlated_n2000_c2000_003.txt\n",
      "✓ benchmarks/generated\\weakly_correlated_n2000_c2000_001.txt\n",
      "✓ benchmarks/generated\\weakly_correlated_n2000_c2000_002.txt\n",
      "✓ benchmarks/generated\\weakly_correlated_n2000_c2000_003.txt\n",
      "\n",
      "9 fichier(s) généré(s) dans benchmarks/generated/\n",
      "✓ benchmarks/generated\\uncorrelated_n5000_c2000_001.txt\n",
      "✓ benchmarks/generated\\uncorrelated_n5000_c2000_002.txt\n",
      "✓ benchmarks/generated\\strongly_correlated_n5000_c2000_001.txt\n",
      "✓ benchmarks/generated\\strongly_correlated_n5000_c2000_002.txt\n",
      "✓ benchmarks/generated\\weakly_correlated_n5000_c2000_001.txt\n",
      "✓ benchmarks/generated\\weakly_correlated_n5000_c2000_002.txt\n",
      "\n",
      "6 fichier(s) généré(s) dans benchmarks/generated/\n",
      "✓ benchmarks/generated\\uncorrelated_n10000_c2000.txt\n",
      "✓ benchmarks/generated\\strongly_correlated_n10000_c2000.txt\n",
      "✓ benchmarks/generated\\weakly_correlated_n10000_c2000.txt\n",
      "\n",
      "3 fichier(s) généré(s) dans benchmarks/generated/\n",
      "✓ benchmarks/generated\\similar_weights_n100_c2000_001.txt\n",
      "✓ benchmarks/generated\\similar_weights_n100_c2000_002.txt\n",
      "✓ benchmarks/generated\\similar_weights_n100_c2000_003.txt\n",
      "\n",
      "3 fichier(s) généré(s) dans benchmarks/generated/\n",
      "✓ benchmarks/generated\\similar_weights_n200_c2000_001.txt\n",
      "✓ benchmarks/generated\\similar_weights_n200_c2000_002.txt\n",
      "\n",
      "2 fichier(s) généré(s) dans benchmarks/generated/\n",
      "✓ benchmarks/generated\\similar_weights_n1000_c2000_001.txt\n",
      "✓ benchmarks/generated\\similar_weights_n1000_c2000_002.txt\n",
      "\n",
      "2 fichier(s) généré(s) dans benchmarks/generated/\n",
      "✓ benchmarks/generated\\similar_weights_n500_c2000_001.txt\n",
      "✓ benchmarks/generated\\similar_weights_n500_c2000_002.txt\n",
      "\n",
      "2 fichier(s) généré(s) dans benchmarks/generated/\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[KnapsackInstance(n=500, capacity=2000, weights=[485, 560, 457, 415, 338, 395, 572, 434, 477, 370, 413, 338, 406, 501, 507, 647, 478, 488, 455, 522, 496, 407, 398, 448, 506, 422, 467, 374, 492, 343, 368, 374, 593, 437, 706, 468, 342, 482, 520, 675, 300, 578, 560, 562, 538, 594, 365, 460, 501, 378, 456, 508, 573, 373, 503, 559, 541, 266, 644, 519, 520, 486, 625, 472, 595, 440, 461, 476, 658, 561, 667, 525, 436, 618, 576, 434, 540, 524, 601, 481, 596, 582, 360, 463, 490, 528, 340, 579, 330, 518, 708, 395, 388, 543, 667, 607, 530, 486, 513, 403, 362, 553, 422, 532, 429, 566, 458, 461, 532, 778, 398, 463, 404, 398, 613, 415, 544, 484, 559, 561, 319, 474, 518, 529, 462, 628, 617, 500, 664, 583, 537, 616, 484, 384, 548, 422, 465, 405, 478, 651, 453, 486, 420, 479, 519, 469, 654, 401, 784, 622, 479, 388, 532, 384, 470, 435, 536, 473, 377, 587, 513, 651, 572, 576, 445, 515, 468, 525, 287, 481, 640, 474, 372, 535, 465, 560, 473, 476, 477, 611, 599, 464, 466, 539, 435, 463, 492, 498, 537, 468, 305, 497, 511, 542, 686, 625, 499, 480, 592, 440, 472, 531, 437, 559, 640, 634, 258, 294, 387, 422, 639, 443, 460, 625, 496, 618, 560, 638, 570, 377, 581, 622, 511, 419, 545, 396, 572, 501, 629, 439, 451, 479, 511, 362, 519, 610, 464, 660, 456, 567, 417, 469, 583, 553, 433, 402, 518, 468, 662, 331, 588, 544, 455, 364, 494, 520, 519, 439, 470, 621, 388, 453, 564, 546, 355, 452, 705, 347, 361, 482, 549, 390, 670, 678, 417, 407, 532, 654, 331, 338, 258, 695, 553, 361, 536, 573, 99, 491, 710, 609, 505, 685, 691, 474, 590, 475, 506, 467, 535, 310, 565, 496, 285, 707, 772, 467, 633, 550, 457, 409, 678, 391, 345, 536, 441, 537, 549, 476, 477, 669, 517, 465, 536, 623, 613, 348, 505, 419, 586, 360, 586, 385, 317, 380, 435, 477, 647, 543, 553, 470, 494, 766, 618, 605, 547, 571, 379, 461, 463, 458, 445, 404, 562, 539, 433, 503, 508, 433, 691, 379, 560, 514, 505, 671, 469, 567, 543, 551, 491, 406, 489, 404, 607, 449, 424, 649, 379, 522, 531, 538, 480, 486, 664, 358, 480, 538, 362, 570, 551, 494, 372, 451, 428, 439, 511, 661, 332, 303, 503, 524, 548, 741, 494, 472, 666, 514, 688, 489, 569, 569, 296, 484, 315, 450, 468, 434, 562, 537, 483, 418, 615, 822, 393, 334, 601, 380, 465, 308, 479, 679, 450, 400, 466, 472, 536, 536, 526, 512, 545, 430, 553, 670, 537, 284, 442, 429, 693, 350, 522, 490, 330, 641, 470, 580, 508, 547, 505, 303, 437, 502, 610, 526, 384, 528, 433, 413, 451, 422, 476, 594, 598, 600, 531, 479, 453, 528, 408, 489, 593, 547, 338, 599, 699, 559, 404, 281, 677, 574, 436, 296, 376, 529, 592, 713, 428, 383, 385, 682, 276, 452], values=[942, 24, 257, 142, 784, 252, 135, 377, 894, 605, 137, 116, 226, 308, 204, 656, 689, 808, 212, 631, 415, 88, 428, 102, 628, 874, 652, 180, 897, 210, 426, 890, 997, 426, 997, 813, 304, 69, 612, 848, 705, 734, 912, 901, 694, 928, 977, 862, 967, 425, 936, 24, 172, 898, 103, 391, 538, 506, 990, 23, 559, 813, 374, 893, 865, 405, 984, 124, 252, 417, 283, 371, 317, 309, 126, 867, 124, 389, 715, 494, 256, 454, 46, 482, 343, 199, 488, 185, 928, 610, 105, 517, 733, 567, 295, 991, 105, 654, 505, 869, 764, 590, 311, 68, 961, 232, 990, 338, 699, 754, 445, 932, 712, 866, 895, 426, 912, 77, 90, 88, 471, 803, 527, 162, 217, 183, 193, 985, 640, 436, 424, 794, 691, 313, 706, 580, 810, 948, 584, 909, 70, 979, 242, 216, 19, 189, 980, 797, 593, 326, 297, 607, 528, 616, 838, 547, 800, 428, 366, 288, 189, 413, 30, 748, 857, 134, 230, 424, 789, 168, 411, 153, 819, 68, 853, 981, 545, 28, 768, 101, 281, 517, 349, 984, 443, 844, 108, 594, 40, 760, 754, 754, 246, 752, 876, 869, 120, 503, 160, 758, 826, 395, 901, 901, 71, 273, 364, 513, 625, 422, 267, 914, 656, 855, 575, 847, 627, 679, 177, 628, 532, 246, 636, 712, 62, 281, 582, 828, 696, 632, 514, 207, 658, 232, 816, 340, 276, 29, 294, 996, 947, 674, 363, 990, 648, 928, 945, 298, 828, 535, 731, 854, 733, 27, 907, 741, 591, 539, 860, 719, 204, 530, 762, 264, 941, 804, 581, 420, 846, 939, 682, 831, 240, 618, 398, 648, 197, 665, 678, 427, 395, 947, 133, 841, 711, 841, 711, 190, 449, 801, 27, 438, 741, 958, 695, 361, 289, 229, 557, 509, 961, 579, 232, 679, 453, 828, 11, 433, 600, 908, 188, 901, 915, 512, 641, 333, 156, 725, 634, 502, 582, 153, 121, 161, 316, 62, 621, 933, 422, 259, 340, 741, 368, 821, 502, 955, 850, 21, 300, 994, 167, 766, 64, 55, 16, 359, 944, 668, 409, 766, 985, 943, 682, 557, 525, 826, 196, 311, 398, 511, 473, 288, 587, 846, 636, 870, 898, 83, 120, 742, 75, 860, 484, 383, 154, 444, 18, 946, 81, 698, 493, 573, 877, 421, 355, 794, 313, 957, 370, 814, 676, 812, 132, 897, 752, 63, 902, 896, 65, 652, 948, 802, 958, 405, 20, 642, 345, 229, 960, 653, 777, 941, 504, 753, 488, 822, 698, 861, 170, 620, 318, 227, 492, 758, 768, 1, 519, 725, 281, 290, 51, 404, 681, 757, 688, 780, 649, 385, 364, 559, 522, 690, 563, 448, 780, 237, 959, 899, 983, 409, 919, 790, 801, 728, 911, 148, 934, 548, 172, 175, 615, 683, 630, 214, 338, 859, 292, 976, 188, 565, 100, 926, 936, 602, 177, 603, 894, 749, 233, 187, 761, 960, 549, 93, 742, 737, 307, 67, 133, 564, 126, 379, 96, 253, 53, 317, 493, 561, 346, 767], correlation_type='similar_weights'),\n",
       " KnapsackInstance(n=500, capacity=2000, weights=[585, 527, 516, 405, 532, 592, 415, 564, 559, 431, 610, 551, 343, 503, 462, 441, 511, 710, 489, 461, 567, 432, 640, 504, 476, 425, 500, 343, 483, 633, 517, 465, 505, 470, 318, 529, 363, 450, 608, 510, 606, 774, 650, 652, 331, 484, 471, 468, 224, 600, 405, 546, 589, 586, 481, 661, 584, 531, 521, 420, 519, 470, 419, 619, 466, 464, 411, 505, 515, 381, 543, 399, 560, 480, 316, 464, 526, 584, 653, 475, 485, 522, 510, 413, 455, 599, 518, 317, 465, 435, 548, 532, 586, 386, 326, 591, 591, 478, 468, 575, 505, 676, 534, 467, 387, 374, 511, 568, 652, 439, 670, 448, 420, 597, 490, 468, 518, 459, 471, 512, 439, 512, 539, 436, 481, 730, 484, 367, 584, 587, 603, 428, 369, 577, 517, 568, 539, 532, 582, 627, 330, 518, 575, 453, 482, 593, 502, 623, 547, 423, 623, 310, 453, 431, 472, 570, 503, 389, 695, 519, 494, 444, 439, 488, 574, 553, 491, 447, 399, 612, 583, 417, 523, 390, 377, 588, 482, 629, 468, 543, 591, 664, 444, 527, 596, 482, 587, 457, 547, 478, 383, 450, 572, 455, 525, 628, 489, 585, 597, 464, 474, 501, 631, 540, 616, 387, 391, 331, 466, 344, 559, 404, 454, 719, 633, 518, 435, 554, 644, 607, 428, 681, 388, 460, 535, 582, 499, 503, 399, 572, 544, 495, 416, 639, 829, 419, 357, 544, 485, 484, 463, 355, 697, 485, 512, 467, 581, 526, 536, 514, 594, 555, 497, 415, 519, 485, 518, 475, 446, 361, 573, 445, 627, 634, 533, 649, 403, 538, 531, 410, 519, 547, 450, 406, 282, 652, 608, 702, 539, 499, 621, 503, 355, 411, 530, 498, 426, 635, 705, 458, 521, 428, 388, 433, 467, 333, 588, 354, 596, 454, 337, 547, 472, 544, 546, 480, 490, 361, 566, 557, 497, 488, 330, 410, 368, 772, 450, 409, 611, 450, 438, 473, 696, 588, 539, 512, 453, 489, 611, 426, 267, 519, 489, 480, 405, 513, 587, 338, 310, 569, 533, 440, 343, 675, 412, 511, 471, 705, 567, 523, 555, 630, 434, 355, 563, 543, 503, 394, 621, 383, 415, 470, 359, 441, 197, 381, 462, 507, 673, 511, 414, 530, 388, 495, 428, 380, 430, 471, 438, 510, 554, 484, 744, 529, 500, 450, 605, 638, 490, 667, 520, 439, 462, 427, 536, 609, 466, 547, 428, 461, 435, 657, 410, 503, 690, 577, 282, 470, 551, 445, 598, 454, 613, 408, 436, 392, 540, 513, 317, 608, 487, 277, 471, 377, 546, 621, 397, 511, 489, 571, 527, 496, 452, 459, 548, 607, 487, 555, 515, 536, 577, 354, 270, 712, 532, 604, 718, 555, 597, 564, 538, 601, 602, 262, 381, 398, 444, 270, 485, 488, 477, 353, 566, 469, 527, 551, 541, 544, 433, 451, 475, 324, 338, 615, 396, 614, 414, 572, 516, 502, 500, 515, 356, 588, 334, 627, 530, 490, 486, 639, 465, 373, 667, 454, 464, 362, 624, 331, 518, 592], values=[13, 55, 862, 342, 281, 408, 85, 603, 721, 429, 770, 230, 857, 569, 162, 617, 561, 310, 808, 193, 419, 643, 109, 614, 527, 362, 89, 395, 392, 536, 180, 442, 586, 650, 345, 83, 294, 476, 218, 917, 299, 251, 120, 680, 275, 417, 958, 719, 25, 843, 522, 650, 444, 61, 482, 464, 338, 36, 909, 225, 328, 387, 252, 78, 398, 546, 960, 482, 389, 968, 621, 97, 71, 533, 327, 772, 963, 313, 973, 231, 525, 187, 692, 311, 490, 794, 110, 510, 728, 407, 81, 203, 63, 802, 371, 781, 114, 19, 139, 82, 497, 187, 631, 422, 166, 167, 357, 583, 842, 447, 824, 315, 282, 811, 183, 151, 323, 483, 605, 127, 69, 365, 323, 397, 532, 480, 548, 160, 278, 597, 899, 536, 851, 280, 221, 152, 716, 179, 683, 791, 556, 692, 597, 619, 782, 590, 504, 685, 736, 727, 390, 409, 98, 220, 734, 228, 660, 443, 980, 767, 398, 945, 521, 972, 688, 441, 17, 801, 702, 788, 743, 745, 243, 145, 563, 28, 468, 192, 968, 961, 832, 269, 309, 342, 647, 142, 156, 854, 820, 867, 709, 785, 780, 621, 151, 615, 525, 529, 327, 553, 280, 711, 968, 141, 394, 879, 989, 219, 147, 321, 381, 78, 890, 985, 717, 959, 789, 878, 850, 82, 76, 21, 673, 379, 948, 703, 566, 572, 27, 679, 393, 173, 710, 204, 664, 468, 541, 803, 290, 885, 840, 220, 303, 155, 413, 694, 884, 853, 394, 53, 618, 128, 700, 18, 915, 945, 883, 149, 812, 505, 649, 46, 680, 811, 888, 652, 874, 901, 525, 79, 493, 522, 369, 574, 226, 580, 36, 256, 368, 500, 853, 523, 701, 546, 966, 858, 877, 481, 755, 842, 411, 872, 682, 629, 917, 322, 504, 582, 171, 44, 515, 398, 789, 697, 515, 615, 439, 938, 245, 827, 289, 151, 398, 993, 860, 238, 307, 124, 515, 771, 554, 122, 69, 274, 94, 876, 961, 68, 352, 873, 25, 247, 813, 984, 889, 6, 327, 721, 48, 560, 515, 349, 987, 408, 334, 806, 865, 919, 53, 815, 908, 347, 923, 849, 54, 652, 85, 124, 584, 108, 129, 835, 297, 639, 707, 488, 864, 936, 858, 840, 105, 193, 996, 624, 659, 282, 616, 12, 177, 207, 727, 365, 769, 141, 931, 610, 683, 89, 194, 111, 286, 855, 783, 425, 361, 730, 986, 382, 674, 681, 333, 64, 418, 773, 903, 420, 445, 32, 579, 203, 554, 113, 398, 495, 918, 332, 694, 63, 296, 769, 214, 277, 983, 564, 94, 855, 436, 381, 799, 869, 254, 325, 86, 302, 593, 606, 214, 165, 721, 703, 849, 434, 755, 613, 518, 930, 612, 591, 739, 884, 750, 692, 804, 878, 395, 386, 806, 36, 144, 719, 492, 301, 839, 819, 594, 666, 740, 385, 911, 17, 721, 503, 133, 577, 249, 926, 364, 689, 239, 214, 745, 516, 481, 789, 809, 38, 671, 432, 673, 314, 441, 391, 330, 97, 95, 412, 617, 592, 398, 100], correlation_type='similar_weights')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# GÉNÉRATION DE BENCHMARKS\n",
    "# =============================================================================\n",
    "# Types: 'uncorrelated', 'weakly_correlated', 'strongly_correlated', \n",
    "#        'similar_weights', 'inverse_strongly_correlated'\n",
    "# =============================================================================\n",
    "generate_benchmarks(n=100, capacity=2000, correlation=['uncorrelated', 'strongly_correlated', 'weakly_correlated'], count=3)\n",
    "generate_benchmarks(n=500, capacity=2000, correlation=['uncorrelated', 'strongly_correlated', 'weakly_correlated'], count=3)\n",
    "generate_benchmarks(n=1000, capacity=2000, correlation=['uncorrelated', 'strongly_correlated', 'weakly_correlated'], count=3)\n",
    "generate_benchmarks(n=2000, capacity=2000, correlation=['uncorrelated', 'strongly_correlated', 'weakly_correlated'], count=3)\n",
    "generate_benchmarks(n=5000, capacity=2000, correlation=['uncorrelated', 'strongly_correlated', 'weakly_correlated'], count=2)\n",
    "generate_benchmarks(n=10000, capacity=2000, correlation=['uncorrelated', 'strongly_correlated', 'weakly_correlated'])\n",
    "generate_benchmarks(n=100, capacity=2000, correlation='similar_weights', count=3)\n",
    "generate_benchmarks(n=200, capacity=2000, correlation='similar_weights', count=2)\n",
    "generate_benchmarks(n=1000, capacity=2000, correlation='similar_weights', count=2)\n",
    "generate_benchmarks(n=500, capacity=2000, correlation='similar_weights', count=2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# generate_benchmarks(n=100, capacity=5000, correlation='uncorrelated')\n",
    "# generate_benchmarks(n=100, capacity=5000, correlation='strongly_correlated', count=5)\n",
    "# generate_benchmarks(n=100, capacity=5000, correlation=['uncorrelated', 'strongly_correlated', 'weakly_correlated'])\n",
    "# generate_benchmarks(n=100, capacity=5000, correlation=['uncorrelated', 'strongly_correlated', 'weakly_correlated'], count=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Structures de Données Communes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Item:\n",
    "    \"\"\"Représente un item avec son poids et sa valeur\"\"\"\n",
    "    def __init__(self, item_id, weight, value):\n",
    "        self.id = item_id\n",
    "        self.weight = weight\n",
    "        self.value = value\n",
    "    \n",
    "    def ratio(self):\n",
    "        return self.value / self.weight if self.weight > 0 else 0\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"Item({self.id}, w={self.weight}, v={self.value})\"\n",
    "\n",
    "\n",
    "class Problem:\n",
    "    \"\"\"Représente une instance du problème de knapsack\"\"\"\n",
    "    def __init__(self, items, capacity):\n",
    "        self.items = items\n",
    "        self.capacity = capacity\n",
    "        self.n = len(items)\n",
    "\n",
    "\n",
    "class Solution:\n",
    "    \"\"\"Représente une solution au problème\"\"\"\n",
    "    def __init__(self, selected_items, total_value, total_weight, time_taken):\n",
    "        self.selected_items = selected_items\n",
    "        self.total_value = total_value\n",
    "        self.total_weight = total_weight\n",
    "        self.time = time_taken\n",
    "        self.usage_percent = (total_weight / 1.0) * 100  # Sera mis à jour"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Parsing et Gestion des Benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: La fonction parse_benchmark_file() est maintenant définie dans la section 5.1\n",
    "# avec discover_benchmarks() pour supporter les deux formats de fichiers (.txt et .kp)\n",
    "# Voir la cellule \"Découverte automatique des benchmarks\" plus bas\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Algorithmes Implémentés\n",
    "\n",
    "### 4.1 Brute Force"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def brute_force(problem):\n",
    "    \"\"\"Algorithme exhaustif O(2^n)\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    best_value = 0\n",
    "    best_weight = 0\n",
    "    best_items = []\n",
    "    \n",
    "    for size in range(problem.n + 1):\n",
    "        for combo in combinations(range(problem.n), size):\n",
    "            total_weight = sum(problem.items[i].weight for i in combo)\n",
    "            total_value = sum(problem.items[i].value for i in combo)\n",
    "            \n",
    "            if total_weight <= problem.capacity and total_value > best_value:\n",
    "                best_value = total_value\n",
    "                best_weight = total_weight\n",
    "                best_items = list(combo)\n",
    "    \n",
    "    time_taken = time.time() - start_time\n",
    "    sol = Solution(best_items, best_value, best_weight, time_taken)\n",
    "    sol.usage_percent = (best_weight / problem.capacity) * 100 if problem.capacity > 0 else 0\n",
    "    return sol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Programmation Dynamique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dynamic_programming(problem):\n",
    "    \"\"\"Programmation dynamique O(n x C) avec protection\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    n = problem.n\n",
    "    C = problem.capacity\n",
    "    \n",
    "    total_cells = n * C\n",
    "    if total_cells > 10_000_000:\n",
    "        print(f\"DP Skip: matrice trop grande ({n}×{C:,} = {total_cells:,})\")\n",
    "        return None\n",
    "    \n",
    "    estimated_mb = (total_cells * 8) / (1024 * 1024)\n",
    "    if estimated_mb > 500:\n",
    "        print(f\"DP Skip: mémoire > 500 MB ({estimated_mb:.0f} MB)\")\n",
    "        return None\n",
    "    \n",
    "    dp = [[0 for _ in range(C + 1)] for _ in range(n + 1)]\n",
    "    \n",
    "    for i in range(1, n + 1):\n",
    "        item = problem.items[i - 1]\n",
    "        for w in range(C + 1):\n",
    "            dp[i][w] = dp[i - 1][w]\n",
    "            if item.weight <= w:\n",
    "                dp[i][w] = max(dp[i][w], dp[i - 1][w - item.weight] + item.value)\n",
    "    \n",
    "    # Reconstruction\n",
    "    selected = []\n",
    "    w = C\n",
    "    for i in range(n, 0, -1):\n",
    "        if dp[i][w] != dp[i - 1][w]:\n",
    "            selected.append(i - 1)\n",
    "            w -= problem.items[i - 1].weight\n",
    "    \n",
    "    total_value = dp[n][C]\n",
    "    total_weight = sum(problem.items[i].weight for i in selected)\n",
    "    \n",
    "    time_taken = time.time() - start_time\n",
    "    sol = Solution(selected, total_value, total_weight, time_taken)\n",
    "    sol.usage_percent = (total_weight / problem.capacity) * 100 if problem.capacity > 0 else 0\n",
    "    return sol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.1 Programmation Dynamique Top-Down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dynamic_programming_topdown(problem):\n",
    "    \"\"\"\n",
    "    Programmation dynamique Top-Down avec mémoïsation\n",
    "    \n",
    "    Complexité temporelle: O(n × C)\n",
    "    Complexité spatiale: O(n × C) pour le cache + O(n) pour la pile de récursion\n",
    "    \n",
    "    Avantages par rapport à Bottom-Up:\n",
    "    - Ne calcule que les sous-problèmes nécessaires\n",
    "    - Plus intuitif (suit la définition récursive)\n",
    "    - Peut être plus rapide si tous les sous-problèmes ne sont pas nécessaires\n",
    "    \n",
    "    Returns:\n",
    "        Solution object\n",
    "    \"\"\"\n",
    "    import sys\n",
    "    start_time = time.time()\n",
    "    \n",
    "    n = problem.n\n",
    "    C = problem.capacity\n",
    "    items = problem.items\n",
    "    \n",
    "    # Protection contre les grandes instances\n",
    "    total_cells = n * C\n",
    "    if total_cells > 10_000_000:\n",
    "        print(f\"DP Top-Down Skip: cache trop grand ({n}×{C:,} = {total_cells:,})\")\n",
    "        return None\n",
    "    \n",
    "    # Augmenter la limite de récursion si nécessaire\n",
    "    old_limit = sys.getrecursionlimit()\n",
    "    if n + 100 > old_limit:\n",
    "        sys.setrecursionlimit(max(n + 100, old_limit))\n",
    "    \n",
    "    # Cache pour mémoïsation: memo[i][w] = valeur max avec items 0..i-1 et capacité w\n",
    "    memo = {}\n",
    "    \n",
    "    def knapsack(i, w):\n",
    "        \"\"\"\n",
    "        Retourne la valeur maximale possible avec les items 0..i-1 et capacité w\n",
    "        \"\"\"\n",
    "        # Cas de base\n",
    "        if i == 0 or w == 0:\n",
    "            return 0\n",
    "        \n",
    "        # Vérifier le cache\n",
    "        if (i, w) in memo:\n",
    "            return memo[(i, w)]\n",
    "        \n",
    "        item = items[i - 1]\n",
    "        \n",
    "        # Si l'item est trop lourd, on ne peut pas le prendre\n",
    "        if item.weight > w:\n",
    "            result = knapsack(i - 1, w)\n",
    "        else:\n",
    "            # Max entre prendre et ne pas prendre l'item\n",
    "            not_take = knapsack(i - 1, w)\n",
    "            take = knapsack(i - 1, w - item.weight) + item.value\n",
    "            result = max(not_take, take)\n",
    "        \n",
    "        memo[(i, w)] = result\n",
    "        return result\n",
    "    \n",
    "    # Calculer la valeur optimale\n",
    "    try:\n",
    "        best_value = knapsack(n, C)\n",
    "    except RecursionError:\n",
    "        print(f\"DP Top-Down Skip: récursion trop profonde (n={n})\")\n",
    "        sys.setrecursionlimit(old_limit)\n",
    "        return None\n",
    "    \n",
    "    # Reconstruction de la solution\n",
    "    selected = []\n",
    "    w = C\n",
    "    for i in range(n, 0, -1):\n",
    "        if w == 0:\n",
    "            break\n",
    "        item = items[i - 1]\n",
    "        # Si la valeur change quand on exclut cet item, c'est qu'on l'a pris\n",
    "        val_with = memo.get((i, w), 0)\n",
    "        val_without = memo.get((i - 1, w), 0)\n",
    "        if val_with != val_without:\n",
    "            selected.append(i - 1)\n",
    "            w -= item.weight\n",
    "    \n",
    "    total_weight = sum(items[i].weight for i in selected)\n",
    "    \n",
    "    # Restaurer la limite de récursion\n",
    "    sys.setrecursionlimit(old_limit)\n",
    "    \n",
    "    time_taken = time.time() - start_time\n",
    "    sol = Solution(selected, best_value, total_weight, time_taken)\n",
    "    sol.usage_percent = (total_weight / problem.capacity) * 100 if problem.capacity > 0 else 0\n",
    "    return sol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Branch and Bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def branch_and_bound(problem):\n",
    "    \"\"\"Branch and Bound avec élagage\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    sorted_indices = sorted(range(problem.n), \n",
    "                          key=lambda i: problem.items[i].ratio(), \n",
    "                          reverse=True)\n",
    "    \n",
    "    best_value = 0\n",
    "    best_solution = []\n",
    "    \n",
    "    def bound(level, current_weight, current_value):\n",
    "        if current_weight >= problem.capacity:\n",
    "            return 0\n",
    "        \n",
    "        value_bound = current_value\n",
    "        total_weight = current_weight\n",
    "        \n",
    "        for i in range(level, problem.n):\n",
    "            idx = sorted_indices[i]\n",
    "            item = problem.items[idx]\n",
    "            \n",
    "            if total_weight + item.weight <= problem.capacity:\n",
    "                total_weight += item.weight\n",
    "                value_bound += item.value\n",
    "            else:\n",
    "                remaining = problem.capacity - total_weight\n",
    "                value_bound += item.value * (remaining / item.weight)\n",
    "                break\n",
    "        \n",
    "        return value_bound\n",
    "    \n",
    "    def branch(level, current_weight, current_value, current_items):\n",
    "        nonlocal best_value, best_solution\n",
    "        \n",
    "        if level == problem.n:\n",
    "            if current_value > best_value:\n",
    "                best_value = current_value\n",
    "                best_solution = current_items[:]\n",
    "            return\n",
    "        \n",
    "        idx = sorted_indices[level]\n",
    "        item = problem.items[idx]\n",
    "        \n",
    "        if current_weight + item.weight <= problem.capacity:\n",
    "            new_value = current_value + item.value\n",
    "            if bound(level + 1, current_weight + item.weight, new_value) > best_value:\n",
    "                current_items.append(idx)\n",
    "                branch(level + 1, current_weight + item.weight, new_value, current_items)\n",
    "                current_items.pop()\n",
    "        \n",
    "        if bound(level + 1, current_weight, current_value) > best_value:\n",
    "            branch(level + 1, current_weight, current_value, current_items)\n",
    "    \n",
    "    branch(0, 0, 0, [])\n",
    "    \n",
    "    total_weight = sum(problem.items[i].weight for i in best_solution)\n",
    "    time_taken = time.time() - start_time\n",
    "    \n",
    "    sol = Solution(best_solution, best_value, total_weight, time_taken)\n",
    "    sol.usage_percent = (total_weight / problem.capacity) * 100 if problem.capacity > 0 else 0\n",
    "    return sol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Algorithmes Gloutons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_by_value(problem):\n",
    "    \"\"\"Greedy par valeur décroissante\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    sorted_items = sorted(enumerate(problem.items), key=lambda x: x[1].value, reverse=True)\n",
    "    \n",
    "    selected = []\n",
    "    total_weight = 0\n",
    "    total_value = 0\n",
    "    \n",
    "    for idx, item in sorted_items:\n",
    "        if total_weight + item.weight <= problem.capacity:\n",
    "            selected.append(idx)\n",
    "            total_weight += item.weight\n",
    "            total_value += item.value\n",
    "    \n",
    "    time_taken = time.time() - start_time\n",
    "    sol = Solution(selected, total_value, total_weight, time_taken)\n",
    "    sol.usage_percent = (total_weight / problem.capacity) * 100 if problem.capacity > 0 else 0\n",
    "    return sol\n",
    "\n",
    "\n",
    "def greedy_by_weight(problem):\n",
    "    \"\"\"Greedy par poids croissant\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    sorted_items = sorted(enumerate(problem.items), key=lambda x: x[1].weight)\n",
    "    \n",
    "    selected = []\n",
    "    total_weight = 0\n",
    "    total_value = 0\n",
    "    \n",
    "    for idx, item in sorted_items:\n",
    "        if total_weight + item.weight <= problem.capacity:\n",
    "            selected.append(idx)\n",
    "            total_weight += item.weight\n",
    "            total_value += item.value\n",
    "    \n",
    "    time_taken = time.time() - start_time\n",
    "    sol = Solution(selected, total_value, total_weight, time_taken)\n",
    "    sol.usage_percent = (total_weight / problem.capacity) * 100 if problem.capacity > 0 else 0\n",
    "    return sol\n",
    "\n",
    "\n",
    "def greedy_by_ratio(problem):\n",
    "    \"\"\"Greedy par ratio valeur/poids décroissant\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    sorted_items = sorted(enumerate(problem.items), key=lambda x: x[1].ratio(), reverse=True)\n",
    "    \n",
    "    selected = []\n",
    "    total_weight = 0\n",
    "    total_value = 0\n",
    "    \n",
    "    for idx, item in sorted_items:\n",
    "        if total_weight + item.weight <= problem.capacity:\n",
    "            selected.append(idx)\n",
    "            total_weight += item.weight\n",
    "            total_value += item.value\n",
    "    \n",
    "    time_taken = time.time() - start_time\n",
    "    sol = Solution(selected, total_value, total_weight, time_taken)\n",
    "    sol.usage_percent = (total_weight / problem.capacity) * 100 if problem.capacity > 0 else 0\n",
    "    return sol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4.1 Fractional Knapsack (Sac à dos fractionnel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fractional_knapsack(problem):\n",
    "    \"\"\"\n",
    "    Fractional Knapsack - Algorithme glouton optimal pour le sac à dos fractionnel\n",
    "    \n",
    "    Complexité temporelle: O(n log n) pour le tri\n",
    "    Complexité spatiale: O(n)\n",
    "    \n",
    "    Différence avec 0-1 Knapsack:\n",
    "    - Permet de prendre une FRACTION d'un item\n",
    "    - Solution optimale garantie (contrairement au 0-1)\n",
    "    - Sert de borne supérieure pour le 0-1 Knapsack\n",
    "    \n",
    "    Stratégie: Trier par ratio valeur/poids décroissant et prendre \n",
    "    les items dans cet ordre (fractions si nécessaire)\n",
    "    \n",
    "    Returns:\n",
    "        Solution object avec fraction_taken indiquant les fractions prises\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    n = problem.n\n",
    "    capacity = problem.capacity\n",
    "    items = problem.items\n",
    "    \n",
    "    # Trier les items par ratio valeur/poids décroissant\n",
    "    sorted_items = sorted(enumerate(items), key=lambda x: x[1].ratio(), reverse=True)\n",
    "    \n",
    "    total_value = 0.0\n",
    "    total_weight = 0.0\n",
    "    selected = []  # Liste de tuples (index, fraction_prise)\n",
    "    fractions = {}  # Pour stocker les fractions de chaque item\n",
    "    \n",
    "    remaining_capacity = capacity\n",
    "    \n",
    "    for idx, item in sorted_items:\n",
    "        if remaining_capacity <= 0:\n",
    "            break\n",
    "            \n",
    "        if item.weight <= remaining_capacity:\n",
    "            # Prendre l'item entier\n",
    "            selected.append(idx)\n",
    "            fractions[idx] = 1.0\n",
    "            total_value += item.value\n",
    "            total_weight += item.weight\n",
    "            remaining_capacity -= item.weight\n",
    "        else:\n",
    "            # Prendre une fraction de l'item\n",
    "            fraction = remaining_capacity / item.weight\n",
    "            fractions[idx] = fraction\n",
    "            total_value += item.value * fraction\n",
    "            total_weight += item.weight * fraction\n",
    "            selected.append(idx)\n",
    "            remaining_capacity = 0\n",
    "    \n",
    "    time_taken = time.time() - start_time\n",
    "    \n",
    "    # Créer la solution\n",
    "    # Note: total_value peut être un float, on le garde ainsi pour la précision\n",
    "    sol = Solution(selected, total_value, total_weight, time_taken)\n",
    "    sol.usage_percent = (total_weight / capacity) * 100 if capacity > 0 else 0\n",
    "    sol.fractions = fractions  # Stocker les fractions pour référence\n",
    "    sol.is_fractional = True  # Marquer comme solution fractionnelle\n",
    "    \n",
    "    return sol\n",
    "\n",
    "\n",
    "def fractional_knapsack_bound(problem):\n",
    "    \"\"\"\n",
    "    Calcule uniquement la borne supérieure (valeur max du fractional knapsack)\n",
    "    Utile pour Branch and Bound et comparaisons\n",
    "    \n",
    "    Returns:\n",
    "        float: Valeur maximale possible (borne supérieure pour 0-1)\n",
    "    \"\"\"\n",
    "    sorted_items = sorted(problem.items, key=lambda x: x.ratio(), reverse=True)\n",
    "    \n",
    "    total_value = 0.0\n",
    "    remaining_capacity = problem.capacity\n",
    "    \n",
    "    for item in sorted_items:\n",
    "        if remaining_capacity <= 0:\n",
    "            break\n",
    "        if item.weight <= remaining_capacity:\n",
    "            total_value += item.value\n",
    "            remaining_capacity -= item.weight\n",
    "        else:\n",
    "            total_value += item.value * (remaining_capacity / item.weight)\n",
    "            break\n",
    "    \n",
    "    return total_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Approche Randomisée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomized_approach(problem, iterations=1000, seed=None):\n",
    "    \"\"\"Approche randomisée multi-start\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    if seed is not None:\n",
    "        random.seed(seed)\n",
    "    \n",
    "    best_value = 0\n",
    "    best_weight = 0\n",
    "    best_items = []\n",
    "    \n",
    "    for _ in range(iterations):\n",
    "        indices = list(range(problem.n))\n",
    "        random.shuffle(indices)\n",
    "        \n",
    "        selected = []\n",
    "        total_weight = 0\n",
    "        total_value = 0\n",
    "        \n",
    "        for idx in indices:\n",
    "            item = problem.items[idx]\n",
    "            if total_weight + item.weight <= problem.capacity:\n",
    "                selected.append(idx)\n",
    "                total_weight += item.weight\n",
    "                total_value += item.value\n",
    "        \n",
    "        if total_value > best_value:\n",
    "            best_value = total_value\n",
    "            best_weight = total_weight\n",
    "            best_items = selected\n",
    "    \n",
    "    time_taken = time.time() - start_time\n",
    "    sol = Solution(best_items, best_value, best_weight, time_taken)\n",
    "    sol.usage_percent = (best_weight / problem.capacity) * 100 if problem.capacity > 0 else 0\n",
    "    return sol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 Algorithme Génétique (Genetic Algorithm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genetic_algorithm(problem, population_size=100, generations=50, \n",
    "                     crossover_rate=0.8, mutation_rate=0.02, \n",
    "                     elitism_count=5, seed=None):\n",
    "    \"\"\"\n",
    "\n",
    "    Args:\n",
    "        problem: Instance du problème (Problem object)\n",
    "        population_size: Taille de la population (nombre de chromosomes)\n",
    "        generations: Nombre de générations (itérations)\n",
    "        crossover_rate: Probabilité de croisement (0.0 à 1.0)\n",
    "        mutation_rate: Probabilité de mutation par gène (0.0 à 1.0)\n",
    "        elitism_count: Nombre de meilleures solutions à conserver\n",
    "        seed: Graine aléatoire pour reproductibilité\n",
    "    \n",
    "    Returns:\n",
    "        Solution object\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    if seed is not None:\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "    \n",
    "    n = problem.n\n",
    "    capacity = problem.capacity\n",
    "    items = problem.items\n",
    "    \n",
    "    # === 1. FONCTION DE FITNESS ===\n",
    "    def fitness(chromosome):\n",
    "        \"\"\"Qualité d'un chromosome (solution)\"\"\"\n",
    "        total_weight = sum(chromosome[i] * items[i].weight for i in range(n))\n",
    "        total_value = sum(chromosome[i] * items[i].value for i in range(n))\n",
    "        \n",
    "        # Pénalité si capacité dépassée\n",
    "        if total_weight > capacity:\n",
    "            # Pénalité proportionnelle au dépassement\n",
    "            penalty = (total_weight - capacity) * 10\n",
    "            return max(0, total_value - penalty)\n",
    "        return total_value\n",
    "    \n",
    "    # POPULATION INITIALE\n",
    "    def create_initial_population():\n",
    "        \"\"\"Crée la population initiale avec différentes stratégies\"\"\"\n",
    "        population = []\n",
    "        \n",
    "        # 50% solutions aléatoires\n",
    "        for _ in range(population_size // 2):\n",
    "            chromosome = [random.randint(0, 1) for _ in range(n)]\n",
    "            population.append(chromosome)\n",
    "        \n",
    "        # 25% solutions greedy (ratio)\n",
    "        sorted_indices = sorted(range(n), key=lambda i: items[i].ratio(), reverse=True)\n",
    "        for _ in range(population_size // 4):\n",
    "            chromosome = [0] * n\n",
    "            weight = 0\n",
    "            for idx in sorted_indices:\n",
    "                if weight + items[idx].weight <= capacity and random.random() > 0.3:\n",
    "                    chromosome[idx] = 1\n",
    "                    weight += items[idx].weight\n",
    "            population.append(chromosome)\n",
    "        \n",
    "        # 25% solutions avec densité variable\n",
    "        for _ in range(population_size - len(population)):\n",
    "            chromosome = [0] * n\n",
    "            density = random.uniform(0.2, 0.8)\n",
    "            weight = 0\n",
    "            for i in range(n):\n",
    "                if random.random() < density and weight + items[i].weight <= capacity:\n",
    "                    chromosome[i] = 1\n",
    "                    weight += items[i].weight\n",
    "            population.append(chromosome)\n",
    "        \n",
    "        return population\n",
    "    \n",
    "    # SÉLECTION PAR TOURNOI\n",
    "    def tournament_selection(population, fitnesses, tournament_size=3):\n",
    "        \"\"\"Sélectionne un individu par tournoi\"\"\"\n",
    "        tournament_indices = random.sample(range(len(population)), tournament_size)\n",
    "        tournament_fitnesses = [fitnesses[i] for i in tournament_indices]\n",
    "        winner_idx = tournament_indices[tournament_fitnesses.index(max(tournament_fitnesses))]\n",
    "        return population[winner_idx]\n",
    "    \n",
    "    # CROISEMENT (CROSSOVER)\n",
    "    def crossover(parent1, parent2):\n",
    "        \"\"\"Croisement à deux points\"\"\"\n",
    "        if random.random() > crossover_rate:\n",
    "            return parent1[:], parent2[:]\n",
    "        \n",
    "        # Deux points de coupure\n",
    "        point1 = random.randint(1, n - 2)\n",
    "        point2 = random.randint(point1 + 1, n - 1)\n",
    "        \n",
    "        child1 = parent1[:point1] + parent2[point1:point2] + parent1[point2:]\n",
    "        child2 = parent2[:point1] + parent1[point1:point2] + parent2[point2:]\n",
    "        \n",
    "        return child1, child2\n",
    "    \n",
    "    # MUTATION\n",
    "    def mutate(chromosome):\n",
    "        \"\"\"Mutation par flip de bits\"\"\"\n",
    "        mutated = chromosome[:]\n",
    "        for i in range(n):\n",
    "            if random.random() < mutation_rate:\n",
    "                mutated[i] = 1 - mutated[i]  # Flip 0->1 ou 1->0\n",
    "        return mutated\n",
    "    \n",
    "    # ALGORITHME PRINCIPAL\n",
    "    population = create_initial_population()\n",
    "    best_chromosome = None\n",
    "    best_fitness = -1\n",
    "    \n",
    "    for gen in range(generations):\n",
    "        # Évaluation de la population\n",
    "        fitnesses = [fitness(chromo) for chromo in population]\n",
    "        \n",
    "        # Mise à jour de la meilleure solution\n",
    "        gen_best_idx = fitnesses.index(max(fitnesses))\n",
    "        gen_best_fitness = fitnesses[gen_best_idx]\n",
    "        \n",
    "        if gen_best_fitness > best_fitness:\n",
    "            best_fitness = gen_best_fitness\n",
    "            best_chromosome = population[gen_best_idx][:]\n",
    "        \n",
    "        # Tri par fitness (pour l'élitisme)\n",
    "        sorted_indices = sorted(range(len(population)), key=lambda i: fitnesses[i], reverse=True)\n",
    "        \n",
    "        # Nouvelle génération\n",
    "        new_population = []\n",
    "        \n",
    "        # Élitisme : garder les meilleurs\n",
    "        for i in range(elitism_count):\n",
    "            new_population.append(population[sorted_indices[i]][:])\n",
    "        \n",
    "        # Génération du reste de la population\n",
    "        while len(new_population) < population_size:\n",
    "            # Sélection\n",
    "            parent1 = tournament_selection(population, fitnesses)\n",
    "            parent2 = tournament_selection(population, fitnesses)\n",
    "            \n",
    "            # Croisement\n",
    "            child1, child2 = crossover(parent1, parent2)\n",
    "            \n",
    "            # Mutation\n",
    "            child1 = mutate(child1)\n",
    "            child2 = mutate(child2)\n",
    "            \n",
    "            new_population.append(child1)\n",
    "            if len(new_population) < population_size:\n",
    "                new_population.append(child2)\n",
    "        \n",
    "        population = new_population\n",
    "    \n",
    "    # MEILLEURE SOLUTION\n",
    "    selected_items = [i for i in range(n) if best_chromosome[i] == 1]\n",
    "    total_value = sum(items[i].value for i in selected_items)\n",
    "    total_weight = sum(items[i].weight for i in selected_items)\n",
    "    \n",
    "    time_taken = time.time() - start_time\n",
    "    \n",
    "    sol = Solution(selected_items, total_value, total_weight, time_taken)\n",
    "    sol.usage_percent = (total_weight / capacity * 100) if capacity > 0 else 0\n",
    "    \n",
    "    return sol\n",
    "\n",
    "\n",
    "def genetic_algorithm_adaptive(problem):\n",
    "    \"\"\"\n",
    "    Version adaptative de l'algorithme génétique\n",
    "    Ajuste les paramètres selon la taille du problème\n",
    "    \"\"\"\n",
    "    n = problem.n\n",
    "    \n",
    "    if n <= 50:\n",
    "        return genetic_algorithm(problem, population_size=50, generations=30, mutation_rate=0.03)\n",
    "    elif n <= 100:\n",
    "        return genetic_algorithm(problem, population_size=80, generations=40, mutation_rate=0.02)\n",
    "    elif n <= 500:\n",
    "        return genetic_algorithm(problem, population_size=100, generations=50, mutation_rate=0.02)\n",
    "    elif n <= 1000:\n",
    "        return genetic_algorithm(problem, population_size=120, generations=40, mutation_rate=0.01)\n",
    "    else:\n",
    "        return genetic_algorithm(problem, population_size=150, generations=30, mutation_rate=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6.1 Simulated Annealing (Recuit Simulé)\n",
    "\n",
    "**Principe:** Inspiré du recuit métallurgique, l'algorithme explore l'espace des solutions en acceptant parfois des solutions moins bonnes pour échapper aux optima locaux. La probabilité d'accepter une mauvaise solution diminue avec la \"température\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulated_annealing(problem, initial_temp=1000, cooling_rate=0.995, \n",
    "                        min_temp=1, max_iterations=10000, seed=None):\n",
    "    \"\"\"\n",
    "    Simulated Annealing (Recuit Simulé) pour le Knapsack 0-1\n",
    "    \n",
    "    Complexité temporelle: O(max_iterations × n)\n",
    "    Complexité spatiale: O(n)\n",
    "    \n",
    "    Principe:\n",
    "    - Commence avec une solution initiale (greedy)\n",
    "    - À chaque itération, génère un voisin en flippant un bit\n",
    "    - Accepte toujours les améliorations\n",
    "    - Accepte les dégradations avec probabilité exp(-ΔE/T)\n",
    "    - La température T diminue progressivement (refroidissement)\n",
    "    \n",
    "    Args:\n",
    "        problem: Instance du problème\n",
    "        initial_temp: Température initiale (contrôle l'exploration)\n",
    "        cooling_rate: Taux de refroidissement (0.9 à 0.999)\n",
    "        min_temp: Température minimale (critère d'arrêt)\n",
    "        max_iterations: Nombre maximum d'itérations\n",
    "        seed: Graine aléatoire pour reproductibilité\n",
    "    \n",
    "    Returns:\n",
    "        Solution object\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    if seed is not None:\n",
    "        random.seed(seed)\n",
    "    \n",
    "    n = problem.n\n",
    "    capacity = problem.capacity\n",
    "    items = problem.items\n",
    "    \n",
    "    # === FONCTION D'ÉVALUATION ===\n",
    "    def evaluate(solution):\n",
    "        \"\"\"Calcule valeur et poids d'une solution (liste de 0/1)\"\"\"\n",
    "        total_value = sum(solution[i] * items[i].value for i in range(n))\n",
    "        total_weight = sum(solution[i] * items[i].weight for i in range(n))\n",
    "        return total_value, total_weight\n",
    "    \n",
    "    def fitness(solution):\n",
    "        \"\"\"Fitness avec pénalité si capacité dépassée\"\"\"\n",
    "        value, weight = evaluate(solution)\n",
    "        if weight > capacity:\n",
    "            # Pénalité proportionnelle au dépassement\n",
    "            return value - (weight - capacity) * 10\n",
    "        return value\n",
    "    \n",
    "    # === SOLUTION INITIALE (Greedy par ratio) ===\n",
    "    current_solution = [0] * n\n",
    "    sorted_indices = sorted(range(n), key=lambda i: items[i].ratio(), reverse=True)\n",
    "    current_weight = 0\n",
    "    for idx in sorted_indices:\n",
    "        if current_weight + items[idx].weight <= capacity:\n",
    "            current_solution[idx] = 1\n",
    "            current_weight += items[idx].weight\n",
    "    \n",
    "    current_fitness = fitness(current_solution)\n",
    "    best_solution = current_solution[:]\n",
    "    best_fitness = current_fitness\n",
    "    \n",
    "    # === BOUCLE PRINCIPALE ===\n",
    "    temperature = initial_temp\n",
    "    iteration = 0\n",
    "    \n",
    "    while temperature > min_temp and iteration < max_iterations:\n",
    "        # Générer un voisin en flippant un bit aléatoire\n",
    "        neighbor = current_solution[:]\n",
    "        flip_idx = random.randint(0, n - 1)\n",
    "        neighbor[flip_idx] = 1 - neighbor[flip_idx]\n",
    "        \n",
    "        neighbor_fitness = fitness(neighbor)\n",
    "        \n",
    "        # Calculer la différence d'énergie\n",
    "        delta = neighbor_fitness - current_fitness\n",
    "        \n",
    "        # Décision d'acceptation\n",
    "        if delta > 0:\n",
    "            # Amélioration : toujours accepter\n",
    "            current_solution = neighbor\n",
    "            current_fitness = neighbor_fitness\n",
    "        else:\n",
    "            # Dégradation : accepter avec probabilité exp(delta/T)\n",
    "            acceptance_prob = math.exp(delta / temperature)\n",
    "            if random.random() < acceptance_prob:\n",
    "                current_solution = neighbor\n",
    "                current_fitness = neighbor_fitness\n",
    "        \n",
    "        # Mettre à jour la meilleure solution\n",
    "        if current_fitness > best_fitness:\n",
    "            # Vérifier que la solution est valide\n",
    "            _, weight = evaluate(current_solution)\n",
    "            if weight <= capacity:\n",
    "                best_solution = current_solution[:]\n",
    "                best_fitness = current_fitness\n",
    "        \n",
    "        # Refroidissement\n",
    "        temperature *= cooling_rate\n",
    "        iteration += 1\n",
    "    \n",
    "    # === RÉSULTAT FINAL ===\n",
    "    # S'assurer que la meilleure solution est valide\n",
    "    selected_items = [i for i in range(n) if best_solution[i] == 1]\n",
    "    total_value = sum(items[i].value for i in selected_items)\n",
    "    total_weight = sum(items[i].weight for i in selected_items)\n",
    "    \n",
    "    # Réparer si nécessaire (retirer des items si surpoids)\n",
    "    if total_weight > capacity:\n",
    "        # Trier par ratio croissant et retirer\n",
    "        selected_sorted = sorted(selected_items, key=lambda i: items[i].ratio())\n",
    "        while total_weight > capacity and selected_sorted:\n",
    "            remove_idx = selected_sorted.pop(0)\n",
    "            total_weight -= items[remove_idx].weight\n",
    "            total_value -= items[remove_idx].value\n",
    "            selected_items.remove(remove_idx)\n",
    "    \n",
    "    time_taken = time.time() - start_time\n",
    "    \n",
    "    sol = Solution(selected_items, total_value, total_weight, time_taken)\n",
    "    sol.usage_percent = (total_weight / capacity * 100) if capacity > 0 else 0\n",
    "    sol.iterations = iteration\n",
    "    sol.final_temperature = temperature\n",
    "    \n",
    "    return sol\n",
    "\n",
    "\n",
    "def simulated_annealing_adaptive(problem):\n",
    "    \"\"\"\n",
    "    Version adaptative de Simulated Annealing\n",
    "    Ajuste les paramètres selon la taille du problème\n",
    "    \"\"\"\n",
    "    n = problem.n\n",
    "    \n",
    "    if n <= 50:\n",
    "        return simulated_annealing(problem, initial_temp=500, cooling_rate=0.99, max_iterations=5000)\n",
    "    elif n <= 200:\n",
    "        return simulated_annealing(problem, initial_temp=1000, cooling_rate=0.995, max_iterations=10000)\n",
    "    elif n <= 1000:\n",
    "        return simulated_annealing(problem, initial_temp=2000, cooling_rate=0.997, max_iterations=15000)\n",
    "    else:\n",
    "        return simulated_annealing(problem, initial_temp=5000, cooling_rate=0.999, max_iterations=20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7 FTPAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ftpas(problem, epsilon=0.1):\n",
    "    \"\"\"\n",
    "    Complexité: O(n³/ε)\n",
    "    \n",
    "    Args:\n",
    "        problem: Instance du problème (Problem object)\n",
    "        epsilon: Paramètre d'approximation (0 < ε < 1)\n",
    "                Plus ε est petit, meilleure est l'approximation (mais plus lent)\n",
    "    \n",
    "    Returns:\n",
    "        Solution object\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    n = problem.n\n",
    "    items = problem.items\n",
    "    capacity = problem.capacity\n",
    "    \n",
    "    if epsilon <= 0 or epsilon >= 1:\n",
    "        print(f\"FTPAS: epsilon doit être dans ]0,1[, reçu {epsilon}\")\n",
    "        return None\n",
    "    \n",
    "    v_max = max(item.value for item in items)\n",
    "    \n",
    "    # Facteur de scaling\n",
    "    # K = (ε * v_max) / n\n",
    "    K = (epsilon * v_max) / n\n",
    "    \n",
    "    # Si K trop petit, problèmes numériques\n",
    "    if K < 1e-10:\n",
    "        K = 1e-10\n",
    "    \n",
    "    # Créer les valeurs scalées (arrondi inférieur)\n",
    "    scaled_items = []\n",
    "    for item in items:\n",
    "        scaled_value = math.floor(item.value / K)\n",
    "        scaled_items.append({\n",
    "            'original_idx': item.id,\n",
    "            'weight': item.weight,\n",
    "            'value': item.value,\n",
    "            'scaled_value': scaled_value\n",
    "        })\n",
    "    \n",
    "    V_scaled = sum(si['scaled_value'] for si in scaled_items)\n",
    "    \n",
    "    if V_scaled > 1_000_000:\n",
    "        print(f\"FTPAS Skip: V_scaled trop grand ({V_scaled:,})\")\n",
    "        return None\n",
    "    \n",
    "    # Protection supplémentaire\n",
    "    estimated_mb = (n * V_scaled * 8) / (1024 * 1024)\n",
    "    if estimated_mb > 200:  # Max 200 MB\n",
    "        print(f\"FTPAS Skip: mémoire estimée trop grande ({estimated_mb:.0f} MB)\")\n",
    "        return None\n",
    "    \n",
    "    # DP sur les valeurs scalées\n",
    "    # dp[i][v] = poids minimum pour obtenir exactement la valeur scalée v avec les i premiers items\n",
    "    INF = float('inf')\n",
    "    dp = [[INF for _ in range(int(V_scaled) + 1)] for _ in range(n + 1)]\n",
    "    dp[0][0] = 0\n",
    "    \n",
    "    for i in range(1, n + 1):\n",
    "        si = scaled_items[i - 1]\n",
    "        for v in range(int(V_scaled) + 1):\n",
    "            # Ne pas prendre l'item i\n",
    "            dp[i][v] = dp[i-1][v]\n",
    "            \n",
    "            # Prendre l'item i\n",
    "            if v >= si['scaled_value']:\n",
    "                prev_v = v - si['scaled_value']\n",
    "                if dp[i-1][prev_v] != INF:\n",
    "                    new_weight = dp[i-1][prev_v] + si['weight']\n",
    "                    if new_weight <= capacity:\n",
    "                        dp[i][v] = min(dp[i][v], new_weight)\n",
    "    \n",
    "    best_scaled_value = 0\n",
    "    for v in range(int(V_scaled) + 1):\n",
    "        if dp[n][v] <= capacity:\n",
    "            best_scaled_value = v\n",
    "    \n",
    "    # Reconstruction de la solution\n",
    "    selected = []\n",
    "    v = best_scaled_value\n",
    "    for i in range(n, 0, -1):\n",
    "        if v == 0:\n",
    "            break\n",
    "        si = scaled_items[i - 1]\n",
    "        prev_v = v - si['scaled_value']\n",
    "        if prev_v >= 0 and dp[i-1][prev_v] != INF:\n",
    "            if dp[i][v] == dp[i-1][prev_v] + si['weight']:\n",
    "                selected.append(si['original_idx'])\n",
    "                v = prev_v\n",
    "    \n",
    "    # Calculer la valeur non scalée de la solution\n",
    "    total_value = sum(items[idx].value for idx in selected)\n",
    "    total_weight = sum(items[idx].weight for idx in selected)\n",
    "    \n",
    "    time_taken = time.time() - start_time\n",
    "    \n",
    "    # Créer l'objet Solution\n",
    "    sol = SimpleNamespace(\n",
    "        selected_items=selected,\n",
    "        total_value=total_value,\n",
    "        total_weight=total_weight,\n",
    "        time=time_taken,\n",
    "        usage_percent=(total_weight / capacity * 100) if capacity > 0 else 0,\n",
    "        epsilon=epsilon,\n",
    "        scaling_factor=K\n",
    "    )\n",
    "    \n",
    "    return sol\n",
    "\n",
    "\n",
    "def ftpas_adaptive(problem, time_budget=None):\n",
    "    \"\"\"\n",
    "    Ajuste epsilon selon la taille du problème\n",
    "    \n",
    "    Args:\n",
    "        problem: Instance du problème\n",
    "        time_budget: Budget de temps optionnel (non utilisé pour l'instant)\n",
    "    \n",
    "    Returns:\n",
    "        Solution object\n",
    "    \"\"\"\n",
    "    n = problem.n\n",
    "    \n",
    "    if n <= 50:\n",
    "        epsilon = 0.1\n",
    "    elif n <= 100:\n",
    "        epsilon = 0.2\n",
    "    elif n <= 500:\n",
    "        epsilon = 0.3\n",
    "    else:\n",
    "        epsilon = 0.5  # Très rapide pour grandes instances\n",
    "    \n",
    "    return ftpas(problem, epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Système de Benchmarking\n",
    "\n",
    "### 5.1 Configuration des Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Découvert 85 benchmarks\n",
      "\n",
      "Configuration actuelle:\n",
      "  - large_scale & low_dimension: TOUS les fichiers (y compris n=10000)\n",
      "  - Corrélés (uncorrelated, weakly, strongly): max n=1000\n",
      "\n",
      "Catégories disponibles:\n",
      "  - generated_similar: 9 fichiers (n=[100, 200, 500, 1000])\n",
      "  - generated_strongly: 15 fichiers (n=[100, 500, 1000, 2000, 5000, 10000])\n",
      "  - generated_uncorrelated: 15 fichiers (n=[100, 500, 1000, 2000, 5000, 10000])\n",
      "  - generated_weakly: 15 fichiers (n=[100, 500, 1000, 2000, 5000, 10000])\n",
      "  - large_scale: 21 fichiers (n=[100, 200, 500, 1000, 2000, 5000, 10000])\n",
      "  - low_dimension: 10 fichiers (n=[4, 5, 7, 10, 15, 20, 23])\n"
     ]
    }
   ],
   "source": [
    "MAX_N_CORRELATED = 1000 \n",
    "\n",
    "INCLUDE_CORRELATED = True  # Inclure uncorrelated/weakly/strongly_correlated\n",
    "INCLUDE_GENERATED = True   # Inclure les benchmarks générés (benchmarks/generated/)\n",
    "\n",
    "\n",
    "def discover_benchmarks(base_path='benchmarks', max_n_correlated=None):\n",
    "    \"\"\"\n",
    "    Découvre automatiquement tous les fichiers benchmark disponibles.\n",
    "    Inclut: low_dimension, large_scale, uncorrelated, weakly_correlated, strongly_correlated, generated\n",
    "    \n",
    "    Args:\n",
    "        base_path: Chemin vers le dossier benchmarks\n",
    "        max_n_correlated: Taille max pour les benchmarks corrélés (None = utilise MAX_N_CORRELATED)\n",
    "    \n",
    "    Returns:\n",
    "        dict: Structure contenant les benchmarks organisés par catégorie\n",
    "    \"\"\"\n",
    "    if max_n_correlated is None:\n",
    "        max_n_correlated = MAX_N_CORRELATED\n",
    "        \n",
    "    base = Path(base_path)\n",
    "    \n",
    "    if not base.exists():\n",
    "        print(f\"Dossier '{base_path}' non trouvé\")\n",
    "        return None\n",
    "    \n",
    "    structure = {\n",
    "        'base_path': str(base),\n",
    "        'benchmarks': {}\n",
    "    }\n",
    "    \n",
    "    # Scanner low_dimension et large_scale - SANS limite de taille\n",
    "    simple_categories = ['low_dimension', 'large_scale']\n",
    "    \n",
    "    for category in simple_categories:\n",
    "        category_path = base / category\n",
    "        if not category_path.exists():\n",
    "            continue\n",
    "            \n",
    "        for file_path in category_path.glob('*.txt'):\n",
    "            filename = file_path.name\n",
    "            \n",
    "            if category == 'low_dimension':\n",
    "                parts = filename.replace('.txt', '').split('_')\n",
    "                try:\n",
    "                    n = int(parts[3])\n",
    "                    cap = int(parts[4])\n",
    "                except (IndexError, ValueError):\n",
    "                    continue\n",
    "            else:\n",
    "                parts = filename.replace('.txt', '').split('_')\n",
    "                try:\n",
    "                    n = int(parts[2])\n",
    "                    cap = int(parts[3])\n",
    "                except (IndexError, ValueError):\n",
    "                    continue\n",
    "            \n",
    "            # PAS de filtre de taille pour large_scale et low_dimension\n",
    "            key = f\"{category}_{filename}\"\n",
    "            structure['benchmarks'][key] = {\n",
    "                'path': str(file_path),\n",
    "                'correlation': category,\n",
    "                'size': f\"n={n}\",\n",
    "                'capacity': f\"c={cap}\",\n",
    "                'n': n,\n",
    "                'capacity_value': cap,\n",
    "                'format': 'standard'\n",
    "            }\n",
    "    \n",
    "    # Scanner uncorrelated, weakly_correlated, strongly_correlated - AVEC limite de taille\n",
    "    if INCLUDE_CORRELATED:\n",
    "        correlated_categories = ['uncorrelated', 'weakly_correlated', 'strongly_correlated']\n",
    "        \n",
    "        for category in correlated_categories:\n",
    "            category_path = base / category\n",
    "            if not category_path.exists():\n",
    "                continue\n",
    "            \n",
    "            for file_path in category_path.glob('**/*.kp'):\n",
    "                path_parts = file_path.relative_to(category_path).parts\n",
    "                \n",
    "                n = None\n",
    "                r_value = None\n",
    "                \n",
    "                for part in path_parts:\n",
    "                    if part.startswith('n') and part[1:].isdigit():\n",
    "                        n = int(part[1:])\n",
    "                    elif part.startswith('r') and part[1:].isdigit():\n",
    "                        r_value = int(part[1:])\n",
    "                \n",
    "                if n is None:\n",
    "                    continue\n",
    "                \n",
    "                # Filtrer par taille maximale SEULEMENT pour les corrélés\n",
    "                if n > max_n_correlated:\n",
    "                    continue\n",
    "                \n",
    "                try:\n",
    "                    with open(file_path, 'r') as f:\n",
    "                        lines = f.readlines()\n",
    "                        cap = int(lines[2].strip()) if len(lines) > 2 else 0\n",
    "                except:\n",
    "                    cap = r_value if r_value else 0\n",
    "                \n",
    "                filename = file_path.name\n",
    "                key = f\"{category}_{'/'.join(path_parts)}\"\n",
    "                \n",
    "                structure['benchmarks'][key] = {\n",
    "                    'path': str(file_path),\n",
    "                    'correlation': category,\n",
    "                    'size': f\"n={n}\",\n",
    "                    'capacity': f\"c={cap}\",\n",
    "                    'n': n,\n",
    "                    'capacity_value': cap,\n",
    "                    'format': 'kp'\n",
    "                }\n",
    "    \n",
    "    # Scanner generated/ - fichiers .txt générés par generate_benchmarks()\n",
    "    if INCLUDE_GENERATED:\n",
    "        generated_path = base / 'generated'\n",
    "        if generated_path.exists():\n",
    "            for file_path in generated_path.glob('*.txt'):\n",
    "                filename = file_path.name\n",
    "                # Format: {correlation}_n{n}_c{capacity}.txt ou {correlation}_n{n}_c{capacity}_{index}.txt\n",
    "                parts = filename.replace('.txt', '').split('_')\n",
    "                try:\n",
    "                    # Trouver n et c dans le nom\n",
    "                    correlation = parts[0]\n",
    "                    n = None\n",
    "                    cap = None\n",
    "                    for part in parts:\n",
    "                        if part.startswith('n') and part[1:].isdigit():\n",
    "                            n = int(part[1:])\n",
    "                        elif part.startswith('c') and part[1:].isdigit():\n",
    "                            cap = int(part[1:])\n",
    "                    \n",
    "                    if n is None or cap is None:\n",
    "                        continue\n",
    "                    \n",
    "                    key = f\"generated_{filename}\"\n",
    "                    structure['benchmarks'][key] = {\n",
    "                        'path': str(file_path),\n",
    "                        'correlation': f\"generated_{correlation}\",\n",
    "                        'size': f\"n={n}\",\n",
    "                        'capacity': f\"c={cap}\",\n",
    "                        'n': n,\n",
    "                        'capacity_value': cap,\n",
    "                        'format': 'standard'\n",
    "                    }\n",
    "                except (IndexError, ValueError):\n",
    "                    continue\n",
    "    \n",
    "    print(f\"Découvert {len(structure['benchmarks'])} benchmarks\")\n",
    "    return structure\n",
    "\n",
    "\n",
    "def parse_benchmark_file(filepath):\n",
    "    \"\"\"Parse un fichier benchmark .txt ou .kp\n",
    "    \n",
    "    Format .txt (standard): \n",
    "    - Ligne 1: n capacity (séparés par espace)\n",
    "    - Lignes suivantes: value weight (profit puis poids)\n",
    "    \n",
    "    Format .kp:\n",
    "    - Ligne 1: vide ou commentaire\n",
    "    - Ligne 2: n\n",
    "    - Ligne 3: capacity\n",
    "    - Lignes suivantes: value weight\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(filepath, 'r') as f:\n",
    "            lines = [line.strip() for line in f.readlines()]\n",
    "        \n",
    "        if filepath.endswith('.kp'):\n",
    "            start_idx = 0\n",
    "            while start_idx < len(lines) and not lines[start_idx]:\n",
    "                start_idx += 1\n",
    "            \n",
    "            n = int(lines[start_idx])\n",
    "            capacity = int(lines[start_idx + 1])\n",
    "            \n",
    "            items = []\n",
    "            for i in range(n):\n",
    "                line_idx = start_idx + 2 + i\n",
    "                if line_idx >= len(lines):\n",
    "                    break\n",
    "                parts = lines[line_idx].split()\n",
    "                if len(parts) >= 2:\n",
    "                    value = int(parts[0])\n",
    "                    weight = int(parts[1])\n",
    "                    items.append(Item(i, weight, value))\n",
    "        else:\n",
    "            first_line_parts = lines[0].split()\n",
    "            n = int(first_line_parts[0])\n",
    "            capacity = int(first_line_parts[1])\n",
    "            \n",
    "            items = []\n",
    "            for i in range(n):\n",
    "                parts = lines[1 + i].split()\n",
    "                value = int(parts[0])\n",
    "                weight = int(parts[1])\n",
    "                items.append(Item(i, weight, value))\n",
    "        \n",
    "        return Problem(items, capacity)\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur parsing {filepath}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Initialiser la structure des benchmarks\n",
    "BENCHMARK_STRUCTURE = discover_benchmarks()\n",
    "\n",
    "if BENCHMARK_STRUCTURE:\n",
    "    print(f\"\\nConfiguration actuelle:\")\n",
    "    print(f\"  - large_scale & low_dimension: TOUS les fichiers (y compris n=10000)\")\n",
    "    print(f\"  - Corrélés (uncorrelated, weakly, strongly): max n={MAX_N_CORRELATED}\")\n",
    "    print(f\"\\nCatégories disponibles:\")\n",
    "    categories = {}\n",
    "    sizes_by_cat = {}\n",
    "    for key, info in BENCHMARK_STRUCTURE['benchmarks'].items():\n",
    "        cat = info['correlation']\n",
    "        if cat not in categories:\n",
    "            categories[cat] = 0\n",
    "            sizes_by_cat[cat] = set()\n",
    "        categories[cat] += 1\n",
    "        sizes_by_cat[cat].add(info['n'])\n",
    "    for cat, count in sorted(categories.items()):\n",
    "        sizes = sorted(sizes_by_cat[cat])\n",
    "        print(f\"  - {cat}: {count} fichiers (n={sizes})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition des algorithmes à tester\n",
    "# max_n = taille maximale pour exécuter l'algo (float('inf') = pas de limite)\n",
    "ALL_ALGORITHMS = [\n",
    "    ('Brute Force', brute_force, 25),                    # O(2^n) - max ~25 sinon trop long\n",
    "    ('Dynamic Programming', dynamic_programming, 5000),   # O(n×C) - protection mémoire interne\n",
    "    ('DP Top-Down', dynamic_programming_topdown, 5000),   # O(n×C) - protection mémoire interne\n",
    "    ('Branch and Bound', branch_and_bound, 500),\n",
    "    ('Greedy Ratio', greedy_by_ratio, float('inf')),\n",
    "    ('Greedy Value', greedy_by_value, float('inf')),\n",
    "    ('Greedy Weight', greedy_by_weight, float('inf')),\n",
    "    ('Fractional Knapsack', fractional_knapsack, float('inf')),\n",
    "    ('Randomized', lambda p: randomized_approach(p, iterations=100), float('inf')),\n",
    "    ('Genetic Algorithm', lambda p: genetic_algorithm(p, population_size=100, generations=50), float('inf')),\n",
    "    ('Genetic Adaptive', genetic_algorithm_adaptive, float('inf')),\n",
    "    ('Simulated Annealing', lambda p: simulated_annealing(p, initial_temp=1000, cooling_rate=0.995), float('inf')),\n",
    "    ('SA Adaptive', simulated_annealing_adaptive, float('inf')),\n",
    "    ('FTPAS (ε=0.1)', lambda p: ftpas(p, epsilon=0.1), float('inf')),\n",
    "    ('FTPAS (ε=0.05)', lambda p: ftpas(p, epsilon=0.05), float('inf')),\n",
    "    ('FTPAS Adaptive', ftpas_adaptive, float('inf')),\n",
    "]\n",
    "\n",
    "\n",
    "def should_run_algorithm(algo_name, n, max_n, correlation=None):\n",
    "    \"\"\"Détermine si un algorithme doit être exécuté selon la taille et la corrélation\"\"\"\n",
    "    # Brute Force uniquement sur low_dimension\n",
    "    if algo_name == 'Brute Force' and correlation != 'low_dimension':\n",
    "        return False\n",
    "    return n <= max_n\n",
    "\n",
    "\n",
    "def run_benchmark(benchmark_info, algorithms=None, timeout=300):\n",
    "    \"\"\"\n",
    "    Exécute un benchmark sur un fichier.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Résultats pour chaque algorithme\n",
    "    \"\"\"\n",
    "    if algorithms is None:\n",
    "        algorithms = ALL_ALGORITHMS\n",
    "    \n",
    "    problem = parse_benchmark_file(benchmark_info['path'])\n",
    "    if problem is None:\n",
    "        return None\n",
    "    \n",
    "    results = {\n",
    "        'info': benchmark_info,\n",
    "        'n': problem.n,\n",
    "        'capacity': problem.capacity,\n",
    "        'algorithms': {}\n",
    "    }\n",
    "    \n",
    "    for algo_name, algo_func, max_n in algorithms:\n",
    "        if not should_run_algorithm(algo_name, problem.n, max_n):\n",
    "            results['algorithms'][algo_name] = {'skipped': True, 'reason': f'n={problem.n} > max_n={max_n}'}\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            #GESTION DES ERREURS\n",
    "            start = time.time()\n",
    "            sol = algo_func(problem)\n",
    "            elapsed = time.time() - start\n",
    "            \n",
    "            # Vérifier timeout\n",
    "            if elapsed > timeout:\n",
    "                results['algorithms'][algo_name] = {\n",
    "                    'skipped': True,\n",
    "                    'reason': f'timeout (>{timeout}s)'\n",
    "                }\n",
    "                print(f\"{algo_name}: timeout ({elapsed:.1f}s)\")\n",
    "                continue\n",
    "            \n",
    "            # Vérifier si algo a retourné None (protection interne)\n",
    "            if sol is None:\n",
    "                results['algorithms'][algo_name] = {\n",
    "                    'skipped': True,\n",
    "                    'reason': 'protection_triggered'\n",
    "                }\n",
    "                continue\n",
    "            # FIN GESTION ERREURS\n",
    "            \n",
    "            results['algorithms'][algo_name] = {\n",
    "                'value': sol.total_value,\n",
    "                'weight': sol.total_weight,\n",
    "                'time': sol.time,\n",
    "                'usage': sol.usage_percent,\n",
    "                'items_count': len(sol.selected_items),\n",
    "                'skipped': False\n",
    "            }\n",
    "            \n",
    "        except KeyboardInterrupt:\n",
    "            print(f\"\\nInterruption manuelle sur {algo_name}\")\n",
    "            results['algorithms'][algo_name] = {\n",
    "                'skipped': True,\n",
    "                'reason': 'interrupted'\n",
    "            }\n",
    "            continue  # Continuer avec les autres algorithmes\n",
    "            \n",
    "        except MemoryError:\n",
    "            results['algorithms'][algo_name] = {\n",
    "                'skipped': True,\n",
    "                'reason': 'memory_error'\n",
    "            }\n",
    "            print(f\"{algo_name}: Mémoire insuffisante\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            results['algorithms'][algo_name] = {\n",
    "                'skipped': True,\n",
    "                'reason': str(e)\n",
    "            }\n",
    "            print(f\"{algo_name}: {str(e)}\")\n",
    "\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Exécution Complète des Benchmarks\n",
    "\n",
    "**Attention:** Cette cellule peut prendre plusieurs minutes selon le nombre de benchmarks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_all_benchmarks():\n",
    "    \"\"\"\n",
    "    Exécute tous les benchmarks disponibles.\n",
    "    Sauvegarde UNIQUEMENT à la fin, pas de sauvegarde partielle.\n",
    "    Continue même en cas d'erreur sur un algorithme ou un benchmark.\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame avec tous les résultats\n",
    "    \"\"\"\n",
    "    if BENCHMARK_STRUCTURE is None:\n",
    "        print(\"Aucun benchmark disponible\")\n",
    "        return None\n",
    "    \n",
    "    all_results = []\n",
    "    total = len(BENCHMARK_STRUCTURE['benchmarks'])\n",
    "    \n",
    "    print(f\"Exécution de {total} benchmarks...\")    \n",
    "    for i, (key, bench_info) in enumerate(BENCHMARK_STRUCTURE['benchmarks'].items(), 1):\n",
    "        print(f\"\\n[{i}/{total}] {bench_info['correlation']} | {bench_info['size']} | {bench_info['capacity']}\")\n",
    "        \n",
    "        try:\n",
    "            # Parser le problème\n",
    "            problem = parse_benchmark_file(bench_info['path'])\n",
    "            if problem is None:\n",
    "                print(f\"  ERREUR: Impossible de parser ce benchmark, skip\")\n",
    "                continue\n",
    "            \n",
    "            # Informations sur le problème\n",
    "            print(f\"  n={problem.n}, capacity={problem.capacity}\")\n",
    "            \n",
    "            # Tester chaque algorithme\n",
    "            for algo_name, algo_func, max_n in ALL_ALGORITHMS:\n",
    "                # Vérifier si on doit exécuter cet algo\n",
    "                if not should_run_algorithm(algo_name, problem.n, max_n, bench_info['correlation']):\n",
    "                    if algo_name == 'Brute Force' and bench_info['correlation'] != 'low_dimension':\n",
    "                        print(f\"  SKIP {algo_name}: uniquement sur low_dimension\")\n",
    "                    else:\n",
    "                        print(f\"  SKIP {algo_name}: n={problem.n} > max_n={max_n}\")\n",
    "                    continue\n",
    "                \n",
    "                try:\n",
    "                    # Exécuter l'algorithme\n",
    "                    start_algo = time.time()\n",
    "                    sol = algo_func(problem)\n",
    "                    elapsed = time.time() - start_algo\n",
    "                    \n",
    "                    # Si l'algo prend plus de 5 minutes, on le note mais on garde le résultat\n",
    "                    if elapsed > 300:\n",
    "                        print(f\"  WARNING {algo_name}: temps très long ({elapsed:.1f}s)\")\n",
    "                    \n",
    "                    # Vérifier si l'algo a retourné None (protection interne)\n",
    "                    if sol is None:\n",
    "                        print(f\"  SKIP {algo_name}: protection déclenchée\")\n",
    "                        continue\n",
    "                    \n",
    "                    # Enregistrer le résultat\n",
    "                    row = {\n",
    "                        'correlation': bench_info['correlation'],\n",
    "                        'n': problem.n,\n",
    "                        'capacity_type': bench_info['capacity'],\n",
    "                        'capacity_value': problem.capacity,\n",
    "                        'algorithm': algo_name,\n",
    "                        'value': sol.total_value,\n",
    "                        'time_ms': sol.time * 1000,\n",
    "                        'usage_percent': sol.usage_percent,\n",
    "                        'items_selected': len(sol.selected_items)\n",
    "                    }\n",
    "                    all_results.append(row)\n",
    "                                    \n",
    "                except MemoryError:\n",
    "                    print(f\"  ERROR {algo_name}: Erreur mémoire\")\n",
    "                    continue\n",
    "                \n",
    "                except KeyboardInterrupt:\n",
    "                    print(f\"\\n\\nINTERRUPTION MANUELLE DÉTECTÉE\")\n",
    "                    print(f\"Résultats collectés jusqu'ici: {len(all_results)} lignes\")\n",
    "                    \n",
    "                    if len(all_results) > 0:\n",
    "                        df = pd.DataFrame(all_results)\n",
    "                        df.to_csv('benchmark_results_interrupted.csv', index=False)\n",
    "                        print(\"Sauvegarde d'urgence: 'benchmark_results_interrupted.csv'\")\n",
    "                        return df\n",
    "                    else:\n",
    "                        print(\"Aucun résultat à sauvegarder\")\n",
    "                        return None\n",
    "                \n",
    "                except Exception as e:\n",
    "                    print(f\"  ERROR {algo_name}: {str(e)}\")\n",
    "                    continue\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"  ERROR sur ce benchmark: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    # FIN DE TOUS LES BENCHMARKS\n",
    "    print(f\"\\n\\n{'='*60}\")\n",
    "    print(f\"TERMINÉ ! {len(all_results)} résultats collectés\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    if len(all_results) == 0:\n",
    "        print(\"ATTENTION: Aucun résultat collecté\")\n",
    "        return None\n",
    "    \n",
    "    # Créer le DataFrame final\n",
    "    df = pd.DataFrame(all_results)\n",
    "    \n",
    "    # Sauvegarder\n",
    "    try:\n",
    "        df.to_csv('benchmark_results.csv', index=False)\n",
    "        print(\"Résultats sauvegardés: 'benchmark_results.csv'\")\n",
    "        \n",
    "        # Afficher un résumé\n",
    "        print(f\"\\nRésumé:\")\n",
    "        print(f\"  - {df['algorithm'].nunique()} algorithmes testés\")\n",
    "        print(f\"  - {df['n'].nunique()} tailles différentes\")\n",
    "        print(f\"  - {df['correlation'].nunique()} types de corrélation\")\n",
    "        print(f\"\\nAperçu des résultats:\")\n",
    "        print(df.groupby('algorithm')['value'].agg(['count', 'mean', 'std']))\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"ERROR lors de la sauvegarde: {e}\")\n",
    "        print(\"Le DataFrame est quand même retourné\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Chargement des Résultats\n",
    "\n",
    "Si les benchmarks ont déjà été exécutés, charger les résultats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exécution de 85 benchmarks...\n",
      "\n",
      "[1/85] low_dimension | n=20 | c=879\n",
      "  n=20, capacity=879\n",
      "\n",
      "[2/85] low_dimension | n=10 | c=269\n",
      "  n=10, capacity=269\n",
      "\n",
      "[3/85] low_dimension | n=20 | c=878\n",
      "  n=20, capacity=878\n",
      "\n",
      "[4/85] low_dimension | n=4 | c=20\n",
      "  n=4, capacity=20\n",
      "\n",
      "[5/85] low_dimension | n=4 | c=11\n",
      "  n=4, capacity=11\n",
      "\n",
      "[6/85] low_dimension | n=15 | c=375\n",
      "Erreur parsing benchmarks\\low_dimension\\f5_l-d_kp_15_375.txt: invalid literal for int() with base 10: '0.125126'\n",
      "  ERREUR: Impossible de parser ce benchmark, skip\n",
      "\n",
      "[7/85] low_dimension | n=10 | c=60\n",
      "  n=10, capacity=60\n",
      "\n",
      "[8/85] low_dimension | n=7 | c=50\n",
      "  n=7, capacity=50\n",
      "\n",
      "[9/85] low_dimension | n=23 | c=10000\n",
      "  n=23, capacity=10000\n",
      "\n",
      "[10/85] low_dimension | n=5 | c=80\n",
      "  n=5, capacity=80\n",
      "\n",
      "[11/85] large_scale | n=10000 | c=1000\n",
      "  n=10000, capacity=49877\n",
      "  SKIP Brute Force: uniquement sur low_dimension\n",
      "  SKIP Dynamic Programming: n=10000 > max_n=5000\n",
      "  SKIP DP Top-Down: n=10000 > max_n=5000\n",
      "  SKIP Branch and Bound: n=10000 > max_n=1000\n",
      "FTPAS Skip: V_scaled trop grand (497,906,700)\n",
      "  SKIP FTPAS (ε=0.1): protection déclenchée\n",
      "FTPAS Skip: V_scaled trop grand (995,813,400)\n",
      "  SKIP FTPAS (ε=0.05): protection déclenchée\n",
      "FTPAS Skip: V_scaled trop grand (99,581,340)\n",
      "  SKIP FTPAS Adaptive: protection déclenchée\n",
      "\n",
      "[12/85] large_scale | n=1000 | c=1000\n",
      "  n=1000, capacity=5002\n",
      "  SKIP Brute Force: uniquement sur low_dimension\n",
      "FTPAS Skip: V_scaled trop grand (4,874,289)\n",
      "  SKIP FTPAS (ε=0.1): protection déclenchée\n",
      "FTPAS Skip: V_scaled trop grand (9,749,086)\n",
      "  SKIP FTPAS (ε=0.05): protection déclenchée\n",
      "FTPAS Skip: mémoire estimée trop grande (7434 MB)\n",
      "  SKIP FTPAS Adaptive: protection déclenchée\n",
      "\n",
      "[13/85] large_scale | n=100 | c=1000\n",
      "  n=100, capacity=995\n",
      "  SKIP Brute Force: uniquement sur low_dimension\n",
      "\n",
      "[14/85] large_scale | n=2000 | c=1000\n",
      "  n=2000, capacity=10011\n",
      "  SKIP Brute Force: uniquement sur low_dimension\n",
      "DP Skip: matrice trop grande (2000×10,011 = 20,022,000)\n",
      "  SKIP Dynamic Programming: protection déclenchée\n",
      "DP Top-Down Skip: cache trop grand (2000×10,011 = 20,022,000)\n",
      "  SKIP DP Top-Down: protection déclenchée\n",
      "  SKIP Branch and Bound: n=2000 > max_n=1000\n",
      "FTPAS Skip: V_scaled trop grand (19,554,740)\n",
      "  SKIP FTPAS (ε=0.1): protection déclenchée\n",
      "FTPAS Skip: V_scaled trop grand (39,109,480)\n",
      "  SKIP FTPAS (ε=0.05): protection déclenchée\n",
      "FTPAS Skip: V_scaled trop grand (3,910,948)\n",
      "  SKIP FTPAS Adaptive: protection déclenchée\n",
      "\n",
      "[15/85] large_scale | n=200 | c=1000\n",
      "  n=200, capacity=1008\n",
      "  SKIP Brute Force: uniquement sur low_dimension\n",
      "FTPAS Skip: mémoire estimée trop grande (305 MB)\n",
      "  SKIP FTPAS (ε=0.1): protection déclenchée\n",
      "FTPAS Skip: mémoire estimée trop grande (610 MB)\n",
      "  SKIP FTPAS (ε=0.05): protection déclenchée\n",
      "\n",
      "[16/85] large_scale | n=5000 | c=1000\n",
      "  n=5000, capacity=25016\n",
      "  SKIP Brute Force: uniquement sur low_dimension\n",
      "DP Skip: matrice trop grande (5000×25,016 = 125,080,000)\n",
      "  SKIP Dynamic Programming: protection déclenchée\n",
      "DP Top-Down Skip: cache trop grand (5000×25,016 = 125,080,000)\n",
      "  SKIP DP Top-Down: protection déclenchée\n",
      "  SKIP Branch and Bound: n=5000 > max_n=1000\n",
      "FTPAS Skip: V_scaled trop grand (123,738,600)\n",
      "  SKIP FTPAS (ε=0.1): protection déclenchée\n",
      "FTPAS Skip: V_scaled trop grand (247,477,200)\n",
      "  SKIP FTPAS (ε=0.05): protection déclenchée\n",
      "FTPAS Skip: V_scaled trop grand (24,747,720)\n",
      "  SKIP FTPAS Adaptive: protection déclenchée\n",
      "\n",
      "[17/85] large_scale | n=500 | c=1000\n",
      "  n=500, capacity=2543\n",
      "  SKIP Brute Force: uniquement sur low_dimension\n",
      "FTPAS Skip: V_scaled trop grand (1,237,583)\n",
      "  SKIP FTPAS (ε=0.1): protection déclenchée\n",
      "FTPAS Skip: V_scaled trop grand (2,475,405)\n",
      "  SKIP FTPAS (ε=0.05): protection déclenchée\n",
      "FTPAS Skip: mémoire estimée trop grande (1573 MB)\n",
      "  SKIP FTPAS Adaptive: protection déclenchée\n",
      "\n",
      "[18/85] large_scale | n=10000 | c=1000\n",
      "  n=10000, capacity=49877\n",
      "  SKIP Brute Force: uniquement sur low_dimension\n",
      "  SKIP Dynamic Programming: n=10000 > max_n=5000\n",
      "  SKIP DP Top-Down: n=10000 > max_n=5000\n",
      "  SKIP Branch and Bound: n=10000 > max_n=1000\n",
      "FTPAS Skip: V_scaled trop grand (459,707,978)\n",
      "  SKIP FTPAS (ε=0.1): protection déclenchée\n",
      "FTPAS Skip: V_scaled trop grand (919,420,627)\n",
      "  SKIP FTPAS (ε=0.05): protection déclenchée\n",
      "FTPAS Skip: V_scaled trop grand (91,938,045)\n",
      "  SKIP FTPAS Adaptive: protection déclenchée\n",
      "\n",
      "[19/85] large_scale | n=1000 | c=1000\n",
      "  n=1000, capacity=5002\n",
      "  SKIP Brute Force: uniquement sur low_dimension\n",
      "FTPAS Skip: V_scaled trop grand (4,655,315)\n",
      "  SKIP FTPAS (ε=0.1): protection déclenchée\n",
      "FTPAS Skip: V_scaled trop grand (9,311,137)\n",
      "  SKIP FTPAS (ε=0.05): protection déclenchée\n",
      "FTPAS Skip: mémoire estimée trop grande (7100 MB)\n",
      "  SKIP FTPAS Adaptive: protection déclenchée\n",
      "\n",
      "[20/85] large_scale | n=100 | c=1000\n",
      "  n=100, capacity=995\n",
      "  SKIP Brute Force: uniquement sur low_dimension\n",
      "\n",
      "[21/85] large_scale | n=2000 | c=1000\n",
      "  n=2000, capacity=10011\n",
      "  SKIP Brute Force: uniquement sur low_dimension\n",
      "DP Skip: matrice trop grande (2000×10,011 = 20,022,000)\n",
      "  SKIP Dynamic Programming: protection déclenchée\n",
      "DP Top-Down Skip: cache trop grand (2000×10,011 = 20,022,000)\n",
      "  SKIP DP Top-Down: protection déclenchée\n",
      "  SKIP Branch and Bound: n=2000 > max_n=1000\n",
      "FTPAS Skip: V_scaled trop grand (18,608,683)\n",
      "  SKIP FTPAS (ε=0.1): protection déclenchée\n",
      "FTPAS Skip: V_scaled trop grand (37,218,378)\n",
      "  SKIP FTPAS (ε=0.05): protection déclenchée\n",
      "FTPAS Skip: V_scaled trop grand (3,720,914)\n",
      "  SKIP FTPAS Adaptive: protection déclenchée\n",
      "\n",
      "[22/85] large_scale | n=200 | c=1000\n",
      "  n=200, capacity=1008\n",
      "  SKIP Brute Force: uniquement sur low_dimension\n",
      "FTPAS Skip: mémoire estimée trop grande (293 MB)\n",
      "  SKIP FTPAS (ε=0.1): protection déclenchée\n",
      "FTPAS Skip: mémoire estimée trop grande (587 MB)\n",
      "  SKIP FTPAS (ε=0.05): protection déclenchée\n",
      "\n",
      "[23/85] large_scale | n=5000 | c=1000\n",
      "  n=5000, capacity=25016\n",
      "  SKIP Brute Force: uniquement sur low_dimension\n",
      "DP Skip: matrice trop grande (5000×25,016 = 125,080,000)\n",
      "  SKIP Dynamic Programming: protection déclenchée\n",
      "DP Top-Down Skip: cache trop grand (5000×25,016 = 125,080,000)\n",
      "  SKIP DP Top-Down: protection déclenchée\n",
      "  SKIP Branch and Bound: n=5000 > max_n=1000\n",
      "FTPAS Skip: V_scaled trop grand (116,152,367)\n",
      "  SKIP FTPAS (ε=0.1): protection déclenchée\n",
      "FTPAS Skip: V_scaled trop grand (232,307,365)\n",
      "  SKIP FTPAS (ε=0.05): protection déclenchée\n",
      "FTPAS Skip: V_scaled trop grand (23,228,487)\n",
      "  SKIP FTPAS Adaptive: protection déclenchée\n",
      "\n",
      "[24/85] large_scale | n=500 | c=1000\n",
      "  n=500, capacity=2543\n",
      "  SKIP Brute Force: uniquement sur low_dimension\n",
      "FTPAS Skip: V_scaled trop grand (1,202,340)\n",
      "  SKIP FTPAS (ε=0.1): protection déclenchée\n",
      "FTPAS Skip: V_scaled trop grand (2,404,944)\n",
      "  SKIP FTPAS (ε=0.05): protection déclenchée\n",
      "FTPAS Skip: mémoire estimée trop grande (1528 MB)\n",
      "  SKIP FTPAS Adaptive: protection déclenchée\n",
      "\n",
      "[25/85] large_scale | n=10000 | c=1000\n",
      "  n=10000, capacity=49519\n",
      "  SKIP Brute Force: uniquement sur low_dimension\n",
      "  SKIP Dynamic Programming: n=10000 > max_n=5000\n",
      "  SKIP DP Top-Down: n=10000 > max_n=5000\n",
      "  SKIP Branch and Bound: n=10000 > max_n=1000\n",
      "FTPAS Skip: V_scaled trop grand (545,578,976)\n",
      "  SKIP FTPAS (ε=0.1): protection déclenchée\n",
      "FTPAS Skip: V_scaled trop grand (1,091,162,497)\n",
      "  SKIP FTPAS (ε=0.05): protection déclenchée\n",
      "FTPAS Skip: V_scaled trop grand (109,112,160)\n",
      "  SKIP FTPAS Adaptive: protection déclenchée\n",
      "\n",
      "[26/85] large_scale | n=1000 | c=1000\n",
      "  n=1000, capacity=4990\n",
      "  SKIP Brute Force: uniquement sur low_dimension\n",
      "FTPAS Skip: V_scaled trop grand (5,500,440)\n",
      "  SKIP FTPAS (ε=0.1): protection déclenchée\n",
      "FTPAS Skip: V_scaled trop grand (11,001,362)\n",
      "  SKIP FTPAS (ε=0.05): protection déclenchée\n",
      "FTPAS Skip: V_scaled trop grand (1,099,678)\n",
      "  SKIP FTPAS Adaptive: protection déclenchée\n",
      "\n",
      "[27/85] large_scale | n=100 | c=1000\n",
      "  n=100, capacity=997\n",
      "  SKIP Brute Force: uniquement sur low_dimension\n",
      "\n",
      "[28/85] large_scale | n=2000 | c=1000\n",
      "  n=2000, capacity=9819\n",
      "  SKIP Brute Force: uniquement sur low_dimension\n",
      "DP Skip: matrice trop grande (2000×9,819 = 19,638,000)\n",
      "  SKIP Dynamic Programming: protection déclenchée\n",
      "DP Top-Down Skip: cache trop grand (2000×9,819 = 19,638,000)\n",
      "  SKIP DP Top-Down: protection déclenchée\n",
      "  SKIP Branch and Bound: n=2000 > max_n=1000\n",
      "FTPAS Skip: V_scaled trop grand (21,668,078)\n",
      "  SKIP FTPAS (ε=0.1): protection déclenchée\n",
      "FTPAS Skip: V_scaled trop grand (43,337,061)\n",
      "  SKIP FTPAS (ε=0.05): protection déclenchée\n",
      "FTPAS Skip: V_scaled trop grand (4,332,764)\n",
      "  SKIP FTPAS Adaptive: protection déclenchée\n",
      "\n",
      "[29/85] large_scale | n=200 | c=1000\n",
      "  n=200, capacity=997\n",
      "  SKIP Brute Force: uniquement sur low_dimension\n",
      "FTPAS Skip: mémoire estimée trop grande (335 MB)\n",
      "  SKIP FTPAS (ε=0.1): protection déclenchée\n",
      "FTPAS Skip: mémoire estimée trop grande (670 MB)\n",
      "  SKIP FTPAS (ε=0.05): protection déclenchée\n",
      "\n",
      "[30/85] large_scale | n=5000 | c=1000\n",
      "  n=5000, capacity=24805\n",
      "  SKIP Brute Force: uniquement sur low_dimension\n",
      "DP Skip: matrice trop grande (5000×24,805 = 124,025,000)\n",
      "  SKIP Dynamic Programming: protection déclenchée\n",
      "DP Top-Down Skip: cache trop grand (5000×24,805 = 124,025,000)\n",
      "  SKIP DP Top-Down: protection déclenchée\n",
      "  SKIP Branch and Bound: n=5000 > max_n=1000\n",
      "FTPAS Skip: V_scaled trop grand (136,606,720)\n",
      "  SKIP FTPAS (ε=0.1): protection déclenchée\n",
      "FTPAS Skip: V_scaled trop grand (273,215,722)\n",
      "  SKIP FTPAS (ε=0.05): protection déclenchée\n",
      "FTPAS Skip: V_scaled trop grand (27,319,506)\n",
      "  SKIP FTPAS Adaptive: protection déclenchée\n",
      "\n",
      "[31/85] large_scale | n=500 | c=1000\n",
      "  n=500, capacity=2517\n",
      "  SKIP Brute Force: uniquement sur low_dimension\n",
      "FTPAS Skip: V_scaled trop grand (1,385,469)\n",
      "  SKIP FTPAS (ε=0.1): protection déclenchée\n",
      "FTPAS Skip: V_scaled trop grand (2,771,201)\n",
      "  SKIP FTPAS (ε=0.05): protection déclenchée\n",
      "FTPAS Skip: mémoire estimée trop grande (1761 MB)\n",
      "  SKIP FTPAS Adaptive: protection déclenchée\n",
      "\n",
      "[32/85] generated_similar | n=1000 | c=2000\n",
      "  n=1000, capacity=2000\n",
      "  SKIP Brute Force: uniquement sur low_dimension\n",
      "FTPAS Skip: V_scaled trop grand (5,103,530)\n",
      "  SKIP FTPAS (ε=0.1): protection déclenchée\n",
      "FTPAS Skip: V_scaled trop grand (10,207,060)\n",
      "  SKIP FTPAS (ε=0.05): protection déclenchée\n",
      "FTPAS Skip: V_scaled trop grand (1,020,706)\n",
      "  SKIP FTPAS Adaptive: protection déclenchée\n",
      "\n",
      "[33/85] generated_similar | n=1000 | c=2000\n",
      "  n=1000, capacity=2000\n",
      "  SKIP Brute Force: uniquement sur low_dimension\n",
      "FTPAS Skip: V_scaled trop grand (5,068,390)\n",
      "  SKIP FTPAS (ε=0.1): protection déclenchée\n",
      "FTPAS Skip: V_scaled trop grand (10,136,780)\n",
      "  SKIP FTPAS (ε=0.05): protection déclenchée\n",
      "FTPAS Skip: V_scaled trop grand (1,013,678)\n",
      "  SKIP FTPAS Adaptive: protection déclenchée\n",
      "\n",
      "[34/85] generated_similar | n=100 | c=2000\n",
      "  n=100, capacity=2000\n",
      "  SKIP Brute Force: uniquement sur low_dimension\n",
      "\n",
      "[35/85] generated_similar | n=100 | c=2000\n",
      "  n=100, capacity=2000\n",
      "  SKIP Brute Force: uniquement sur low_dimension\n",
      "\n",
      "[36/85] generated_similar | n=100 | c=2000\n",
      "  n=100, capacity=2000\n",
      "  SKIP Brute Force: uniquement sur low_dimension\n",
      "\n",
      "[37/85] generated_similar | n=200 | c=2000\n",
      "  n=200, capacity=2000\n",
      "  SKIP Brute Force: uniquement sur low_dimension\n",
      "FTPAS Skip: mémoire estimée trop grande (311 MB)\n",
      "  SKIP FTPAS (ε=0.1): protection déclenchée\n",
      "FTPAS Skip: mémoire estimée trop grande (621 MB)\n",
      "  SKIP FTPAS (ε=0.05): protection déclenchée\n",
      "\n",
      "[38/85] generated_similar | n=200 | c=2000\n",
      "  n=200, capacity=2000\n",
      "  SKIP Brute Force: uniquement sur low_dimension\n",
      "FTPAS Skip: mémoire estimée trop grande (306 MB)\n",
      "  SKIP FTPAS (ε=0.1): protection déclenchée\n",
      "FTPAS Skip: mémoire estimée trop grande (612 MB)\n",
      "  SKIP FTPAS (ε=0.05): protection déclenchée\n",
      "\n",
      "[39/85] generated_similar | n=500 | c=2000\n",
      "  n=500, capacity=2000\n",
      "  SKIP Brute Force: uniquement sur low_dimension\n",
      "FTPAS Skip: V_scaled trop grand (1,335,807)\n",
      "  SKIP FTPAS (ε=0.1): protection déclenchée\n",
      "FTPAS Skip: V_scaled trop grand (2,671,850)\n",
      "  SKIP FTPAS (ε=0.05): protection déclenchée\n",
      "FTPAS Skip: mémoire estimée trop grande (1698 MB)\n",
      "  SKIP FTPAS Adaptive: protection déclenchée\n",
      "\n",
      "[40/85] generated_similar | n=500 | c=2000\n",
      "  n=500, capacity=2000\n",
      "  SKIP Brute Force: uniquement sur low_dimension\n",
      "FTPAS Skip: V_scaled trop grand (1,254,785)\n",
      "  SKIP FTPAS (ε=0.1): protection déclenchée\n",
      "FTPAS Skip: V_scaled trop grand (2,509,831)\n",
      "  SKIP FTPAS (ε=0.05): protection déclenchée\n",
      "FTPAS Skip: mémoire estimée trop grande (1595 MB)\n",
      "  SKIP FTPAS Adaptive: protection déclenchée\n",
      "\n",
      "[41/85] generated_strongly | n=10000 | c=2000\n",
      "  n=10000, capacity=2000\n",
      "  SKIP Brute Force: uniquement sur low_dimension\n",
      "  SKIP Dynamic Programming: n=10000 > max_n=5000\n",
      "  SKIP DP Top-Down: n=10000 > max_n=5000\n",
      "  SKIP Branch and Bound: n=10000 > max_n=1000\n",
      "FTPAS Skip: V_scaled trop grand (546,638,613)\n",
      "  SKIP FTPAS (ε=0.1): protection déclenchée\n",
      "FTPAS Skip: V_scaled trop grand (1,093,281,805)\n",
      "  SKIP FTPAS (ε=0.05): protection déclenchée\n",
      "FTPAS Skip: V_scaled trop grand (109,324,045)\n",
      "  SKIP FTPAS Adaptive: protection déclenchée\n",
      "\n",
      "[42/85] generated_strongly | n=1000 | c=2000\n",
      "  n=1000, capacity=2000\n",
      "  SKIP Brute Force: uniquement sur low_dimension\n"
     ]
    }
   ],
   "source": [
    "# Charger les résultats\n",
    "try:\n",
    "    results_df = run_all_benchmarks()\n",
    "    print(f\"\\nAperçu:\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Fichier 'benchmark_results.csv' non trouvé\")\n",
    "    print(\"Exécutez d'abord: results_df = run_all_benchmarks()\")\n",
    "    results_df = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Visualisations simples\n",
    "\n",
    "Ci-dessous :\n",
    "- Graphe 1 : Valeur totale (axe Y) par algorithme (axe X), couleur = type de dataset (correlation).\n",
    "- Graphe 2 : Temps d'exécution (ms) par algorithme, couleur = taille `n`.\n",
    "- Graphe 3 : Scatter Temps (X) vs Valeur (Y), 1 point par algorithme, étiquette avec le nom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graphique 1 : Qualité (valeur totale) par algorithme, couleur = correlation\n",
    "if results_df is not None:\n",
    "    df = results_df.copy()\n",
    "    # moyenne valeur par algorithme x correlation\n",
    "    agg = df.groupby(['algorithm', 'correlation'])['value'].mean().reset_index()\n",
    "    \n",
    "    plt.figure(figsize=(12,6))\n",
    "    # Palette pour toutes les catégories\n",
    "    palette_map = {\n",
    "        'large_scale': '#1f77b4',\n",
    "        'low_dimension': '#ff7f0e',\n",
    "        'uncorrelated': '#2ca02c',\n",
    "        'weakly_correlated': '#d62728',\n",
    "        'strongly_correlated': '#9467bd',\n",
    "        'generated_uncorrelated': '#8c564b',\n",
    "        'generated_weakly_correlated': '#e377c2',\n",
    "        'generated_strongly_correlated': '#7f7f7f',\n",
    "        'generated_similar_weights': '#bcbd22',\n",
    "        'generated_inverse_strongly_correlated': '#17becf',\n",
    "        # Versions courtes au cas où\n",
    "        'generated_weakly': '#e377c2',\n",
    "        'generated_strongly': '#7f7f7f',\n",
    "        'generated_similar': '#bcbd22',\n",
    "    }\n",
    "    # Filtrer la palette pour ne garder que les catégories présentes\n",
    "    categories_present = agg['correlation'].unique()\n",
    "    palette_filtered = {k: v for k, v in palette_map.items() if k in categories_present}\n",
    "    # Ajouter des couleurs auto pour catégories inconnues\n",
    "    for cat in categories_present:\n",
    "        if cat not in palette_filtered:\n",
    "            palette_filtered[cat] = None\n",
    "    \n",
    "    sns.barplot(data=agg, x='algorithm', y='value', hue='correlation', palette=palette_filtered)\n",
    "    plt.xlabel('Algorithme')\n",
    "    plt.ylabel('Valeur totale moyenne')\n",
    "    plt.title('Valeur obtenue par algorithme (couleur = type de dataset)')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"results_df non chargé. Exécutez d'abord la cellule qui charge 'benchmark_results.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graphique 2 : Temps d'exécution par taille n\n",
    "if results_df is not None:\n",
    "    df = results_df.copy()\n",
    "    \n",
    "    # Moyenne du temps par algorithme et taille n\n",
    "    agg_time = df.groupby(['algorithm', 'n'])['time_ms'].mean().reset_index()\n",
    "    \n",
    "    # Trier par n\n",
    "    agg_time = agg_time.sort_values('n')\n",
    "    \n",
    "    plt.figure(figsize=(12,6))\n",
    "    \n",
    "    # Utiliser lineplot avec algorithme comme hue (couleur)\n",
    "    sns.lineplot(data=agg_time, x='n', y='time_ms', hue='algorithm', marker='o', palette='husl', linewidth=2, markersize=8)\n",
    "    \n",
    "    plt.xlabel('Taille du problème (n)')\n",
    "    plt.ylabel('Temps moyen (ms)')\n",
    "    plt.title('Temps d\\'exécution par taille de problème')\n",
    "    plt.yscale('log')\n",
    "    \n",
    "    # Axe X avec ticks tous les 1000\n",
    "    max_n = int(agg_time['n'].max())\n",
    "    plt.xticks(range(0, max_n + 1000, 1000))\n",
    "    plt.xlim(0, max_n + 500)\n",
    "    \n",
    "    plt.legend(title='Algorithme', bbox_to_anchor=(1.05,1), loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"results_df non chargé. Exécutez d'abord la cellule qui charge 'benchmark_results.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graphique 2bis : Temps d'exécution par groupe d'algorithmes\n",
    "if results_df is not None:\n",
    "    df = results_df.copy()\n",
    "    agg_time = df.groupby(['algorithm', 'n'])['time_ms'].mean().reset_index()\n",
    "    \n",
    "    # Groupe 1: Algorithmes limités (n <= 1000)\n",
    "    algos_limited = ['Brute Force', 'Dynamic Programming', 'DP Top-Down', 'Branch and Bound']\n",
    "    \n",
    "    # Groupe 2: Algorithmes scalables (n jusqu'à 10000)\n",
    "    algos_scalable = ['Greedy Ratio', 'Greedy Value', 'Greedy Weight', 'Fractional Knapsack',\n",
    "                      'Randomized', 'Genetic Algorithm', 'Genetic Adaptive', \n",
    "                      'Simulated Annealing', 'SA Adaptive',\n",
    "                      'FTPAS (ε=0.1)', 'FTPAS (ε=0.05)', 'FTPAS Adaptive']\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # --- Graphique 2a: Algorithmes Exacts (n <= 1000) ---\n",
    "    ax1 = axes[0]\n",
    "    data_limited = agg_time[agg_time['algorithm'].isin(algos_limited)]\n",
    "    for algo in algos_limited:\n",
    "        algo_data = data_limited[data_limited['algorithm'] == algo].sort_values('n')\n",
    "        if len(algo_data) > 0:\n",
    "            color = ALGO_COLORS.get(algo, '#333333')\n",
    "            ax1.plot(algo_data['n'], algo_data['time_ms'], 'o-', label=algo, color=color, linewidth=2, markersize=6)\n",
    "    ax1.set_xlabel('Taille du problème (n)', fontsize=11)\n",
    "    ax1.set_ylabel('Temps moyen (ms)', fontsize=11)\n",
    "    ax1.set_title('Algorithmes Exacts (n ≤ 1000)', fontsize=12, fontweight='bold')\n",
    "    ax1.set_yscale('log')\n",
    "    ax1.set_xticks(range(0, 1100, 100))\n",
    "    ax1.set_xlim(0, 1050)\n",
    "    ax1.legend(fontsize=7, loc='lower right')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # --- Graphique 2b: Algorithmes Scalables (n <= 10000) ---\n",
    "    ax2 = axes[1]\n",
    "    data_scalable = agg_time[agg_time['algorithm'].isin(algos_scalable)]\n",
    "    for algo in algos_scalable:\n",
    "        algo_data = data_scalable[data_scalable['algorithm'] == algo].sort_values('n')\n",
    "        if len(algo_data) > 0:\n",
    "            color = ALGO_COLORS.get(algo, '#333333')\n",
    "            ax2.plot(algo_data['n'], algo_data['time_ms'], 'o-', label=algo, color=color, linewidth=2, markersize=6)\n",
    "    ax2.set_xlabel('Taille du problème (n)', fontsize=11)\n",
    "    ax2.set_ylabel('Temps moyen (ms)', fontsize=11)\n",
    "    ax2.set_title('Algorithmes Scalables (n ≤ 10000)', fontsize=12, fontweight='bold')\n",
    "    ax2.set_yscale('log')\n",
    "    ax2.set_xticks(range(0, 11000, 1000))\n",
    "    ax2.set_xlim(0, 10500)\n",
    "    ax2.legend(fontsize=7, loc='lower right')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"results_df non chargé.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graphique 3 : Scatter Temps (X) vs Valeur (Y), 1 point = 1 algorithme (moyennes), annoté\n",
    "if results_df is not None:\n",
    "    df = results_df.copy()\n",
    "    # prendre la moyenne temps et valeur par algorithme\n",
    "    summary = df.groupby('algorithm').agg({'time_ms':'mean','value':'mean'}).reset_index()\n",
    "    \n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.scatter(summary['time_ms'], summary['value'], s=120, alpha=0.8)\n",
    "    for i, row in summary.iterrows():\n",
    "        plt.text(row['time_ms'], row['value'], row['algorithm'], fontsize=9,\n",
    "                 verticalalignment='bottom', horizontalalignment='right')\n",
    "    plt.xscale('log')\n",
    "    plt.xlabel('Temps moyen (ms)')\n",
    "    plt.ylabel('Valeur moyenne')\n",
    "    plt.title('Compromis Temps vs Qualité (un point = un algorithme)')\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"results_df non chargé. Exécutez d'abord la cellule qui charge 'benchmark_results.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if results_df is not None:\n",
    "    df = results_df.copy()\n",
    "    \n",
    "    algorithms = df['algorithm'].unique()\n",
    "    \n",
    "    # couleurs pour les deux catégories principales\n",
    "    colors_cat = {'large_scale':'#1f77b4','low_dimension':'#ff7f0e'}\n",
    "    \n",
    "    for algo in algorithms:\n",
    "        algo_data = df[df['algorithm'] == algo].copy()\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "        fig.suptitle(f'Performance : {algo}', fontsize=14, fontweight='bold')\n",
    "        \n",
    "        # Valeur par catégorie et taille n\n",
    "        if 'category' in algo_data.columns:\n",
    "            agg_value = algo_data.groupby(['category', 'n'])['value'].mean().reset_index()\n",
    "            groups = agg_value['category'].unique()\n",
    "        else:\n",
    "            # fallback vers 'correlation' si présent\n",
    "            agg_value = algo_data.groupby(['correlation', 'n'])['value'].mean().reset_index()\n",
    "            agg_value = agg_value.rename(columns={'correlation':'category'})\n",
    "            groups = agg_value['category'].unique()\n",
    "\n",
    "        for cat in groups:\n",
    "            cat_data = agg_value[agg_value['category'] == cat]\n",
    "            axes[0].plot(cat_data['n'], cat_data['value'], marker='o', label=cat, linewidth=2,\n",
    "                         color=colors_cat.get(cat, None))\n",
    "        \n",
    "        axes[0].set_xlabel('Taille n', fontsize=11)\n",
    "        axes[0].set_ylabel('Valeur moyenne', fontsize=11)\n",
    "        axes[0].set_title('Valeur par taille et catégorie')\n",
    "        axes[0].legend()\n",
    "        axes[0].grid(alpha=0.3)\n",
    "        axes[0].set_xscale('log')\n",
    "        \n",
    "        # Temps d'exécution par taille n\n",
    "        if 'category' in algo_data.columns:\n",
    "            agg_time = algo_data.groupby(['category', 'n'])['time_ms'].mean().reset_index()\n",
    "        else:\n",
    "            agg_time = algo_data.groupby(['correlation', 'n'])['time_ms'].mean().reset_index()\n",
    "            agg_time = agg_time.rename(columns={'correlation':'category'})\n",
    "\n",
    "        for cat in agg_time['category'].unique():\n",
    "            cat_data = agg_time[agg_time['category'] == cat]\n",
    "            axes[1].plot(cat_data['n'], cat_data['time_ms'], marker='s', label=cat, linewidth=2,\n",
    "                         color=colors_cat.get(cat, None))\n",
    "        \n",
    "        axes[1].set_xlabel('Taille n', fontsize=11)\n",
    "        axes[1].set_ylabel('Temps moyen (ms)', fontsize=11)\n",
    "        axes[1].set_title('Temps d\\'exécution par taille')\n",
    "        axes[1].legend()\n",
    "        axes[1].grid(alpha=0.3)\n",
    "        axes[1].set_xscale('log')\n",
    "        axes[1].set_yscale('log')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        print(f\"\\n{'='*60}\\n\")\n",
    "else:\n",
    "    print(\"results_df non chargé. Exécutez d'abord la cellule qui charge 'benchmark_results.csv'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Analyse par Type de Corrélation\n",
    "\n",
    "Ces graphiques comparent les performances des algorithmes selon le type de données :\n",
    "- **uncorrelated**: Poids et valeurs indépendants\n",
    "- **weakly_correlated**: Légère corrélation entre poids et valeurs  \n",
    "- **strongly_correlated**: Forte corrélation (valeur ≈ poids + constante)\n",
    "\n",
    "Les instances fortement corrélées sont généralement plus difficiles pour les heuristiques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graphique 4 : Comparaison par type de corrélation\n",
    "if results_df is not None:\n",
    "    df = results_df.copy()\n",
    "    \n",
    "    # Vérifier quels types de corrélation sont présents\n",
    "    correlation_types = df['correlation'].unique()\n",
    "    print(f\"Types de corrélation présents: {correlation_types}\")\n",
    "    \n",
    "    # Palette de couleurs pour les corrélations\n",
    "    corr_colors = {\n",
    "        'uncorrelated': '#2ecc71',        # Vert\n",
    "        'weakly_correlated': '#f39c12',   # Orange\n",
    "        'strongly_correlated': '#e74c3c', # Rouge\n",
    "        'low_dimension': '#3498db',       # Bleu\n",
    "        'large_scale': '#9b59b6'          # Violet\n",
    "    }\n",
    "    \n",
    "    # Si on a les 3 types de corrélation, créer des graphiques comparatifs\n",
    "    corr_types_present = [c for c in ['uncorrelated', 'weakly_correlated', 'strongly_correlated'] \n",
    "                          if c in correlation_types]\n",
    "    \n",
    "    if len(corr_types_present) > 0:\n",
    "        print(f\"\\nAnalyse des corrélations: {corr_types_present}\")\n",
    "        \n",
    "        # ---- Graphique 4a: Valeur moyenne par algorithme et corrélation ----\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "        \n",
    "        # Filtrer seulement les types de corrélation\n",
    "        df_corr = df[df['correlation'].isin(corr_types_present)]\n",
    "        \n",
    "        if len(df_corr) > 0:\n",
    "            # Valeur moyenne\n",
    "            agg_val = df_corr.groupby(['algorithm', 'correlation'])['value'].mean().reset_index()\n",
    "            pivot_val = agg_val.pivot(index='algorithm', columns='correlation', values='value')\n",
    "            \n",
    "            ax1 = axes[0]\n",
    "            pivot_val.plot(kind='bar', ax=ax1, color=[corr_colors.get(c, '#333') for c in pivot_val.columns])\n",
    "            ax1.set_xlabel('Algorithme', fontsize=11)\n",
    "            ax1.set_ylabel('Valeur moyenne', fontsize=11)\n",
    "            ax1.set_title('Valeur obtenue par type de corrélation', fontsize=12, fontweight='bold')\n",
    "            ax1.tick_params(axis='x', rotation=45)\n",
    "            ax1.legend(title='Corrélation')\n",
    "            ax1.grid(alpha=0.3, axis='y')\n",
    "            \n",
    "            # Temps moyen\n",
    "            agg_time = df_corr.groupby(['algorithm', 'correlation'])['time_ms'].mean().reset_index()\n",
    "            pivot_time = agg_time.pivot(index='algorithm', columns='correlation', values='time_ms')\n",
    "            \n",
    "            ax2 = axes[1]\n",
    "            pivot_time.plot(kind='bar', ax=ax2, color=[corr_colors.get(c, '#333') for c in pivot_time.columns])\n",
    "            ax2.set_xlabel('Algorithme', fontsize=11)\n",
    "            ax2.set_ylabel('Temps moyen (ms)', fontsize=11)\n",
    "            ax2.set_title('Temps d\\'exécution par type de corrélation', fontsize=12, fontweight='bold')\n",
    "            ax2.tick_params(axis='x', rotation=45)\n",
    "            ax2.set_yscale('log')\n",
    "            ax2.legend(title='Corrélation')\n",
    "            ax2.grid(alpha=0.3, axis='y')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        else:\n",
    "            print(\"Pas de données pour les types de corrélation\")\n",
    "    else:\n",
    "        print(\"\\nLes benchmarks de corrélation (uncorrelated, weakly_correlated, strongly_correlated)\")\n",
    "        print(\"n'ont pas encore été exécutés.\")\n",
    "        print(\"Pour les inclure, relancez run_all_benchmarks() après avoir modifié discover_benchmarks()\")\n",
    "else:\n",
    "    print(\"results_df non chargé.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graphique 5 : Évolution des performances selon n pour chaque type de corrélation\n",
    "if results_df is not None:\n",
    "    df = results_df.copy()\n",
    "    \n",
    "    # Palette de couleurs pour les corrélations\n",
    "    corr_colors = {\n",
    "        'uncorrelated': '#2ecc71',\n",
    "        'weakly_correlated': '#f39c12', \n",
    "        'strongly_correlated': '#e74c3c',\n",
    "        'low_dimension': '#3498db',\n",
    "        'large_scale': '#9b59b6'\n",
    "    }\n",
    "    \n",
    "    # Sélectionner quelques algorithmes représentatifs pour la lisibilité\n",
    "    algos_to_show = ['Dynamic Programming', 'Greedy Ratio', 'Genetic Algorithm', 'Simulated Annealing']\n",
    "    algos_present = [a for a in algos_to_show if a in df['algorithm'].unique()]\n",
    "    \n",
    "    if len(algos_present) > 0:\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "        axes = axes.flatten()\n",
    "        \n",
    "        for idx, algo in enumerate(algos_present[:4]):\n",
    "            ax = axes[idx]\n",
    "            algo_data = df[df['algorithm'] == algo]\n",
    "            \n",
    "            for corr in algo_data['correlation'].unique():\n",
    "                corr_data = algo_data[algo_data['correlation'] == corr]\n",
    "                agg = corr_data.groupby('n').agg({'value': 'mean', 'time_ms': 'mean'}).reset_index()\n",
    "                agg = agg.sort_values('n')\n",
    "                \n",
    "                color = corr_colors.get(corr, '#333')\n",
    "                ax.plot(agg['n'], agg['value'], 'o-', label=corr, color=color, linewidth=2, markersize=6)\n",
    "            \n",
    "            ax.set_xlabel('Taille n', fontsize=10)\n",
    "            ax.set_ylabel('Valeur moyenne', fontsize=10)\n",
    "            ax.set_title(f'{algo}', fontsize=11, fontweight='bold')\n",
    "            ax.legend(fontsize=8)\n",
    "            ax.grid(alpha=0.3)\n",
    "            ax.set_xscale('log')\n",
    "        \n",
    "        plt.suptitle('Valeur obtenue par taille et type de corrélation', fontsize=13, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "else:\n",
    "    print(\"results_df non chargé.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graphique 6 : Heatmap de performance relative par corrélation\n",
    "if results_df is not None:\n",
    "    df = results_df.copy()\n",
    "    \n",
    "    # Calculer la valeur moyenne par algorithme et corrélation\n",
    "    pivot = df.groupby(['algorithm', 'correlation'])['value'].mean().unstack()\n",
    "    \n",
    "    if pivot.shape[1] > 1:  # Au moins 2 types de corrélation\n",
    "        # Normaliser par ligne (algorithme) pour voir les différences relatives\n",
    "        pivot_norm = pivot.div(pivot.max(axis=1), axis=0) * 100\n",
    "        \n",
    "        plt.figure(figsize=(12, 8))\n",
    "        sns.heatmap(pivot_norm, annot=True, fmt='.1f', cmap='RdYlGn', \n",
    "                    cbar_kws={'label': '% de la meilleure valeur'},\n",
    "                    linewidths=0.5)\n",
    "        plt.title('Performance relative par type de corrélation\\n', \n",
    "                  fontsize=12, fontweight='bold')\n",
    "        plt.xlabel('Type de corrélation', fontsize=11)\n",
    "        plt.ylabel('Algorithme', fontsize=11)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "else:\n",
    "    print(\"results_df non chargé.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graphique 7 : Tableau récapitulatif détaillé\n",
    "if results_df is not None:\n",
    "    df = results_df.copy()\n",
    "    \n",
    "    # Créer un tableau récapitulatif complet\n",
    "    summary = df.groupby(['correlation', 'algorithm']).agg({\n",
    "        'value': ['mean', 'std', 'count'],\n",
    "        'time_ms': ['mean', 'std'],\n",
    "        'n': ['min', 'max']\n",
    "    }).round(2)\n",
    "    \n",
    "    # Aplatir les colonnes multi-niveaux\n",
    "    summary.columns = ['_'.join(col).strip() for col in summary.columns.values]\n",
    "    summary = summary.reset_index()\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"TABLEAU RÉCAPITULATIF DES RÉSULTATS PAR TYPE DE CORRÉLATION\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for corr in summary['correlation'].unique():\n",
    "        print(f\"\\n>>> {corr.upper()} <<<\")\n",
    "        print(\"-\" * 60)\n",
    "        corr_data = summary[summary['correlation'] == corr]\n",
    "        \n",
    "        # Trier par valeur moyenne décroissante\n",
    "        corr_data = corr_data.sort_values('value_mean', ascending=False)\n",
    "        \n",
    "        for _, row in corr_data.iterrows():\n",
    "            print(f\"  {row['algorithm']:25s} | Valeur: {row['value_mean']:>10.1f} ± {row['value_std']:>8.1f} | \"\n",
    "                  f\"Temps: {row['time_ms_mean']:>10.2f}ms | n: {int(row['n_min'])}-{int(row['n_max'])}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "else:\n",
    "    print(\"results_df non chargé.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Analyse de Complexité\n",
    "\n",
    "### 7.1 Complexité Théorique des Algorithmes\n",
    "\n",
    "| Algorithme | Complexité Temporelle | Complexité Spatiale | Type |\n",
    "|------------|----------------------|---------------------|------|\n",
    "| **Brute Force** | $O(2^n)$ | $O(n)$ | Exact |\n",
    "| **DP Bottom-Up** | $O(n \\cdot C)$ | $O(n \\cdot C)$ | Exact (pseudo-poly) |\n",
    "| **DP Top-Down** | $O(n \\cdot C)$ | $O(n \\cdot C)$ | Exact (pseudo-poly) |\n",
    "| **Branch & Bound** | $O(2^n)$ pire cas | $O(n)$ | Exact |\n",
    "| **Greedy (ratio)** | $O(n \\log n)$ | $O(n)$ | Approximation |\n",
    "| **Fractional Knapsack** | $O(n \\log n)$ | $O(n)$ | Exact (relaxé) |\n",
    "| **Randomized** | $O(k \\cdot n)$ | $O(n)$ | Heuristique |\n",
    "| **Genetic Algorithm** | $O(g \\cdot p \\cdot n)$ | $O(p \\cdot n)$ | Métaheuristique |\n",
    "| **Simulated Annealing** | $O(i \\cdot n)$ | $O(n)$ | Métaheuristique |\n",
    "| **FPTAS** | $O(n^2 / \\varepsilon)$ | $O(n / \\varepsilon)$ | Approximation |\n",
    "\n",
    "$n$ = nombre d'items\n",
    "$C$ = capacité \n",
    "$k$ = itérations\n",
    "$g$ = générations\n",
    "$p$ = population\n",
    "$i$ = itérations SA\n",
    "$\\varepsilon$ = paramètre d'approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Classification des Algorithmes\n",
    "\n",
    "**Algorithmes Exacts:**\n",
    "- Garantissent la solution optimale\n",
    "- Brute Force: $O(2^n)$ - inutilisable pour $n > 25$\n",
    "- DP: $O(n \\cdot C)$ - pseudo-polynomial, dépend de la capacité\n",
    "- Branch & Bound: Élagage efficace, souvent meilleur que $O(2^n)$ en pratique\n",
    "\n",
    "**Algorithmes d'Approximation:**\n",
    "- FPTAS garantit $(1-\\varepsilon) \\times OPT$ en temps polynomial\n",
    "- Greedy ratio: approximation $\\frac{1}{2} \\times OPT$ garantie\n",
    "\n",
    "**Métaheuristiques:**\n",
    "- Pas de garantie théorique sur la qualité\n",
    "- Genetic Algorithm et Simulated Annealing explorent l'espace de recherche\n",
    "- Bons résultats en pratique, surtout pour grandes instances"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
